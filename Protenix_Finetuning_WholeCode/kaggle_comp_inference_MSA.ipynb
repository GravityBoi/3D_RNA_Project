{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c86973",
   "metadata": {},
   "source": [
    "## Install requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, tempfile, pathlib, subprocess, re, time\n",
    "from   timeit import default_timer as timer\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from   tqdm   import tqdm\n",
    "\n",
    "# ── user flags ──────────────────────────────────────────────────────────\n",
    "MODE        = \"local\"            #  <<<  \"local\"  or  \"submit\"\n",
    "RUN_LOCAL   = True\n",
    "RUN_KAGGLE  = not RUN_LOCAL\n",
    "\n",
    "NUM_CONF=5\n",
    "MAX_LENGTH=1000\n",
    "\n",
    "# assert torch.cuda.is_available(), \"Need an NVIDIA GPU.\"\n",
    "# print(\"torch\", torch.__version__, \"| cuda:\", torch.version.cuda,\n",
    "#       \"| gpu:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ── pip installs (done once) ────────────────────────────────────────────\n",
    "!pip install --no-deps protenix biopython ml-collections \\\n",
    "                      biotite==1.0.1 rdkit\n",
    "\n",
    "# ── Protenix resource directory ────────────────────────────────────────\n",
    "os.environ[\"USE_DEEPSPEED_EVO_ATTENTION\"] = \"True\"\n",
    "\n",
    "if RUN_LOCAL:\n",
    "    ROOT_DIR = \"/home/max/Documents/Protenix-KaggleRNA3D/af3-dev\"\n",
    "else:\n",
    "    ROOT_DIR = \"/kaggle/input/protenix/af3-dev\"\n",
    "        \n",
    "os.environ[\"PROTENIX_DATA_ROOT_DIR\"] = ROOT_DIR\n",
    "print(\"PROTENIX_DATA_ROOT_DIR →\", ROOT_DIR)\n",
    "\n",
    "print(\"Setting random seeds for deterministic prediction...\")\n",
    "np.random.seed(0)\n",
    "torch.random.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_CSV = (\"/home/max/Documents/Protenix-KaggleRNA3D/data/stanford-rna-3d-folding/\"\n",
    "           f'{\"validation\" if MODE==\"local\" else \"test\"}_sequences.csv')\n",
    "# SEQ_CSV = (\"/kaggle/input/stanford-rna-3d-folding/\"\n",
    "#            f'{\"validation\" if MODE==\"local\" else \"test\"}_sequences.csv')\n",
    "df      = pd.read_csv(SEQ_CSV)\n",
    "\n",
    "if MODE == \"local\":\n",
    "    # LABEL_CSV  = \"/home/max/Documents/Protenix-KaggleRNA3D/data/stanford-rna-3d-folding/validation_labels.csv\"\n",
    "    LABEL_CSV  = \"/kaggle/input/stanford-rna-3d-folding/validation_labels.csv\"\n",
    "    label_df   = pd.read_csv(LABEL_CSV)\n",
    "    label_df[\"target_id\"] = label_df.ID.str.rsplit(pat=\"_\", n=1).str[0]\n",
    "\n",
    "if MODE == \"local\":\n",
    "    LABEL_CSV  = \"/home/max/Documents/Protenix-KaggleRNA3D/data/stanford-rna-3d-folding/validation_labels_clean.csv\"\n",
    "    label_df   = pd.read_csv(LABEL_CSV)\n",
    "    label_df[\"target_id\"] = label_df.ID.str.rsplit(pat=\"_\", n=1).str[0]\n",
    "\n",
    "# build input JSON --------------------------------------------------------\n",
    "samples = [{\"name\":tid,\n",
    "            \"sequences\":[{\"rnaSequence\":{\"sequence\":seq,\"count\":1}}]}\n",
    "           for seq,tid in zip(df.sequence, df.target_id)]\n",
    "json_path = tempfile.mktemp(prefix=\"protenix_inputs_\", suffix=\".json\")\n",
    "json.dump(samples, open(json_path,\"w\"))\n",
    "print(\"json →\", json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38560872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protenix.data.json_to_feature import SampleDictToFeatures\n",
    "from protenix.data.featurizer import Featurizer as ProtenixFeaturizer\n",
    "from protenix.data.msa_featurizer import MSAFeaturizer\n",
    "from protenix.data.utils import make_dummy_feature, data_type_transform\n",
    "from protenix.utils.torch_utils import dict_to_tensor\n",
    "\n",
    "\n",
    "def featurize_rna_with_msa(sample_row: pd.Series, msa_featurizer: MSAFeaturizer) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a row from the sequence DataFrame and produces the feature dictionary\n",
    "    required by the Protenix model, including MSA features.\n",
    "    \"\"\"\n",
    "    name = sample_row[\"target_id\"]\n",
    "    sequence = sample_row[\"sequence\"]\n",
    "\n",
    "    # 1. Initial Featurization using SampleDictToFeatures (SDF)\n",
    "    # This creates the initial atom and token representations.\n",
    "    sample_input_item = {\"name\": name, \"sequences\": [{\"rnaSequence\": {\"sequence\": sequence, \"count\": 1}}]}\n",
    "    sdf_instance = SampleDictToFeatures(sample_input_item)\n",
    "    _, atom_array, token_array = sdf_instance.get_feature_dict()\n",
    "\n",
    "    # 2. MSA Featurization\n",
    "    # We use the full (uncropped) token array for inference.\n",
    "    selected_token_indices = np.arange(len(token_array))\n",
    "    \n",
    "    # The MSA featurizer needs a specific dictionary format.\n",
    "    # Note: `original_full_sequence` is used for MSA lookup.\n",
    "    bioassembly_for_msa = {\n",
    "        \"pdb_id\": name,\n",
    "        \"sequences\": {\"1\": sequence}, # Assuming single RNA chain, entity_id \"1\"\n",
    "        \"atom_array\": atom_array,\n",
    "        \"token_array\": token_array,\n",
    "        \"entity_poly_type\": {\"1\": \"polyribonucleotide\"}\n",
    "    }\n",
    "    entity_to_asym_id_int = {\"1\": [0]} # Assuming single chain, asym_id_int 0\n",
    "    \n",
    "    msa_features = msa_featurizer(\n",
    "        bioassembly_dict=bioassembly_for_msa,\n",
    "        selected_indices=selected_token_indices,\n",
    "        entity_to_asym_id_int=entity_to_asym_id_int\n",
    "    )\n",
    "    \n",
    "    msa_features_added = msa_features is not None and len(msa_features) > 0\n",
    "\n",
    "    # 3. Final Featurization using ProtenixFeaturizer\n",
    "    # This generates the final geometric and chemical features.\n",
    "    final_protenix_featurizer = ProtenixFeaturizer(\n",
    "        cropped_token_array=token_array, # Using full token_array\n",
    "        cropped_atom_array=atom_array,   # Using full atom_array\n",
    "        ref_pos_augment=False,           # No augmentation at inference\n",
    "        lig_atom_rename=False\n",
    "    )\n",
    "    final_feat_dict = final_protenix_featurizer.get_all_input_features()\n",
    "\n",
    "    # 4. Merge MSA features into the final feature dictionary\n",
    "    if msa_features_added:\n",
    "        final_feat_dict.update(dict_to_tensor(msa_features))\n",
    "\n",
    "    # 5. Add Dummy Features and Finalize\n",
    "    # Template features are not used, so we create a dummy.\n",
    "    # If MSA featurization failed, create a dummy for that too.\n",
    "    dummy_feature_list = [\"template\"]\n",
    "    if not msa_features_added:\n",
    "        dummy_feature_list.append(\"msa\")\n",
    "        \n",
    "    final_feat_dict = make_dummy_feature(final_feat_dict, dummy_feature_list)\n",
    "    final_feat_dict = data_type_transform(final_feat_dict)\n",
    "\n",
    "    # 6. Construct the final data dictionary for the model\n",
    "    basic_info = {\n",
    "        \"pdb_id\": name,\n",
    "        \"N_token\": torch.tensor([final_feat_dict[\"token_index\"].shape[0]]),\n",
    "        \"N_atom\": torch.tensor([final_feat_dict[\"atom_to_token_idx\"].shape[0]]),\n",
    "    }\n",
    "    if msa_features_added:\n",
    "         basic_info[\"N_msa\"] = torch.tensor([final_feat_dict[\"msa\"].shape[0]])\n",
    "\n",
    "    data = {\n",
    "        \"input_feature_dict\": final_feat_dict,\n",
    "        \"label_dict\": {}, # No labels at inference\n",
    "        \"label_full_dict\": {},\n",
    "        \"basic\": basic_info,\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8629c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.configs_base       import configs as cfg_base\n",
    "from configs.configs_data       import data_configs\n",
    "from configs.configs_inference  import inference_configs\n",
    "from protenix.config.config     import parse_configs\n",
    "from runner.inference           import InferenceRunner, update_inference_configs\n",
    "\n",
    "ckpt_path = f\"/home/max/Documents/ProtenixFinetuningFinalResults/5999.pt\"\n",
    "\n",
    "os.environ[\"CUTLASS_PATH\"] = \"/home/max/Documents/Protenix-KaggleRNA3D/Cutlass/cutlass\"\n",
    "\n",
    "cfg_base[\"use_deepspeed_evo_attention\"]     = True\n",
    "cfg_base[\"model\"][\"N_cycle\"]                = 10\n",
    "cfg_base[\"sample_diffusion\"][\"N_step\"]      = 200\n",
    "cfg_base[\"sample_diffusion\"][\"N_sample\"]    = 5\n",
    "inference_configs[\"load_checkpoint_path\"]   = ckpt_path\n",
    "inference_configs[\"dtype\"]                  = \"bf16\"\n",
    "inference_configs[\"template\"] = {\n",
    "    \"use_templates\": False,\n",
    "    \"template_mmcif_dir\": \"\"\n",
    "}\n",
    "\n",
    "msa_configs = {\n",
    "    \"enable\": True,\n",
    "    \"enable_rna_msa\": True,\n",
    "    \"enable_prot_msa\": False,\n",
    "    \"merge_method\": \"dense_max\",\n",
    "    \"strategy\": \"random\",\n",
    "    \"max_size\": {\"train\": 512, \"test\": 2048},\n",
    "    \"sample_cutoff\": {\"train\": 64, \"test\": 1024},\n",
    "    \"min_size\": {\"train\": 1, \"test\": 1},\n",
    "    \"prot\": {},\n",
    "    \"rna\": {\n",
    "        \"rna_msa_dir\": \"/home/max/Documents/Protenix-KaggleRNA3D/data/stanford-rna-3d-folding/MSA_v2/RNA_MSA_Stockholm\",\n",
    "        \"seq_to_pdb_idx_path\": os.path.join(\"/home/max/Documents/Protenix-KaggleRNA3D/data/stanford-rna-3d-folding\", \"MSA_v2/seq_to_target_map.json\"),\n",
    "        \"indexing_method\": \"sequence\",\n",
    "        \"seq_limits\": {},\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Initializing standalone MSAFeaturizer for data prep...\")\n",
    "shared_msa_args = {\n",
    "    \"merge_method\": msa_configs[\"merge_method\"],\n",
    "    \"max_size\": msa_configs[\"max_size\"][\"test\"]  # Explicitly use the test size for inference\n",
    "}\n",
    "msa_featurizer = MSAFeaturizer(\n",
    "    prot_msa_args={},  # Protein MSA is disabled\n",
    "    rna_msa_args={**msa_configs.get('rna', {}), **shared_msa_args},\n",
    "    enable_rna_msa=msa_configs.get('enable_rna_msa', False),\n",
    "    enable_prot_msa=msa_configs.get('enable_prot_msa', False)\n",
    ")\n",
    "\n",
    "# 1. Start with the base model and inference configs\n",
    "cfg = {**cfg_base, **inference_configs}\n",
    "\n",
    "# 2. Inject your custom MSA settings into the main data configurations\n",
    "data_configs['msa'] = msa_configs\n",
    "\n",
    "# 3. Add the entire data_configs under a 'data' key in the final config\n",
    "cfg['data'] = data_configs\n",
    "\n",
    "# 4. Add any other top-level keys\n",
    "cfg[\"dump_dir\"] = tempfile.mkdtemp(prefix=\"pred_out_\")\n",
    "\n",
    "# Now, parse the correctly structured config\n",
    "cfg = parse_configs(cfg, fill_required_with_null=True)\n",
    "\n",
    "# --- Initialize Inference Runner ---\n",
    "print(\"Initializing InferenceRunner with corrected config...\")\n",
    "runner = InferenceRunner(cfg)\n",
    "print(\"Model ready:\", type(runner.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting prediction loop...\")\n",
    "df_test = pd.read_csv(SEQ_CSV)\n",
    "rows = []\n",
    "\n",
    "for _, sample_row in tqdm(\n",
    "        df_test.iterrows(),\n",
    "        total=len(df_test),\n",
    "        desc=\"Featurize → Predict\",\n",
    "        leave=True\n",
    "    ):\n",
    "    tid = sample_row[\"target_id\"]\n",
    "    seq = sample_row[\"sequence\"]\n",
    "\n",
    "    try:\n",
    "        # 1. Generate features\n",
    "        data = featurize_rna_with_msa(sample_row, msa_featurizer)\n",
    "\n",
    "        # 2. Update model configs\n",
    "        runner.update_model_configs(\n",
    "            update_inference_configs(cfg, int(data[\"basic\"][\"N_token\"]))\n",
    "        )\n",
    "\n",
    "        # 3. Predict\n",
    "        with torch.no_grad():\n",
    "            prediction_output = runner.predict(data)\n",
    "            coord = prediction_output[\"coordinate\"]\n",
    "\n",
    "        # 4. Extract C1' coords\n",
    "        c1_mask = data[\"input_feature_dict\"][\"atom_to_tokatom_idx\"] == 12\n",
    "        coord = coord[:, c1_mask, :]\n",
    "\n",
    "        # 5. Sanity check / padding\n",
    "        if coord.shape[1] != len(seq):\n",
    "            tqdm.write(\n",
    "                f\"Warning: Mismatch in predicted length for {tid}. \"\n",
    "                f\"Expected {len(seq)}, got {coord.shape[1]}. Padding with zeros.\"\n",
    "            )\n",
    "            new_coord = torch.zeros(coord.shape[0], len(seq), 3, device=coord.device)\n",
    "            min_len = min(len(seq), coord.shape[1])\n",
    "            new_coord[:, :min_len, :] = coord[:, :min_len, :]\n",
    "            coord = new_coord\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        tqdm.write(f\"ERROR processing {tid}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        coord = torch.zeros(NUM_CONF, len(seq), 3)\n",
    "\n",
    "    # 6. Ensure NUM_CONF predictions\n",
    "    while coord.shape[0] < NUM_CONF:\n",
    "        coord = torch.cat([coord, coord[-1:]], dim=0)\n",
    "\n",
    "    # 7. Build rows for submission\n",
    "    for i, res in enumerate(seq, 1):\n",
    "        triplets = coord[:NUM_CONF, i - 1, :].cpu().numpy().reshape(-1)\n",
    "        rows.append([f\"{tid}_{i}\", res, i] + triplets.tolist())\n",
    "\n",
    "# write out\n",
    "cols = (\n",
    "    [\"ID\", \"resname\", \"resid\"] +\n",
    "    [f\"{ax}_{k}\" for k in range(1,6) for ax in (\"x\",\"y\",\"z\")]\n",
    ")\n",
    "sub = pd.DataFrame(rows, columns=cols)\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv written — shape:\", sub.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_tmscore_output(output):\n",
    "    tm_score_match = re.findall(r'TM-score=\\s+([\\d.]+)', output)[1]\n",
    "    return float(tm_score_match)\n",
    "\n",
    "def write_target_line(\n",
    "    atom_name, atom_serial, residue_name, chain_id, residue_num,\n",
    "    x_coord, y_coord, z_coord, occupancy=1.0, b_factor=0.0, atom_type='P'\n",
    ") -> str:\n",
    "    return (\n",
    "        f'ATOM  {atom_serial:>5d}  {atom_name:<5s} {residue_name:<3s} '\n",
    "        f'{residue_num:>3d}    {x_coord:>8.3f}{y_coord:>8.3f}'\n",
    "        f'{z_coord:>8.3f}{occupancy:>6.2f}{b_factor:>6.2f}           {atom_type}\\n'\n",
    "    )\n",
    "\n",
    "def write2pdb(df: pd.DataFrame, xyz_id: int, target_path: str) -> int:\n",
    "    resolved_cnt = 0\n",
    "    with open(target_path, 'w') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            x = row[f'x_{xyz_id}']; y = row[f'y_{xyz_id}']; z = row[f'z_{xyz_id}']\n",
    "            if x > -1e17 and y > -1e17 and z > -1e17:\n",
    "                resolved_cnt += 1\n",
    "                f.write(write_target_line(\n",
    "                    atom_name=\"C1'\", atom_serial=int(row['resid']),\n",
    "                    residue_name=row['resname'], chain_id='0',\n",
    "                    residue_num=int(row['resid']),\n",
    "                    x_coord=x, y_coord=y, z_coord=z, atom_type='C'\n",
    "                ))\n",
    "    return resolved_cnt\n",
    "\n",
    "def get_base_target_id(long_id):\n",
    "    return \"_\".join(str(long_id).split(\"_\")[:-1])\n",
    "\n",
    "def score_and_report(solution: pd.DataFrame, submission: pd.DataFrame):\n",
    "    solution['target_id'] = solution['ID'].apply(get_base_target_id)\n",
    "    submission['target_id'] = submission['ID'].apply(get_base_target_id)\n",
    "\n",
    "    native_idxs = sorted(int(c.split('_')[1])\n",
    "                         for c in solution.columns if c.startswith('x_'))\n",
    "\n",
    "    usalign = \"/home/max/Documents/Protenix-KaggleRNA3D/af3-dev/USalign/USalign\"\n",
    "    \n",
    "    per_target = {}\n",
    "    all_best_scores = []\n",
    "    \n",
    "    temp_dir = \"./scoring_temp/\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    common_targets = sorted(list(set(solution['target_id'].unique()) & set(submission['target_id'].unique())))\n",
    "    print(f\"Scoring {len(common_targets)} common targets...\")\n",
    "\n",
    "    for tid in tqdm(common_targets): # Use tqdm for a nice progress bar\n",
    "        grp_nat = solution[solution['target_id'] == tid]\n",
    "        grp_pred = submission[submission['target_id'] == tid]\n",
    "        best_of_five = []\n",
    "\n",
    "        native_path = os.path.join(temp_dir, f'native_{tid}.pdb')\n",
    "        predicted_path = os.path.join(temp_dir, f'predicted_{tid}.pdb')\n",
    "\n",
    "        for pred_cnt in range(1, 6):\n",
    "            best_for_this_pred = 0.0\n",
    "            n_pred = write2pdb(grp_pred, pred_cnt, predicted_path)\n",
    "            if n_pred == 0:\n",
    "                best_of_five.append(0.0)\n",
    "                continue\n",
    "\n",
    "            for nat_cnt in native_idxs:\n",
    "                n_nat = write2pdb(grp_nat, nat_cnt, native_path)\n",
    "                if n_nat > 0:\n",
    "                    out = os.popen(\n",
    "                        f'{usalign} {predicted_path} {native_path} -atom \" C1\\'\"'\n",
    "                    ).read()\n",
    "                    best_for_this_pred = max(best_for_this_pred,\n",
    "                                             parse_tmscore_output(out))\n",
    "            best_of_five.append(best_for_this_pred)\n",
    "\n",
    "        per_target[tid] = best_of_five\n",
    "\n",
    "        target_best_score = max(best_of_five)\n",
    "        all_best_scores.append(target_best_score)\n",
    "\n",
    "    mean_tm = np.mean(all_best_scores) if all_best_scores else 0.0\n",
    "    print(f\"\\n>>> FINAL mean best-of-5 TM-score = {mean_tm:.4f}\")\n",
    "    \n",
    "    return per_target, mean_tm\n",
    "\n",
    "    \n",
    "solution   = pd.read_csv(\n",
    "    \"/home/max/Documents/Protenix-KaggleRNA3D/data/stanford-rna-3d-folding/validation_labels_clean.csv\"\n",
    ")\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "per_target_scores, mean_tm = score_and_report(solution, submission)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
