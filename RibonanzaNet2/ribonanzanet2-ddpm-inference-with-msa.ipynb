{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87793,"databundleVersionId":12276181,"sourceType":"competition"},{"sourceId":10855324,"sourceType":"datasetVersion","datasetId":6742586},{"sourceId":10880419,"sourceType":"datasetVersion","datasetId":6760509},{"sourceId":11278691,"sourceType":"datasetVersion","datasetId":7051341},{"sourceId":11279607,"sourceType":"datasetVersion","datasetId":7051942},{"sourceId":11469248,"sourceType":"datasetVersion","datasetId":7187409},{"sourceId":12199811,"sourceType":"datasetVersion","datasetId":7684811},{"sourceId":12199817,"sourceType":"datasetVersion","datasetId":7684816},{"sourceId":311741,"sourceType":"modelInstanceVersion","modelInstanceId":264400,"modelId":285488}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- MSA parsing utility ---\nimport numpy as np\n\ndef parse_msa(msa_path: str, max_seqs: int = 100) -> np.ndarray:\n    with open(msa_path, 'r') as f:\n        lines = f.read().splitlines()\n    seqs = []\n    current_seq = \"\"\n    for line in lines:\n        if line.startswith(\">\"):\n            if current_seq:\n                seqs.append(current_seq)\n                current_seq = \"\"\n        else:\n            current_seq += line.strip()\n    if current_seq:\n        seqs.append(current_seq)\n    seqs = seqs[:max_seqs]\n    vocab = {'A': 0, 'U': 1, 'G': 2, 'C': 3, 'N': 4, '-': 5, '.': 5}\n    L = len(seqs[0])\n    N = len(seqs)\n    msa_tensor = np.zeros((N, L, 7), dtype=np.float32)\n    for i, seq in enumerate(seqs):\n        for j, res in enumerate(seq):\n            idx = vocab.get(res.upper(), 6)  # unknown = 6\n            msa_tensor[i, j, idx] = 1.0\n    return msa_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:53:26.930547Z","iopub.execute_input":"2025-06-22T11:53:26.930832Z","iopub.status.idle":"2025-06-22T11:53:26.940816Z","shell.execute_reply.started":"2025-06-22T11:53:26.930812Z","shell.execute_reply":"2025-06-22T11:53:26.939960Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport random\nimport pickle\nimport os\nimport sys","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:53:55.547588Z","iopub.execute_input":"2025-06-22T11:53:55.548098Z","iopub.status.idle":"2025-06-22T11:54:00.078849Z","shell.execute_reply.started":"2025-06-22T11:53:55.548073Z","shell.execute_reply":"2025-06-22T11:54:00.078269Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"config = {\n    \"seed\": 0,\n    \"cutoff_date\": \"2020-01-01\",\n    \"test_cutoff_date\": \"2022-05-01\",\n    \"max_len\": 384,\n    \"batch_size\": 1,\n    \"learning_rate\": 1e-4,\n    \"weight_decay\": 0.0,\n    \"mixed_precision\": \"bf16\",\n    \"model_config_path\": \"../working/configs/pairwise.yaml\",  # Adjust path as needed\n    \"epochs\": 10,\n    \"cos_epoch\": 5,\n    \"loss_power_scale\": 1.0,\n    \"max_cycles\": 1,\n    \"grad_clip\": 0.1,\n    \"gradient_accumulation_steps\": 1,\n    \"d_clamp\": 30,\n    \"max_len_filter\": 9999999,\n    \"structural_violation_epoch\": 50,\n    \"balance_weight\": False,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:02.299888Z","iopub.execute_input":"2025-06-22T11:54:02.300948Z","iopub.status.idle":"2025-06-22T11:54:02.306439Z","shell.execute_reply.started":"2025-06-22T11:54:02.300911Z","shell.execute_reply":"2025-06-22T11:54:02.305642Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"test_data=pd.read_csv(\"/kaggle/input/validation-sequences-clean-csv/validation_sequences_clean.csv\")\ntest_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:04.515732Z","iopub.execute_input":"2025-06-22T11:54:04.515993Z","iopub.status.idle":"2025-06-22T11:54:04.580148Z","shell.execute_reply.started":"2025-06-22T11:54:04.515974Z","shell.execute_reply":"2025-06-22T11:54:04.579256Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"  target_id                                           sequence  \\\n0    9L5R_2  AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUU...   \n1   9GFT_AU  GGGGCUAUAGCUCAGCUGGGAGAGCGCUUGCAUGGCAUGCAAGAGG...   \n2    9L0R_K  GCCGUCUCAAUAGUGGCUUAGCACAGAUAAUCCAUAGCGAUAUGGG...   \n3   9GFT_A3  GGCUACGUAGCUCAGUUGGUUAGAGCACAUCACUCAUAAUGAUGGG...   \n4    9B2K_B  AAACAGCAUAGCAAGUUAAAAUAAGGCUAGUCCGUUAUCAACUUGA...   \n\n  temporal_cutoff                                        description  \\\n0      2025-03-12  Cryo-EM structure of the thermophile spliceoso...   \n1      2025-02-12  Structure of the HrpA-bound E. coli disome, Cl...   \n2      2025-03-19       Streptococcus agalactiae GOLLD RNA dodecamer   \n3      2025-02-12  Structure of the HrpA-bound E. coli disome, Cl...   \n4      2025-03-26    SpCas9 with dual-guide RNA in open conformation   \n\n                                       all_sequences  \n0  >9L5R_1|Chain A[auth 2]|U2 snRNA|Chaetomium th...  \n1  >9GFT_1|Chains A[auth 0], N[auth AA]|16S ribos...  \n2  >9L0R_1|Chains A, B, C, D, E, F, G, H, I, J, K...  \n3  >9GFT_1|Chains A[auth 0], N[auth AA]|16S ribos...  \n4  >9B2K_1|Chain A|RNA (5'-R(P*GP*UP*UP*UP*UP*AP*...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target_id</th>\n      <th>sequence</th>\n      <th>temporal_cutoff</th>\n      <th>description</th>\n      <th>all_sequences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9L5R_2</td>\n      <td>AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUU...</td>\n      <td>2025-03-12</td>\n      <td>Cryo-EM structure of the thermophile spliceoso...</td>\n      <td>&gt;9L5R_1|Chain A[auth 2]|U2 snRNA|Chaetomium th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9GFT_AU</td>\n      <td>GGGGCUAUAGCUCAGCUGGGAGAGCGCUUGCAUGGCAUGCAAGAGG...</td>\n      <td>2025-02-12</td>\n      <td>Structure of the HrpA-bound E. coli disome, Cl...</td>\n      <td>&gt;9GFT_1|Chains A[auth 0], N[auth AA]|16S ribos...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9L0R_K</td>\n      <td>GCCGUCUCAAUAGUGGCUUAGCACAGAUAAUCCAUAGCGAUAUGGG...</td>\n      <td>2025-03-19</td>\n      <td>Streptococcus agalactiae GOLLD RNA dodecamer</td>\n      <td>&gt;9L0R_1|Chains A, B, C, D, E, F, G, H, I, J, K...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9GFT_A3</td>\n      <td>GGCUACGUAGCUCAGUUGGUUAGAGCACAUCACUCAUAAUGAUGGG...</td>\n      <td>2025-02-12</td>\n      <td>Structure of the HrpA-bound E. coli disome, Cl...</td>\n      <td>&gt;9GFT_1|Chains A[auth 0], N[auth AA]|16S ribos...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9B2K_B</td>\n      <td>AAACAGCAUAGCAAGUUAAAAUAAGGCUAGUCCGUUAUCAACUUGA...</td>\n      <td>2025-03-26</td>\n      <td>SpCas9 with dual-guide RNA in open conformation</td>\n      <td>&gt;9B2K_1|Chain A|RNA (5'-R(P*GP*UP*UP*UP*UP*AP*...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\ndef create_msa_features(msa_tensor: np.ndarray) -> dict:\n    \"\"\"Create MSA-derived features for the model\"\"\"\n    N, L, _ = msa_tensor.shape\n    \n    # Conservation score (entropy-based)\n    conservation = np.zeros(L)\n    for pos in range(L):\n        counts = np.sum(msa_tensor[:, pos, :4], axis=0)  # Only AUGC\n        total = np.sum(counts)\n        if total > 0:\n            probs = counts / total\n            probs = probs[probs > 0]  # Remove zeros for log\n            conservation[pos] = -np.sum(probs * np.log2(probs + 1e-8))\n    \n    # Coevolution features (simplified mutual information)\n    coevolution = np.zeros((L, L))\n    for i in range(L):\n        for j in range(i+1, L):\n            # Joint distribution\n            joint_counts = np.zeros((4, 4))\n            for seq_idx in range(N):\n                res_i = np.argmax(msa_tensor[seq_idx, i, :4])\n                res_j = np.argmax(msa_tensor[seq_idx, j, :4])\n                joint_counts[res_i, res_j] += 1\n            \n            # Marginal distributions\n            marg_i = np.sum(joint_counts, axis=1)\n            marg_j = np.sum(joint_counts, axis=0)\n            \n            # Mutual information\n            mi = 0\n            total = np.sum(joint_counts)\n            if total > 0:\n                for x in range(4):\n                    for y in range(4):\n                        if joint_counts[x,y] > 0 and marg_i[x] > 0 and marg_j[y] > 0:\n                            pxy = joint_counts[x,y] / total\n                            px = marg_i[x] / total\n                            py = marg_j[y] / total\n                            mi += pxy * np.log2(pxy / (px * py + 1e-8) + 1e-8)\n            \n            coevolution[i, j] = coevolution[j, i] = mi\n    \n    return {\n        'conservation': conservation,\n        'coevolution': coevolution,\n        'msa_depth': N,\n        'msa_tensor': msa_tensor\n    }\n\nclass RNADatasetWithMSA(Dataset):\n    def __init__(self, data, msa_dir=None):\n        self.data = data\n        self.msa_dir = msa_dir\n        # Updated tokens dictionary with fallback for unknown characters\n        self.tokens = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n        self.unknown_token = 0  # Map unknown characters to 'A' (index 0)\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        # Use get() method with default fallback for unknown characters\n        sequence = [self.tokens.get(nt, self.unknown_token) for nt in (self.data.loc[idx, 'sequence'])]\n        sequence = np.array(sequence)\n        sequence = torch.tensor(sequence)\n        \n        result = {'sequence': sequence}\n        \n        # Try to load MSA if directory provided\n        if self.msa_dir:\n            target_id = self.data.loc[idx, 'target_id']\n            msa_path = os.path.join(self.msa_dir, f\"{target_id}.MSA.fasta\")\n            \n            if os.path.exists(msa_path):\n                try:\n                    msa_tensor = parse_msa(msa_path)\n                    msa_features = create_msa_features(msa_tensor)\n                    \n                    result['msa_features'] = {\n                        'conservation': torch.tensor(msa_features['conservation'], dtype=torch.float32),\n                        'coevolution': torch.tensor(msa_features['coevolution'], dtype=torch.float32),\n                        'msa_depth': msa_features['msa_depth'],\n                        'has_msa': True\n                    }\n                except Exception as e:\n                    print(f\"Warning: Could not load MSA for {target_id}: {e}\")\n                    result['msa_features'] = self._get_dummy_msa_features(len(sequence))\n            else:\n                result['msa_features'] = self._get_dummy_msa_features(len(sequence))\n        else:\n            result['msa_features'] = self._get_dummy_msa_features(len(sequence))\n        \n        return result\n    \n    def _get_dummy_msa_features(self, seq_len):\n        \"\"\"Create dummy MSA features when no MSA is available\"\"\"\n        return {\n            'conservation': torch.zeros(seq_len, dtype=torch.float32),\n            'coevolution': torch.zeros(seq_len, seq_len, dtype=torch.float32),\n            'msa_depth': 1,\n            'has_msa': False\n        }\n\nmsa_directory = \"/kaggle/input/stanford-rna-3d-folding/MSA_v2\"\ntest_dataset = RNADatasetWithMSA(test_data, msa_dir=msa_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:06.752173Z","iopub.execute_input":"2025-06-22T11:54:06.752953Z","iopub.status.idle":"2025-06-22T11:54:06.771548Z","shell.execute_reply.started":"2025-06-22T11:54:06.752916Z","shell.execute_reply":"2025-06-22T11:54:06.770708Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"sys.path.append(\"/kaggle/input/ribonanzanet2/pytorch/alpha/1\")\n\nimport torch.nn as nn\nfrom Network import *\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\nclass finetuned_RibonanzaNet(RibonanzaNet):\n    def __init__(self, rnet_config, config, pretrained=False):\n        rnet_config.dropout=0.1\n        rnet_config.use_grad_checkpoint=True\n        super(finetuned_RibonanzaNet, self).__init__(rnet_config)\n        if pretrained:\n            self.load_state_dict(torch.load(config.pretrained_weight_path,map_location='cpu'))\n        # self.ct_predictor=nn.Sequential(nn.Linear(64,256),\n        #                                 nn.ReLU(),\n        #                                 nn.Linear(256,64),\n        #                                 nn.ReLU(),\n        #                                 nn.Linear(64,1)) \n        self.dropout=nn.Dropout(0.0)\n\n        decoder_dim=config.decoder_dim\n        self.structure_module=[SimpleStructureModule(d_model=decoder_dim, nhead=config.decoder_nhead, \n                 dim_feedforward=decoder_dim*4, pairwise_dimension=rnet_config.pairwise_dimension, dropout=0.0) for i in range(config.decoder_num_layers)]\n        self.structure_module=nn.ModuleList(self.structure_module)\n\n        self.xyz_embedder=nn.Linear(3,decoder_dim)\n        self.xyz_norm=nn.LayerNorm(decoder_dim)\n        self.xyz_predictor=nn.Linear(decoder_dim,3)\n        \n        self.adaptor=nn.Sequential(nn.Linear(rnet_config.ninp,decoder_dim),nn.LayerNorm(decoder_dim))\n\n        self.distogram_predictor=nn.Sequential(nn.LayerNorm(rnet_config.pairwise_dimension),\n                                                nn.Linear(rnet_config.pairwise_dimension,40))\n\n        self.time_embedder=SinusoidalPosEmb(decoder_dim)\n\n        self.time_mlp=nn.Sequential(nn.Linear(decoder_dim,decoder_dim),\n                                    nn.ReLU(),  \n                                    nn.Linear(decoder_dim,decoder_dim))\n        self.time_norm=nn.LayerNorm(decoder_dim)\n\n        self.distance2pairwise=nn.Linear(1,rnet_config.pairwise_dimension,bias=False)\n\n        self.pair_mlp=nn.Sequential(nn.Linear(rnet_config.pairwise_dimension,rnet_config.pairwise_dimension),\n                                    nn.ReLU(),\n                                    nn.Linear(rnet_config.pairwise_dimension,rnet_config.pairwise_dimension))\n\n\n        #hyperparameters for diffusion\n        self.n_times = config.n_times\n\n        #self.model = model\n        \n        # define linear variance schedule(betas)\n        beta_1, beta_T = config.beta_min, config.beta_max\n        betas = torch.linspace(start=beta_1, end=beta_T, steps=config.n_times)#.to(device) # follows DDPM paper\n        self.sqrt_betas = torch.sqrt(betas)\n                                     \n        # define alpha for forward diffusion kernel\n        self.alphas = 1 - betas\n        self.sqrt_alphas = torch.sqrt(self.alphas)\n        alpha_bars = torch.cumprod(self.alphas, dim=0)\n        self.sqrt_one_minus_alpha_bars = torch.sqrt(1-alpha_bars)\n        self.sqrt_alpha_bars = torch.sqrt(alpha_bars)\n\n        self.data_std=config.data_std\n\n\n    def custom(self, module):\n        def custom_forward(*inputs):\n            inputs = module(*inputs)\n            return inputs\n        return custom_forward\n    \n    def embed_pair_distance(self,inputs):\n        pairwise_features,xyz=inputs\n        distance_matrix=xyz[:,None,:,:]-xyz[:,:,None,:]\n        distance_matrix=(distance_matrix**2).sum(-1).clip(2,37**2).sqrt()\n        distance_matrix=distance_matrix[:,:,:,None]\n        pairwise_features=pairwise_features+self.distance2pairwise(distance_matrix)\n\n        return pairwise_features\n\n    def forward(self,src,xyz,t):\n        \n        #with torch.no_grad():\n        sequence_features, pairwise_features=self.get_embeddings(src, torch.ones_like(src).long().to(src.device))\n        \n        distogram=self.distogram_predictor(pairwise_features)\n\n        sequence_features=self.adaptor(sequence_features)\n\n        decoder_batch_size=xyz.shape[0]\n        sequence_features=sequence_features.repeat(decoder_batch_size,1,1)\n        \n\n        pairwise_features=pairwise_features.expand(decoder_batch_size,-1,-1,-1)\n\n        pairwise_features= checkpoint.checkpoint(self.custom(self.embed_pair_distance), [pairwise_features,xyz],use_reentrant=False)\n\n        time_embed=self.time_embedder(t).unsqueeze(1)\n        tgt=self.xyz_norm(sequence_features+self.xyz_embedder(xyz)+time_embed)\n\n        tgt=self.time_norm(tgt+self.time_mlp(tgt))\n\n        for layer in self.structure_module:\n            #tgt=layer([tgt, sequence_features,pairwise_features,xyz,None])\n            tgt=checkpoint.checkpoint(self.custom(layer),\n            [tgt, sequence_features,pairwise_features,xyz,None],\n            use_reentrant=False)\n            # xyz=xyz+self.xyz_predictor(sequence_features).squeeze(0)\n            # xyzs.append(xyz)\n            #print(sequence_features.shape)\n        \n        xyz=self.xyz_predictor(tgt).squeeze(0)\n        #.squeeze(0)\n\n        return xyz, distogram\n    \n\n    def denoise(self,sequence_features,pairwise_features,xyz,t):\n        decoder_batch_size=xyz.shape[0]\n        sequence_features=sequence_features.expand(decoder_batch_size,-1,-1)\n        pairwise_features=pairwise_features.expand(decoder_batch_size,-1,-1,-1)\n\n        pairwise_features=self.embed_pair_distance([pairwise_features,xyz])\n\n        sequence_features=self.adaptor(sequence_features)\n        time_embed=self.time_embedder(t).unsqueeze(1)\n        tgt=self.xyz_norm(sequence_features+self.xyz_embedder(xyz)+time_embed)\n        tgt=self.time_norm(tgt+self.time_mlp(tgt))\n        #xyz_batch_size=xyz.shape[0]\n        \n\n\n        for layer in self.structure_module:\n            tgt=layer([tgt, sequence_features,pairwise_features,xyz,None])\n            # xyz=xyz+self.xyz_predictor(sequence_features).squeeze(0)\n            # xyzs.append(xyz)\n            #print(sequence_features.shape)\n        xyz=self.xyz_predictor(tgt).squeeze(0)\n        # print(xyz.shape)\n        # exit()\n        return xyz\n\n\n    def extract(self, a, t, x_shape):\n        \"\"\"\n            from lucidrains' implementation\n                https://github.com/lucidrains/denoising-diffusion-pytorch/blob/beb2f2d8dd9b4f2bd5be4719f37082fe061ee450/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L376\n        \"\"\"\n        b, *_ = t.shape\n        out = a.gather(-1, t)\n        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n    \n    def scale_to_minus_one_to_one(self, x):\n        # according to the DDPMs paper, normalization seems to be crucial to train reverse process network\n        return x * 2 - 1\n    \n    def reverse_scale_to_zero_to_one(self, x):\n        return (x + 1) * 0.5\n    \n    def make_noisy(self, x_zeros, t): \n        # assume we get raw data, so center and scale by 35\n        x_zeros = x_zeros - torch.nanmean(x_zeros,1,keepdim=True)\n        x_zeros = x_zeros/self.data_std\n        #rotate randomly\n        x_zeros = random_rotation_point_cloud_torch_batch(x_zeros)\n\n\n        # perturb x_0 into x_t (i.e., take x_0 samples into forward diffusion kernels)\n        epsilon = torch.randn_like(x_zeros).to(x_zeros.device)\n        \n        sqrt_alpha_bar = self.extract(self.sqrt_alpha_bars.to(x_zeros.device), t, x_zeros.shape)\n        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars.to(x_zeros.device), t, x_zeros.shape)\n        \n        # Let's make noisy sample!: i.e., Forward process with fixed variance schedule\n        #      i.e., sqrt(alpha_bar_t) * x_zero + sqrt(1-alpha_bar_t) * epsilon\n        noisy_sample = x_zeros * sqrt_alpha_bar + epsilon * sqrt_one_minus_alpha_bar\n    \n        return noisy_sample.detach(), epsilon\n    \n    \n    # def forward(self, x_zeros):\n    #     x_zeros = self.scale_to_minus_one_to_one(x_zeros)\n        \n    #     B, _, _, _ = x_zeros.shape\n        \n    #     # (1) randomly choose diffusion time-step\n    #     t = torch.randint(low=0, high=self.n_times, size=(B,)).long().to(x_zeros.device)\n        \n    #     # (2) forward diffusion process: perturb x_zeros with fixed variance schedule\n    #     perturbed_images, epsilon = self.make_noisy(x_zeros, t)\n        \n    #     # (3) predict epsilon(noise) given perturbed data at diffusion-timestep t.\n    #     pred_epsilon = self.model(perturbed_images, t)\n        \n    #     return perturbed_images, epsilon, pred_epsilon\n    \n    \n    def denoise_at_t(self, x_t, sequence_features, pairwise_features, timestep, t):\n        B, _, _ = x_t.shape\n        if t > 1:\n            z = torch.randn_like(x_t).to(sequence_features.device)\n        else:\n            z = torch.zeros_like(x_t).to(sequence_features.device)\n        \n        # at inference, we use predicted noise(epsilon) to restore perturbed data sample.\n        epsilon_pred = self.denoise(sequence_features, pairwise_features, x_t, timestep)\n        \n        alpha = self.extract(self.alphas.to(x_t.device), timestep, x_t.shape)\n        sqrt_alpha = self.extract(self.sqrt_alphas.to(x_t.device), timestep, x_t.shape)\n        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars.to(x_t.device), timestep, x_t.shape)\n        sqrt_beta = self.extract(self.sqrt_betas.to(x_t.device), timestep, x_t.shape)\n        \n        # denoise at time t, utilizing predicted noise\n        x_t_minus_1 = 1 / sqrt_alpha * (x_t - (1-alpha)/sqrt_one_minus_alpha_bar*epsilon_pred) + sqrt_beta*z\n        \n        return x_t_minus_1#.clamp(-1., 1)\n                \n    def sample(self, src, N):\n        # start from random noise vector, NxLx3\n        x_t = torch.randn((N, src.shape[1], 3)).to(src.device)\n        \n        # autoregressively denoise from x_T to x_0\n        #     i.e., generate image from noise, x_T\n\n        #first get conditioning\n        sequence_features, pairwise_features=self.get_embeddings(src, torch.ones_like(src).long().to(src.device))\n        # sequence_features=sequence_features.expand(N,-1,-1)\n        # pairwise_features=pairwise_features.expand(N,-1,-1,-1)\n        distogram=self.distogram_predictor(pairwise_features).squeeze()\n        distogram=distogram.squeeze()[:,:,2:40]*torch.arange(2,40).float().cuda() \n        distogram=distogram.sum(-1)  \n\n        for t in range(self.n_times-1, -1, -1):\n            timestep = torch.tensor([t]).repeat_interleave(N, dim=0).long().to(src.device)\n            x_t = self.denoise_at_t(x_t, sequence_features, pairwise_features, timestep, t)\n        \n        # denormalize x_0 into 0 ~ 1 ranged values.\n        #x_0 = self.reverse_scale_to_zero_to_one(x_t)\n        x_0 = x_t * self.data_std\n        return x_0, distogram\n\n\n\n\nclass SimpleStructureModule(nn.Module):\n\n    def __init__(self, d_model, nhead, \n                 dim_feedforward, pairwise_dimension, dropout=0.1,\n                 ):\n        super(SimpleStructureModule, self).__init__()\n        #self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.self_attn = MultiHeadAttention(d_model, nhead, d_model//nhead, d_model//nhead, dropout=dropout)\n        #self.cross_attn = MultiHeadAttention(d_model, nhead, d_model//nhead, d_model//nhead, dropout=dropout)\n\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.pairwise2heads=nn.Linear(pairwise_dimension,nhead,bias=False)\n        self.pairwise_norm=nn.LayerNorm(pairwise_dimension)\n\n        #self.distance2heads=nn.Linear(1,nhead,bias=False)\n        #self.pairwise_norm=nn.LayerNorm(pairwise_dimension)\n\n        self.activation = nn.GELU()\n\n        \n    def custom(self, module):\n        def custom_forward(*inputs):\n            inputs = module(*inputs)\n            return inputs\n        return custom_forward\n\n    def forward(self, input):\n        tgt , src,  pairwise_features, pred_t, src_mask = input\n        \n        #src = src*src_mask.float().unsqueeze(-1)\n\n        pairwise_bias=self.pairwise2heads(self.pairwise_norm(pairwise_features)).permute(0,3,1,2)\n\n        \n\n\n        #print(pairwise_bias.shape,distance_bias.shape)\n\n        #pairwise_bias=pairwise_bias+distance_bias\n\n\n        res=tgt\n        tgt,attention_weights = self.self_attn(tgt, tgt, tgt, mask=pairwise_bias, src_mask=src_mask)\n        tgt = res + self.dropout1(tgt)\n        tgt = self.norm1(tgt)\n\n        # print(tgt.shape,src.shape)\n        # exit()\n\n        res=tgt\n        tgt = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = res + self.dropout2(tgt)\n        tgt = self.norm2(tgt)\n\n\n        return tgt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:10.151583Z","iopub.execute_input":"2025-06-22T11:54:10.152336Z","iopub.status.idle":"2025-06-22T11:54:13.061937Z","shell.execute_reply.started":"2025-06-22T11:54:10.152281Z","shell.execute_reply":"2025-06-22T11:54:13.061182Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class MSAEnhancedRibonanzaNet(nn.Module):\n    def __init__(self, base_model, msa_embed_dim=64):\n        super().__init__()\n        self.base_model = base_model\n        self.msa_embed_dim = msa_embed_dim\n        \n        # MSA feature processors\n        self.conservation_embedder = nn.Sequential(\n            nn.Linear(1, msa_embed_dim),\n            nn.ReLU(),\n            nn.Linear(msa_embed_dim, msa_embed_dim)\n        )\n        \n        self.coevolution_processor = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(32, msa_embed_dim)\n        )\n        \n        # Integration layers - we'll set this dynamically later\n        self.feature_fusion = None\n        self.msa_embed_dim = msa_embed_dim\n        \n    def process_msa_features(self, sequence_features, msa_features):\n        \"\"\"Process and integrate MSA features\"\"\"\n        if msa_features is None or not msa_features.get('has_msa', False):\n            return sequence_features\n            \n        # Get batch size and sequence length from sequence_features\n        batch_size, seq_len, feat_dim = sequence_features.shape\n        \n        # Conservation features\n        conservation = msa_features['conservation'].to(sequence_features.device)\n        conservation_embed = self.conservation_embedder(conservation.unsqueeze(-1))\n        # Add batch dimension to match sequence_features\n        conservation_embed = conservation_embed.unsqueeze(0).expand(batch_size, -1, -1)\n        \n        # Coevolution features\n        coevolution = msa_features['coevolution'].to(sequence_features.device)\n        coevolution_embed = self.coevolution_processor(coevolution.unsqueeze(0).unsqueeze(0))\n        # coevolution_embed is now [1, 64] after the processor\n        # Expand to match batch and sequence dimensions\n        coevolution_embed = coevolution_embed.expand(batch_size, seq_len, -1)\n        \n        # Debug prints to verify dimensions\n        print(f\"sequence_features shape: {sequence_features.shape}\")\n        print(f\"conservation_embed shape: {conservation_embed.shape}\")\n        print(f\"coevolution_embed shape: {coevolution_embed.shape}\")\n        \n        # Create feature_fusion layer if it doesn't exist yet\n        if self.feature_fusion is None:\n            total_input_dim = feat_dim + self.msa_embed_dim * 2\n            print(f\"Creating feature_fusion layer: {total_input_dim} -> {feat_dim}\")\n            self.feature_fusion = nn.Sequential(\n                nn.Linear(total_input_dim, feat_dim),\n                nn.LayerNorm(feat_dim),\n                nn.ReLU()\n            ).to(sequence_features.device)\n        \n        # Fuse features\n        enhanced_features = torch.cat([\n            sequence_features, \n            conservation_embed, \n            coevolution_embed\n        ], dim=-1)\n        \n        print(f\"enhanced_features shape: {enhanced_features.shape}\")\n        \n        return self.feature_fusion(enhanced_features)\n    \n    def sample(self, src, N, msa_features=None):\n        \"\"\"Enhanced sampling with MSA features\"\"\"\n        # Temporarily modify the base model's get_embeddings method to include MSA\n        original_get_embeddings = self.base_model.get_embeddings\n        \n        def enhanced_get_embeddings(src, mask):\n            seq_feat, pair_feat = original_get_embeddings(src, mask)\n            seq_feat = self.process_msa_features(seq_feat, msa_features)\n            return seq_feat, pair_feat\n        \n        # Monkey patch the method\n        self.base_model.get_embeddings = enhanced_get_embeddings\n        \n        try:\n            result = self.base_model.sample(src, N)\n        finally:\n            # Restore original method\n            self.base_model.get_embeddings = original_get_embeddings\n        \n        return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:15.904490Z","iopub.execute_input":"2025-06-22T11:54:15.904893Z","iopub.status.idle":"2025-06-22T11:54:15.916425Z","shell.execute_reply.started":"2025-06-22T11:54:15.904869Z","shell.execute_reply":"2025-06-22T11:54:15.915686Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import yaml\n\nclass Config:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n        self.entries=entries\n\n    def print(self):\n        print(self.entries)\n\ndef load_config_from_yaml(file_path):\n    with open(file_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return Config(**config)\n\n\ndiffusion_config=load_config_from_yaml(\"/kaggle/input/ribonanzanet2-ddpm-v2/diffusion_config.yaml\")\nrnet_config=load_config_from_yaml(\"/kaggle/input/ribonanzanet2/pytorch/alpha/1/pairwise.yaml\")\n\nbase_model = finetuned_RibonanzaNet(rnet_config, diffusion_config).cuda()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:21.354663Z","iopub.execute_input":"2025-06-22T11:54:21.354944Z","iopub.status.idle":"2025-06-22T11:54:23.791990Z","shell.execute_reply.started":"2025-06-22T11:54:21.354921Z","shell.execute_reply":"2025-06-22T11:54:23.791161Z"}},"outputs":[{"name":"stdout","text":"constructing 48 ConvTransformerEncoderLayers\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"state_dict=torch.load(\"/kaggle/input/ribonanzanet2-ddpm-v2/RibonanzaNet-DDPM-v2.pt\",map_location='cpu')\n\n#get rid of module. from ddp state dict\nnew_state_dict={}\n\nfor key in state_dict:\n    new_state_dict[key[7:]]=state_dict[key]\n\nbase_model.load_state_dict(new_state_dict)\nmodel = MSAEnhancedRibonanzaNet(base_model).cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:23.896927Z","iopub.execute_input":"2025-06-22T11:54:23.897497Z","iopub.status.idle":"2025-06-22T11:54:29.229856Z","shell.execute_reply.started":"2025-06-22T11:54:23.897476Z","shell.execute_reply":"2025-06-22T11:54:29.229243Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Add this diagnostic code to check MSA availability\nimport os\n\nmsa_directory = \"/kaggle/input/stanford-rna-3d-folding/MSA_v2\"\n\nprint(f\"MSA directory exists: {os.path.exists(msa_directory)}\")\n\nif os.path.exists(msa_directory):\n    msa_files = os.listdir(msa_directory)\n    print(f\"Number of MSA files found: {len(msa_files)}\")\n    print(f\"First 10 MSA files: {msa_files[:10]}\")\n    \n    # Check what target IDs we're looking for\n    target_ids = test_data['target_id'].tolist()\n    print(f\"First 10 target IDs: {target_ids}\")\n    \n    # Check if any MSA files match our target IDs\n    matches = []\n    for target_id in target_ids:\n        expected_file = f\"{target_id}.MSA.fasta\"\n        if expected_file in msa_files:\n            matches.append(expected_file)\n    \n    print(f\"Matching MSA files found: {len(matches)}\")\n\nelse:\n    print(\"MSA directory does not exist!\")\n    print(\"Available directories in /kaggle/input/stanford-rna-3d-folding/:\")\n    if os.path.exists(\"/kaggle/input/stanford-rna-3d-folding/\"):\n        print(os.listdir(\"/kaggle/input/stanford-rna-3d-folding/\"))\n    else:\n        print(\"Stanford RNA 3D folding dataset not available!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:30.904250Z","iopub.execute_input":"2025-06-22T11:54:30.904961Z","iopub.status.idle":"2025-06-22T11:54:30.961629Z","shell.execute_reply.started":"2025-06-22T11:54:30.904934Z","shell.execute_reply":"2025-06-22T11:54:30.961010Z"}},"outputs":[{"name":"stdout","text":"MSA directory exists: True\nNumber of MSA files found: 2534\nFirst 10 MSA files: ['3JCS_6.MSA.fasta', '7MSF_R.MSA.fasta', '2OOM_B.MSA.fasta', '1ZDI_S.MSA.fasta', '5FJ1_H.MSA.fasta', '6UF1_C.MSA.fasta', '4V8A_AB.MSA.fasta', '6JDG_G.MSA.fasta', '5NCO_1.MSA.fasta', '5DI4_A.MSA.fasta']\nFirst 10 target IDs: ['9L5R_2', '9GFT_AU', '9L0R_K', '9GFT_A3', '9B2K_B', '9B0S_Et', '9J3T_B', '9LCR_B', '8KEB_A', '9L5S_5', '8VXZ_C', '9J6Y_E', '8QHU_5', '9GHF_Z', '9KPO_B', '9N2B_5', '9N2C_Pt', '9B1Y_4', '9G06_a', '9DE8_A', '9B83_C', '8ZMH_A', '9E2Y_F', '9DE7_A', '8Y9L_B', '9FIB_Y', '9J3R_B', '9DPB_C', '8XTP_A', '8ZTV_Y', '8Y9M_B', '8ZQ9_A', '8XTP_B', '9B89_C', '8SYK_C', '9FN3_B', '8QHU_3', '9DRS_C', '8XTR_A', '9LMF_F', '9DE6_B', '8SYK_B', '9DE6_A', '8R7N_A', '8K85_A', '9FCV_B', '9DPA_C', '9DE5_C', '8VZ6_S', '8YIG_C', '9B84_F', '9C8K_2', '9B0Q_AP', '9E2Z_F', '8Z8Q_B', '9E2W_F', '8KHH_A', '8Z8U_B', '8ZTU_Y', '9GCL_A', '8RRI_Ax', '9L5S_6', '9GCM_A', '8Z9K_B', '9MTY_C', '8QHU_7', '9GBW_R', '8T5O_A', '9DPL_C', '8WFA_B', '9ISV_A', '9AR6_B', '9DE5_D', '8ZDR_A', '8WFB_B', '9L5R_6', '9IS7_B', '9GC0_Q', '8YIH_C', '9HNY_CA', '8VK7_B', '8WF8_B', '8QHU_S4', '8ZAU_A', '9AR7_B', '9AR4_B', '8Y9N_B', '8QHU_4', '8RWG_C', '8YII_C', '9DCF_C', '9L5T_6', '8WF9_B', '9GBZ_R']\nMatching MSA files found: 60\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Comprehensive MSA file matching diagnostic\nimport os\nimport pandas as pd\n\n# Load your test data\ntest_data = pd.read_csv(\"/kaggle/input/validation-sequences-clean-csv/validation_sequences_clean.csv\")\nmsa_directory = \"/kaggle/input/stanford-rna-3d-folding/MSA_v2\"\n\nprint(\"=== MSA DIAGNOSTIC REPORT ===\\n\")\n\n# 1. Basic counts\nprint(f\"Total test sequences: {len(test_data)}\")\nprint(f\"MSA directory exists: {os.path.exists(msa_directory)}\")\n\nif os.path.exists(msa_directory):\n    msa_files = os.listdir(msa_directory)\n    print(f\"Total MSA files available: {len(msa_files)}\")\nelse:\n    print(\"ERROR: MSA directory not found!\")\n    exit()\n\n# 2. Get all target IDs from test data\ntarget_ids = test_data['target_id'].tolist()\nprint(f\"Unique target IDs in test data: {len(set(target_ids))}\")\n\n# 3. Show sample target IDs and expected MSA file names\nprint(f\"\\nFirst 10 target IDs: {target_ids[:10]}\")\nexpected_files = [f\"{tid}.MSA.fasta\" for tid in target_ids[:10]]\nprint(f\"Expected MSA files: {expected_files}\")\n\n# 4. Check exact matches\nexact_matches = []\nmissing_files = []\n\nfor target_id in target_ids:\n    expected_file = f\"{target_id}.MSA.fasta\"\n    if expected_file in msa_files:\n        exact_matches.append(target_id)\n    else:\n        missing_files.append(target_id)\n\nprint(f\"\\n=== EXACT MATCHING RESULTS ===\")\nprint(f\"Exact matches found: {len(exact_matches)}/{len(target_ids)}\")\nprint(f\"Missing MSA files: {len(missing_files)}\")\n\n# 5. Show some examples of what's missing\nif missing_files:\n    print(f\"\\nFirst 10 missing target IDs: {missing_files[:10]}\")\n    print(\"Expected files that are missing:\")\n    for tid in missing_files[:10]:\n        print(f\"  {tid}.MSA.fasta\")\n\n# 6. Look for partial matches or naming patterns\nprint(f\"\\n=== MSA FILE PATTERN ANALYSIS ===\")\nprint(\"Sample MSA files found:\")\nfor i, msa_file in enumerate(msa_files[:10]):\n    print(f\"  {msa_file}\")\n\n# 7. Try to find any MSA files that might match our target IDs with different patterns\npotential_matches = {}\nfor target_id in missing_files[:20]:  # Check first 20 missing ones\n    # Look for files that contain the target_id\n    matches = [f for f in msa_files if target_id in f]\n    if matches:\n        potential_matches[target_id] = matches\n\nif potential_matches:\n    print(f\"\\n=== POTENTIAL ALTERNATIVE MATCHES ===\")\n    for target_id, matches in potential_matches.items():\n        print(f\"{target_id} might match: {matches}\")\n\n# 8. Check if there are MSA files for target IDs we DO have\nprint(f\"\\n=== VERIFICATION OF FOUND MATCHES ===\")\nif exact_matches:\n    print(\"Confirmed MSA files exist for:\")\n    for i, target_id in enumerate(exact_matches[:10]):\n        msa_path = os.path.join(msa_directory, f\"{target_id}.MSA.fasta\")\n        exists = os.path.exists(msa_path)\n        print(f\"  {target_id}.MSA.fasta - exists: {exists}\")\n\n# 9. Check for case sensitivity issues\nprint(f\"\\n=== CASE SENSITIVITY CHECK ===\")\nmsa_files_lower = [f.lower() for f in msa_files]\ncase_matches = 0\nfor target_id in missing_files[:10]:\n    expected_lower = f\"{target_id}.msa.fasta\".lower()\n    if expected_lower in msa_files_lower:\n        case_matches += 1\n        print(f\"Case mismatch found for: {target_id}\")\n\nprint(f\"Potential case sensitivity issues: {case_matches}\")\n\n# 10. Summary\nprint(f\"\\n=== SUMMARY ===\")\nprint(f\"Test sequences: {len(test_data)}\")\nprint(f\"Available MSA files: {len(msa_files)}\")\nprint(f\"Exact matches: {len(exact_matches)}\")\nprint(f\"Missing: {len(missing_files)}\")\nprint(f\"Match rate: {len(exact_matches)/len(target_ids)*100:.1f}%\")\n\n# 11. Double-check the dataset logic\nprint(f\"\\n=== DATASET LOGIC VERIFICATION ===\")\nprint(\"Testing dataset logic with a few examples...\")\n\nclass TestDataset:\n    def __init__(self, data, msa_dir):\n        self.data = data\n        self.msa_dir = msa_dir\n    \n    def check_msa(self, idx):\n        target_id = self.data.loc[idx, 'target_id']\n        msa_path = os.path.join(self.msa_dir, f\"{target_id}.MSA.fasta\")\n        exists = os.path.exists(msa_path)\n        return target_id, msa_path, exists\n\ntest_dataset = TestDataset(test_data, msa_directory)\n\nfor i in range(min(5, len(test_data))):\n    target_id, msa_path, exists = test_dataset.check_msa(i)\n    print(f\"Index {i}: {target_id} -> {exists}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T11:54:34.866106Z","iopub.execute_input":"2025-06-22T11:54:34.866666Z","iopub.status.idle":"2025-06-22T11:54:34.971141Z","shell.execute_reply.started":"2025-06-22T11:54:34.866643Z","shell.execute_reply":"2025-06-22T11:54:34.970516Z"}},"outputs":[{"name":"stdout","text":"=== MSA DIAGNOSTIC REPORT ===\n\nTotal test sequences: 94\nMSA directory exists: True\nTotal MSA files available: 2534\nUnique target IDs in test data: 94\n\nFirst 10 target IDs: ['9L5R_2', '9GFT_AU', '9L0R_K', '9GFT_A3', '9B2K_B', '9B0S_Et', '9J3T_B', '9LCR_B', '8KEB_A', '9L5S_5']\nExpected MSA files: ['9L5R_2.MSA.fasta', '9GFT_AU.MSA.fasta', '9L0R_K.MSA.fasta', '9GFT_A3.MSA.fasta', '9B2K_B.MSA.fasta', '9B0S_Et.MSA.fasta', '9J3T_B.MSA.fasta', '9LCR_B.MSA.fasta', '8KEB_A.MSA.fasta', '9L5S_5.MSA.fasta']\n\n=== EXACT MATCHING RESULTS ===\nExact matches found: 60/94\nMissing MSA files: 34\n\nFirst 10 missing target IDs: ['8ZMH_A', '9E2Y_F', '9J3R_B', '9DPB_C', '8XTP_A', '8ZTV_Y', '8ZQ9_A', '8XTP_B', '8QHU_3', '9DRS_C']\nExpected files that are missing:\n  8ZMH_A.MSA.fasta\n  9E2Y_F.MSA.fasta\n  9J3R_B.MSA.fasta\n  9DPB_C.MSA.fasta\n  8XTP_A.MSA.fasta\n  8ZTV_Y.MSA.fasta\n  8ZQ9_A.MSA.fasta\n  8XTP_B.MSA.fasta\n  8QHU_3.MSA.fasta\n  9DRS_C.MSA.fasta\n\n=== MSA FILE PATTERN ANALYSIS ===\nSample MSA files found:\n  3JCS_6.MSA.fasta\n  7MSF_R.MSA.fasta\n  2OOM_B.MSA.fasta\n  1ZDI_S.MSA.fasta\n  5FJ1_H.MSA.fasta\n  6UF1_C.MSA.fasta\n  4V8A_AB.MSA.fasta\n  6JDG_G.MSA.fasta\n  5NCO_1.MSA.fasta\n  5DI4_A.MSA.fasta\n\n=== VERIFICATION OF FOUND MATCHES ===\nConfirmed MSA files exist for:\n  9L5R_2.MSA.fasta - exists: True\n  9GFT_AU.MSA.fasta - exists: True\n  9L0R_K.MSA.fasta - exists: True\n  9GFT_A3.MSA.fasta - exists: True\n  9B2K_B.MSA.fasta - exists: True\n  9B0S_Et.MSA.fasta - exists: True\n  9J3T_B.MSA.fasta - exists: True\n  9LCR_B.MSA.fasta - exists: True\n  8KEB_A.MSA.fasta - exists: True\n  9L5S_5.MSA.fasta - exists: True\n\n=== CASE SENSITIVITY CHECK ===\nPotential case sensitivity issues: 0\n\n=== SUMMARY ===\nTest sequences: 94\nAvailable MSA files: 2534\nExact matches: 60\nMissing: 34\nMatch rate: 63.8%\n\n=== DATASET LOGIC VERIFICATION ===\nTesting dataset logic with a few examples...\nIndex 0: 9L5R_2 -> True\nIndex 1: 9GFT_AU -> True\nIndex 2: 9L0R_K -> True\nIndex 3: 9GFT_A3 -> True\nIndex 4: 9B2K_B -> True\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from tqdm import tqdm\nmodel.eval()\npreds=[]\nmsa_usage_count = 0\n\nfor i in tqdm(range(len(test_dataset))):\n    sample = test_dataset[i]\n    src = sample['sequence'].long()\n    src = src.unsqueeze(0).cuda()\n    msa_features = sample['msa_features']\n    target_id = test_data.loc[i,'target_id']\n    \n    # Track MSA usage\n    if msa_features['has_msa']:\n        msa_usage_count += 1\n        if i < 10:  # Print info for first 10 samples\n            print(f\"Using MSA for {target_id}: depth={msa_features['msa_depth']}\")\n    \n    with torch.no_grad():\n        xyz, distogram = model.sample(src, 5, msa_features)\n\n    preds.append(xyz.cpu().numpy())\n\nprint(f\"\\nMSA Usage Summary: {msa_usage_count}/{len(test_dataset)} sequences had MSA data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:57:55.817467Z","iopub.execute_input":"2025-06-21T14:57:55.817808Z","iopub.status.idle":"2025-06-21T15:55:52.843588Z","shell.execute_reply.started":"2025-06-21T14:57:55.817783Z","shell.execute_reply":"2025-06-21T15:55:52.842863Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/94 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9L5R_2: depth=100\nsequence_features shape: torch.Size([1, 193, 384])\nconservation_embed shape: torch.Size([1, 193, 64])\ncoevolution_embed shape: torch.Size([1, 193, 64])\nCreating feature_fusion layer: 512 -> 384\nenhanced_features shape: torch.Size([1, 193, 512])\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 1/94 [00:37<58:24, 37.69s/it]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9GFT_AU: depth=100\nsequence_features shape: torch.Size([1, 76, 384])\nconservation_embed shape: torch.Size([1, 76, 64])\ncoevolution_embed shape: torch.Size([1, 76, 64])\nenhanced_features shape: torch.Size([1, 76, 512])\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 2/94 [00:48<33:12, 21.66s/it]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9L0R_K: depth=100\nsequence_features shape: torch.Size([1, 700, 384])\nconservation_embed shape: torch.Size([1, 700, 64])\ncoevolution_embed shape: torch.Size([1, 700, 64])\nenhanced_features shape: torch.Size([1, 700, 512])\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 3/94 [06:34<4:17:51, 170.01s/it]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9GFT_A3: depth=100\nsequence_features shape: torch.Size([1, 77, 384])\nconservation_embed shape: torch.Size([1, 77, 64])\ncoevolution_embed shape: torch.Size([1, 77, 64])\nenhanced_features shape: torch.Size([1, 77, 512])\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 4/94 [06:45<2:40:46, 107.18s/it]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9B2K_B: depth=100\nsequence_features shape: torch.Size([1, 70, 384])\nconservation_embed shape: torch.Size([1, 70, 64])\ncoevolution_embed shape: torch.Size([1, 70, 64])\nenhanced_features shape: torch.Size([1, 70, 512])\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 5/94 [06:55<1:46:54, 72.07s/it] ","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9B0S_Et: depth=100\nsequence_features shape: torch.Size([1, 75, 384])\nconservation_embed shape: torch.Size([1, 75, 64])\ncoevolution_embed shape: torch.Size([1, 75, 64])\nenhanced_features shape: torch.Size([1, 75, 512])\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 6/94 [07:05<1:14:57, 51.10s/it]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9J3T_B: depth=100\nsequence_features shape: torch.Size([1, 580, 384])\nconservation_embed shape: torch.Size([1, 580, 64])\ncoevolution_embed shape: torch.Size([1, 580, 64])\nenhanced_features shape: torch.Size([1, 580, 512])\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 7/94 [11:11<2:46:29, 114.82s/it]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9LCR_B: depth=100\nsequence_features shape: torch.Size([1, 578, 384])\nconservation_embed shape: torch.Size([1, 578, 64])\ncoevolution_embed shape: torch.Size([1, 578, 64])\nenhanced_features shape: torch.Size([1, 578, 512])\n","output_type":"stream"},{"name":"stderr","text":"  9%|▊         | 8/94 [15:14<3:42:45, 155.42s/it]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 8KEB_A: depth=100\nsequence_features shape: torch.Size([1, 72, 384])\nconservation_embed shape: torch.Size([1, 72, 64])\ncoevolution_embed shape: torch.Size([1, 72, 64])\nenhanced_features shape: torch.Size([1, 72, 512])\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 9/94 [15:24<2:35:50, 110.00s/it]","output_type":"stream"},{"name":"stdout","text":"Using MSA for 9L5S_5: depth=100\nsequence_features shape: torch.Size([1, 116, 384])\nconservation_embed shape: torch.Size([1, 116, 64])\ncoevolution_embed shape: torch.Size([1, 116, 64])\nenhanced_features shape: torch.Size([1, 116, 512])\n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 10/94 [15:43<1:54:50, 82.03s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 36, 384])\nconservation_embed shape: torch.Size([1, 36, 64])\ncoevolution_embed shape: torch.Size([1, 36, 64])\nenhanced_features shape: torch.Size([1, 36, 512])\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 11/94 [15:49<1:21:21, 58.82s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 550, 384])\nconservation_embed shape: torch.Size([1, 550, 64])\ncoevolution_embed shape: torch.Size([1, 550, 64])\nenhanced_features shape: torch.Size([1, 550, 512])\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 12/94 [19:13<2:20:24, 102.74s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 135, 384])\nconservation_embed shape: torch.Size([1, 135, 64])\ncoevolution_embed shape: torch.Size([1, 135, 64])\nenhanced_features shape: torch.Size([1, 135, 512])\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 13/94 [19:34<1:45:38, 78.25s/it] ","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 77, 384])\nconservation_embed shape: torch.Size([1, 77, 64])\ncoevolution_embed shape: torch.Size([1, 77, 64])\nenhanced_features shape: torch.Size([1, 77, 512])\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▍        | 14/94 [19:45<1:17:13, 57.92s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 255, 384])\nconservation_embed shape: torch.Size([1, 255, 64])\ncoevolution_embed shape: torch.Size([1, 255, 64])\nenhanced_features shape: torch.Size([1, 255, 512])\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 15/94 [20:42<1:15:47, 57.56s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 120, 384])\nconservation_embed shape: torch.Size([1, 120, 64])\ncoevolution_embed shape: torch.Size([1, 120, 64])\nenhanced_features shape: torch.Size([1, 120, 512])\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 16/94 [21:02<1:00:07, 46.26s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 77, 384])\nconservation_embed shape: torch.Size([1, 77, 64])\ncoevolution_embed shape: torch.Size([1, 77, 64])\nenhanced_features shape: torch.Size([1, 77, 512])\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 17/94 [21:13<45:42, 35.62s/it]  ","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 81, 384])\nconservation_embed shape: torch.Size([1, 81, 64])\ncoevolution_embed shape: torch.Size([1, 81, 64])\nenhanced_features shape: torch.Size([1, 81, 512])\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 18/94 [21:24<35:52, 28.32s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 73, 384])\nconservation_embed shape: torch.Size([1, 73, 64])\ncoevolution_embed shape: torch.Size([1, 73, 64])\nenhanced_features shape: torch.Size([1, 73, 512])\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 19/94 [21:35<28:35, 22.88s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 57, 384])\nconservation_embed shape: torch.Size([1, 57, 64])\ncoevolution_embed shape: torch.Size([1, 57, 64])\nenhanced_features shape: torch.Size([1, 57, 512])\n","output_type":"stream"},{"name":"stderr","text":" 21%|██▏       | 20/94 [21:43<22:44, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 39, 384])\nconservation_embed shape: torch.Size([1, 39, 64])\ncoevolution_embed shape: torch.Size([1, 39, 64])\nenhanced_features shape: torch.Size([1, 39, 512])\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 23/94 [22:19<16:46, 14.18s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 58, 384])\nconservation_embed shape: torch.Size([1, 58, 64])\ncoevolution_embed shape: torch.Size([1, 58, 64])\nenhanced_features shape: torch.Size([1, 58, 512])\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 24/94 [22:28<14:34, 12.50s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 62, 384])\nconservation_embed shape: torch.Size([1, 62, 64])\ncoevolution_embed shape: torch.Size([1, 62, 64])\nenhanced_features shape: torch.Size([1, 62, 512])\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 25/94 [22:36<12:52, 11.19s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 15, 384])\nconservation_embed shape: torch.Size([1, 15, 64])\ncoevolution_embed shape: torch.Size([1, 15, 64])\nenhanced_features shape: torch.Size([1, 15, 512])\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 30/94 [26:16<30:41, 28.78s/it]  ","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 62, 384])\nconservation_embed shape: torch.Size([1, 62, 64])\ncoevolution_embed shape: torch.Size([1, 62, 64])\nenhanced_features shape: torch.Size([1, 62, 512])\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 33/94 [29:53<1:12:41, 71.50s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 35, 384])\nconservation_embed shape: torch.Size([1, 35, 64])\ncoevolution_embed shape: torch.Size([1, 35, 64])\nenhanced_features shape: torch.Size([1, 35, 512])\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 34/94 [29:59<51:53, 51.89s/it]  ","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 107, 384])\nconservation_embed shape: torch.Size([1, 107, 64])\ncoevolution_embed shape: torch.Size([1, 107, 64])\nenhanced_features shape: torch.Size([1, 107, 512])\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 35/94 [30:16<40:36, 41.29s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 58, 384])\nconservation_embed shape: torch.Size([1, 58, 64])\ncoevolution_embed shape: torch.Size([1, 58, 64])\nenhanced_features shape: torch.Size([1, 58, 512])\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 38/94 [31:08<23:46, 25.48s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 145, 384])\nconservation_embed shape: torch.Size([1, 145, 64])\ncoevolution_embed shape: torch.Size([1, 145, 64])\nenhanced_features shape: torch.Size([1, 145, 512])\n","output_type":"stream"},{"name":"stderr","text":" 41%|████▏     | 39/94 [31:33<23:10, 25.28s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 700, 384])\nconservation_embed shape: torch.Size([1, 700, 64])\ncoevolution_embed shape: torch.Size([1, 700, 64])\nenhanced_features shape: torch.Size([1, 700, 512])\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 40/94 [37:22<1:50:07, 122.36s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 57, 384])\nconservation_embed shape: torch.Size([1, 57, 64])\ncoevolution_embed shape: torch.Size([1, 57, 64])\nenhanced_features shape: torch.Size([1, 57, 512])\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▎     | 41/94 [37:30<1:17:50, 88.12s/it] ","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 107, 384])\nconservation_embed shape: torch.Size([1, 107, 64])\ncoevolution_embed shape: torch.Size([1, 107, 64])\nenhanced_features shape: torch.Size([1, 107, 512])\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▍     | 42/94 [37:46<57:44, 66.63s/it]  ","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 57, 384])\nconservation_embed shape: torch.Size([1, 57, 64])\ncoevolution_embed shape: torch.Size([1, 57, 64])\nenhanced_features shape: torch.Size([1, 57, 512])\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 43/94 [37:54<41:43, 49.08s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 135, 384])\nconservation_embed shape: torch.Size([1, 135, 64])\ncoevolution_embed shape: torch.Size([1, 135, 64])\nenhanced_features shape: torch.Size([1, 135, 512])\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 45/94 [38:25<25:24, 31.12s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 81, 384])\nconservation_embed shape: torch.Size([1, 81, 64])\ncoevolution_embed shape: torch.Size([1, 81, 64])\nenhanced_features shape: torch.Size([1, 81, 512])\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 46/94 [38:36<20:08, 25.17s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 76, 384])\nconservation_embed shape: torch.Size([1, 76, 64])\ncoevolution_embed shape: torch.Size([1, 76, 64])\nenhanced_features shape: torch.Size([1, 76, 512])\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 47/94 [38:47<16:16, 20.78s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 57, 384])\nconservation_embed shape: torch.Size([1, 57, 64])\ncoevolution_embed shape: torch.Size([1, 57, 64])\nenhanced_features shape: torch.Size([1, 57, 512])\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████     | 48/94 [38:55<13:01, 16.98s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 50, 384])\nconservation_embed shape: torch.Size([1, 50, 64])\ncoevolution_embed shape: torch.Size([1, 50, 64])\nenhanced_features shape: torch.Size([1, 50, 512])\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 49/94 [39:02<10:34, 14.10s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 104, 384])\nconservation_embed shape: torch.Size([1, 104, 64])\ncoevolution_embed shape: torch.Size([1, 104, 64])\nenhanced_features shape: torch.Size([1, 104, 512])\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 51/94 [39:27<09:13, 12.88s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 27, 384])\nconservation_embed shape: torch.Size([1, 27, 64])\ncoevolution_embed shape: torch.Size([1, 27, 64])\nenhanced_features shape: torch.Size([1, 27, 512])\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 52/94 [39:33<07:30, 10.74s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 71, 384])\nconservation_embed shape: torch.Size([1, 71, 64])\ncoevolution_embed shape: torch.Size([1, 71, 64])\nenhanced_features shape: torch.Size([1, 71, 512])\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▋    | 53/94 [39:43<07:12, 10.56s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 40, 384])\nconservation_embed shape: torch.Size([1, 40, 64])\ncoevolution_embed shape: torch.Size([1, 40, 64])\nenhanced_features shape: torch.Size([1, 40, 512])\n","output_type":"stream"},{"name":"stderr","text":" 60%|█████▉    | 56/94 [40:05<05:22,  8.49s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 71, 384])\nconservation_embed shape: torch.Size([1, 71, 64])\ncoevolution_embed shape: torch.Size([1, 71, 64])\nenhanced_features shape: torch.Size([1, 71, 512])\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 60/94 [40:51<06:44, 11.89s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 70, 384])\nconservation_embed shape: torch.Size([1, 70, 64])\ncoevolution_embed shape: torch.Size([1, 70, 64])\nenhanced_features shape: torch.Size([1, 70, 512])\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▍   | 61/94 [41:00<06:12, 11.30s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 101, 384])\nconservation_embed shape: torch.Size([1, 101, 64])\ncoevolution_embed shape: torch.Size([1, 101, 64])\nenhanced_features shape: torch.Size([1, 101, 512])\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████▏  | 67/94 [42:31<07:15, 16.12s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 124, 384])\nconservation_embed shape: torch.Size([1, 124, 64])\ncoevolution_embed shape: torch.Size([1, 124, 64])\nenhanced_features shape: torch.Size([1, 124, 512])\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 68/94 [42:52<07:36, 17.55s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 76, 384])\nconservation_embed shape: torch.Size([1, 76, 64])\ncoevolution_embed shape: torch.Size([1, 76, 64])\nenhanced_features shape: torch.Size([1, 76, 512])\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 71/94 [46:09<23:58, 62.55s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 147, 384])\nconservation_embed shape: torch.Size([1, 147, 64])\ncoevolution_embed shape: torch.Size([1, 147, 64])\nenhanced_features shape: torch.Size([1, 147, 512])\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 72/94 [46:32<18:38, 50.84s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 57, 384])\nconservation_embed shape: torch.Size([1, 57, 64])\ncoevolution_embed shape: torch.Size([1, 57, 64])\nenhanced_features shape: torch.Size([1, 57, 512])\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 73/94 [46:41<13:19, 38.07s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 159, 384])\nconservation_embed shape: torch.Size([1, 159, 64])\ncoevolution_embed shape: torch.Size([1, 159, 64])\nenhanced_features shape: torch.Size([1, 159, 512])\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▊  | 74/94 [47:07<11:29, 34.47s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 66, 384])\nconservation_embed shape: torch.Size([1, 66, 64])\ncoevolution_embed shape: torch.Size([1, 66, 64])\nenhanced_features shape: torch.Size([1, 66, 512])\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 75/94 [47:16<08:29, 26.83s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 101, 384])\nconservation_embed shape: torch.Size([1, 101, 64])\ncoevolution_embed shape: torch.Size([1, 101, 64])\nenhanced_features shape: torch.Size([1, 101, 512])\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 78/94 [50:48<14:33, 54.59s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 96, 384])\nconservation_embed shape: torch.Size([1, 96, 64])\ncoevolution_embed shape: torch.Size([1, 96, 64])\nenhanced_features shape: torch.Size([1, 96, 512])\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 80/94 [54:19<20:44, 88.90s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 118, 384])\nconservation_embed shape: torch.Size([1, 118, 64])\ncoevolution_embed shape: torch.Size([1, 118, 64])\nenhanced_features shape: torch.Size([1, 118, 512])\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 82/94 [54:47<10:03, 50.29s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 76, 384])\nconservation_embed shape: torch.Size([1, 76, 64])\ncoevolution_embed shape: torch.Size([1, 76, 64])\nenhanced_features shape: torch.Size([1, 76, 512])\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████▏| 86/94 [55:45<03:14, 24.32s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 62, 384])\nconservation_embed shape: torch.Size([1, 62, 64])\ncoevolution_embed shape: torch.Size([1, 62, 64])\nenhanced_features shape: torch.Size([1, 62, 512])\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 87/94 [55:53<02:16, 19.46s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 184, 384])\nconservation_embed shape: torch.Size([1, 184, 64])\ncoevolution_embed shape: torch.Size([1, 184, 64])\nenhanced_features shape: torch.Size([1, 184, 512])\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▎| 88/94 [56:27<02:23, 23.99s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 121, 384])\nconservation_embed shape: torch.Size([1, 121, 64])\ncoevolution_embed shape: torch.Size([1, 121, 64])\nenhanced_features shape: torch.Size([1, 121, 512])\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 90/94 [57:01<01:20, 20.13s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 90, 384])\nconservation_embed shape: torch.Size([1, 90, 64])\ncoevolution_embed shape: torch.Size([1, 90, 64])\nenhanced_features shape: torch.Size([1, 90, 512])\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 91/94 [57:14<00:53, 17.94s/it]","output_type":"stream"},{"name":"stdout","text":"sequence_features shape: torch.Size([1, 101, 384])\nconservation_embed shape: torch.Size([1, 101, 64])\ncoevolution_embed shape: torch.Size([1, 101, 64])\nenhanced_features shape: torch.Size([1, 101, 512])\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 94/94 [57:57<00:00, 36.99s/it]","output_type":"stream"},{"name":"stdout","text":"\nMSA Usage Summary: 60/94 sequences had MSA data\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"ID=[]\nresname=[]\nresid=[]\nx=[]\ny=[]\nz=[]\n\ndata=[]\n\nfor i in range(len(test_data)):\n    #print(test_data.loc[i])\n\n    \n    for j in range(len(test_data.loc[i,'sequence'])):\n        # ID.append(test_data.loc[i,'sequence_id']+f\"_{j+1}\")\n        # resname.append(test_data.loc[i,'sequence'][j])\n        # resid.append(j+1) # 1 indexed\n        row=[test_data.loc[i,'target_id']+f\"_{j+1}\",\n             test_data.loc[i,'sequence'][j],\n             j+1]\n\n        for k in range(5):\n            for kk in range(3):\n                row.append(preds[i][k][j][kk])\n        data.append(row)\n\ncolumns=['ID','resname','resid']\nfor i in range(1,6):\n    columns+=[f\"x_{i}\"]\n    columns+=[f\"y_{i}\"]\n    columns+=[f\"z_{i}\"]\n\n\nsubmission=pd.DataFrame(data,columns=columns)\n\n\nsubmission\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T15:57:22.338417Z","iopub.execute_input":"2025-06-21T15:57:22.338970Z","iopub.status.idle":"2025-06-21T15:57:22.885163Z","shell.execute_reply.started":"2025-06-21T15:57:22.338952Z","shell.execute_reply":"2025-06-21T15:57:22.884573Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import shutil\nimport os\n\n# Copy USalign to working directory and make it executable\nshutil.copy2(\"/kaggle/input/usalign/USalign\", \"/kaggle/working/USalign\")\nos.chmod(\"/kaggle/working/USalign\", 0o755)\n\nprint(\"USalign copied to /kaggle/working/ and made executable\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T15:58:09.527676Z","iopub.execute_input":"2025-06-21T15:58:09.527941Z","iopub.status.idle":"2025-06-21T15:58:09.565739Z","shell.execute_reply.started":"2025-06-21T15:58:09.527923Z","shell.execute_reply":"2025-06-21T15:58:09.565004Z"}},"outputs":[{"name":"stdout","text":"USalign copied to /kaggle/working/ and made executable\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# score val\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\n\ndef parse_tmscore_output(output):\n    tm_score_match = re.findall(r'TM-score=\\s+([\\d.]+)', output)[1]\n    return float(tm_score_match)\n\ndef write_target_line(\n    atom_name, atom_serial, residue_name, chain_id, residue_num,\n    x_coord, y_coord, z_coord, occupancy=1.0, b_factor=0.0, atom_type='P'\n) -> str:\n    return (\n        f'ATOM  {atom_serial:>5d}  {atom_name:<5s} {residue_name:<3s} '\n        f'{residue_num:>3d}    {x_coord:>8.3f}{y_coord:>8.3f}'\n        f'{z_coord:>8.3f}{occupancy:>6.2f}{b_factor:>6.2f}           {atom_type}\\n'\n    )\n\ndef write2pdb(df: pd.DataFrame, xyz_id: int, target_path: str) -> int:\n    resolved_cnt = 0\n    with open(target_path, 'w') as f:\n        for _, row in df.iterrows():\n            x = row[f'x_{xyz_id}']; y = row[f'y_{xyz_id}']; z = row[f'z_{xyz_id}']\n            if x > -1e17 and y > -1e17 and z > -1e17:\n                resolved_cnt += 1\n                f.write(write_target_line(\n                    atom_name=\"C1'\", atom_serial=int(row['resid']),\n                    residue_name=row['resname'], chain_id='0',\n                    residue_num=int(row['resid']),\n                    x_coord=x, y_coord=y, z_coord=z, atom_type='C'\n                ))\n    return resolved_cnt\n\ndef get_base_target_id(long_id):\n    return \"_\".join(str(long_id).split(\"_\")[:-1])\n\ndef score_and_report(solution: pd.DataFrame, submission: pd.DataFrame):\n    solution['target_id'] = solution['ID'].apply(get_base_target_id)\n    submission['target_id'] = submission['ID'].apply(get_base_target_id)\n\n    native_idxs = sorted(int(c.split('_')[1])\n                         for c in solution.columns if c.startswith('x_'))\n\n    usalign = \"/kaggle/working/USalign\"\n    per_target = {}\n    \n    # Find common targets to iterate over\n    common_targets = sorted(list(set(solution['target_id'].unique()) & set(submission['target_id'].unique())))\n    \n    print(f\"Scoring {len(common_targets)} common targets...\")\n\n    for tid in common_targets:\n        grp_nat = solution[solution['target_id'] == tid]\n        grp_pred = submission[submission['target_id'] == tid]\n        best_of_five = []\n\n        for pred_cnt in range(1, 6):\n            best_for_this_pred = 0.0\n            for nat_cnt in native_idxs:\n                n_nat  = write2pdb(grp_nat,   nat_cnt,   'native.pdb')\n                n_pred = write2pdb(grp_pred,  pred_cnt, 'predicted.pdb')\n                if n_nat > 0 and n_pred > 0:\n                    out = os.popen(\n                        f'{usalign} predicted.pdb native.pdb -atom \" C1\\'\"'\n                    ).read()\n                    best_for_this_pred = max(best_for_this_pred,\n                                             parse_tmscore_output(out))\n            best_of_five.append(best_for_this_pred)\n\n        per_target[tid] = best_of_five\n        print(f\"{tid}: TM-scores per model = {best_of_five}, \"\n              f\"best = {max(best_of_five):.4f}\")\n    \n    # Calculate mean TM score\n    all_best_scores = [max(scores) for scores in per_target.values()]\n    mean_tm = np.mean(all_best_scores) if all_best_scores else 0.0\n    \n    return per_target, mean_tm\n\nsolution   = pd.read_csv(\n    \"/kaggle/input/validation-labels-clean-csv/validation_labels_clean.csv\"\n)\n\nper_target_scores, mean_tm = score_and_report(solution, submission)\nprint(f\"\\nMean TM-score: {mean_tm:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:15:41.423303Z","iopub.execute_input":"2025-06-21T17:15:41.423754Z","execution_failed":"2025-06-21T19:40:15.805Z"}},"outputs":[{"name":"stdout","text":"Scoring 94 common targets...\n8K85_A: TM-scores per model = [0.11851, 0.26332, 0.27056, 0.27635, 0.25619], best = 0.2763\n8KEB_A: TM-scores per model = [0.1412, 0.15859, 0.15244, 0.12496, 0.11153], best = 0.1586\n8KHH_A: TM-scores per model = [0.11734, 0.10212, 0.14044, 0.08531, 0.12467], best = 0.1404\n8QHU_3: TM-scores per model = [0.22859, 0.80691, 0.23213, 0.80075, 0.77708], best = 0.8069\n8QHU_4: TM-scores per model = [0.13733, 0.15631, 0.11052, 0.15348, 0.10492], best = 0.1563\n8QHU_5: TM-scores per model = [0.13008, 0.15257, 0.10187, 0.12257, 0.14358], best = 0.1526\n8QHU_7: TM-scores per model = [0.89972, 0.23369, 0.93436, 0.93088, 0.23213], best = 0.9344\n8QHU_S4: TM-scores per model = [0.10372, 0.13692, 0.10917, 0.14957, 0.11647], best = 0.1496\n8R7N_A: TM-scores per model = [0.10665, 0.10993, 0.12394, 0.07499, 0.13397], best = 0.1340\n8RRI_Ax: TM-scores per model = [0.14859, 0.13666, 0.1161, 0.09984, 0.14559], best = 0.1486\n8RWG_C: TM-scores per model = [0.12915, 0.16605, 0.14947, 0.16417, 0.12407], best = 0.1661\n8SYK_B: TM-scores per model = [0.12974, 0.12805, 0.14975, 0.15173, 0.1591], best = 0.1591\n8SYK_C: TM-scores per model = [0.12135, 0.12017, 0.12036, 0.1525, 0.11856], best = 0.1525\n8T5O_A: TM-scores per model = [0.13858, 0.10131, 0.12224, 0.12015, 0.14536], best = 0.1454\n8VK7_B: TM-scores per model = [0.09421, 0.12427, 0.13069, 0.13667, 0.10823], best = 0.1367\n8VXZ_C: TM-scores per model = [0.08883, 0.11916, 0.06867, 0.08932, 0.06909], best = 0.1192\n8VZ6_S: TM-scores per model = [0.10438, 0.0795, 0.10122, 0.13556, 0.12132], best = 0.1356\n8WF8_B: TM-scores per model = [0.1262, 0.1286, 0.12508, 0.15555, 0.13662], best = 0.1555\n8WF9_B: TM-scores per model = [0.13452, 0.16838, 0.16963, 0.15925, 0.08877], best = 0.1696\n8WFA_B: TM-scores per model = [0.13297, 0.10711, 0.13161, 0.1398, 0.14271], best = 0.1427\n8WFB_B: TM-scores per model = [0.0874, 0.07348, 0.07477, 0.06557, 0.08222], best = 0.0874\n8XTP_A: TM-scores per model = [0.26063, 0.26763, 0.2657, 0.27347, 0.20108], best = 0.2735\n8XTP_B: TM-scores per model = [0.18662, 0.18366, 0.16074, 0.18989, 0.16781], best = 0.1899\n8XTR_A: TM-scores per model = [0.11984, 0.09412, 0.10652, 0.1349, 0.08934], best = 0.1349\n8Y9L_B: TM-scores per model = [0.05632, 0.09688, 0.1043, 0.07872, 0.09927], best = 0.1043\n8Y9M_B: TM-scores per model = [0.08835, 0.08655, 0.10906, 0.08447, 0.09911], best = 0.1091\n8Y9N_B: TM-scores per model = [0.08897, 0.06787, 0.07758, 0.08351, 0.08168], best = 0.0890\n8YIG_C: TM-scores per model = [0.09976, 0.11784, 0.11145, 0.14669, 0.10809], best = 0.1467\n8YIH_C: TM-scores per model = [0.15279, 0.11288, 0.11952, 0.09188, 0.15064], best = 0.1528\n8YII_C: TM-scores per model = [0.58505, 0.56193, 0.5696, 0.1864, 0.57463], best = 0.5850\n8Z8Q_B: TM-scores per model = [0.16227, 0.48426, 0.48764, 0.16278, 0.48744], best = 0.4876\n8Z8U_B: TM-scores per model = [0.10001, 0.13069, 0.29048, 0.28103, 0.27706], best = 0.2905\n8Z9K_B: TM-scores per model = [0.2431, 0.24752, 0.12026, 0.11427, 0.24634], best = 0.2475\n8ZAU_A: TM-scores per model = [0.2816, 0.28244, 0.15413, 0.26861, 0.1275], best = 0.2824\n8ZDR_A: TM-scores per model = [0.10336, 0.14164, 0.12768, 0.17012, 0.13656], best = 0.1701\n8ZMH_A: TM-scores per model = [0.19882, 0.30455, 0.20115, 0.28388, 0.22561], best = 0.3045\n8ZQ9_A: TM-scores per model = [0.16516, 0.31763, 0.30307, 0.13314, 0.31673], best = 0.3176\n8ZTU_Y: TM-scores per model = [0.82583, 0.79693, 0.8331, 0.80749, 0.81505], best = 0.8331\n8ZTV_Y: TM-scores per model = [0.81633, 0.78022, 0.80684, 0.8135, 0.80781], best = 0.8163\n9AR4_B: TM-scores per model = [0.38573, 0.39989, 0.16904, 0.39671, 0.38326], best = 0.3999\n9AR6_B: TM-scores per model = [0.12395, 0.1075, 0.13469, 0.09712, 0.08592], best = 0.1347\n9AR7_B: TM-scores per model = [0.35132, 0.33558, 0.37217, 0.38696, 0.38015], best = 0.3870\n9B0Q_AP: TM-scores per model = [0.12218, 0.12654, 0.11093, 0.14137, 0.10502], best = 0.1414\n9B0S_Et: TM-scores per model = [0.13627, 0.11882, 0.11963, 0.11607, 0.13074], best = 0.1363\n9B1Y_4: TM-scores per model = [0.11009, 0.11095, 0.14265, 0.12029, 0.12493], best = 0.1426\n9B2K_B: TM-scores per model = [0.06283, 0.07257, 0.07592, 0.09513, 0.0857], best = 0.0951\n9B83_C: TM-scores per model = [0.03701, 0.05365, 0.06333, 0.05153, 0.05572], best = 0.0633\n9B84_F: TM-scores per model = [0.15894, 0.15725, 0.15427, 0.16865, 0.60281], best = 0.6028\n9B89_C: TM-scores per model = [0.05527, 0.10492, 0.0751, 0.08406, 0.10499], best = 0.1050\n9C8K_2: TM-scores per model = [0.05823, 0.06067, 0.10401, 0.09088, 0.04987], best = 0.1040\n9DCF_C: TM-scores per model = [0.12552, 0.12707, 0.11782, 0.09026, 0.1334], best = 0.1334\n9DE5_C: TM-scores per model = [0.14756, 0.12296, 0.08565, 0.12978, 0.0866], best = 0.1476\n9DE5_D: TM-scores per model = [0.10919, 0.10154, 0.08154, 0.09392, 0.08996], best = 0.1092\n9DE6_A: TM-scores per model = [0.12178, 0.09635, 0.12855, 0.09717, 0.10365], best = 0.1285\n9DE6_B: TM-scores per model = [0.11013, 0.15525, 0.10605, 0.11015, 0.13682], best = 0.1552\n9DE7_A: TM-scores per model = [0.11634, 0.12502, 0.09466, 0.11075, 0.0982], best = 0.1250\n9DE8_A: TM-scores per model = [0.12974, 0.10633, 0.10404, 0.09781, 0.11042], best = 0.1297\n9DPA_C: TM-scores per model = [0.14112, 0.13612, 0.09655, 0.10742, 0.15285], best = 0.1529\n9DPB_C: TM-scores per model = [0.44081, 0.18381, 0.43319, 0.44154, 0.19003], best = 0.4415\n9DPL_C: TM-scores per model = [0.10918, 0.09397, 0.11038, 0.11821, 0.10998], best = 0.1182\n9DRS_C: TM-scores per model = [0.22206, 0.8452, 0.82983, 0.83715, 0.83246], best = 0.8452\n9E2W_F: TM-scores per model = [0.08391, 0.10414, 0.10557, 0.07766, 0.17416], best = 0.1742\n9E2Y_F: TM-scores per model = [0.14165, 0.13398, 0.14324, 0.14538, 0.1389], best = 0.1454\n9E2Z_F: TM-scores per model = [0.08571, 0.04454, 0.0835, 0.07564, 0.07652], best = 0.0857\n9FCV_B: TM-scores per model = [0.10452, 0.09, 0.12359, 0.09702, 0.11455], best = 0.1236\n9FIB_Y: TM-scores per model = [0.04794, 0.0, 0.10515, 0.10516, 0.02157], best = 0.1052\n9FN3_B: TM-scores per model = [0.15558, 0.11924, 0.12382, 0.08695, 0.1119], best = 0.1556\n9G06_a: TM-scores per model = [0.13436, 0.11225, 0.12993, 0.15648, 0.11668], best = 0.1565\n9GBW_R: TM-scores per model = [0.13643, 0.15356, 0.14995, 0.26777, 0.12631], best = 0.2678\n9GBZ_R: TM-scores per model = [0.14349, 0.36713, 0.13696, 0.20158, 0.22523], best = 0.3671\n9GC0_Q: TM-scores per model = [0.10247, 0.19289, 0.11214, 0.1218, 0.23288], best = 0.2329\n9GCL_A: TM-scores per model = [0.17747, 0.0903, 0.19812, 0.23243, 0.08613], best = 0.2324\n9GCM_A: TM-scores per model = [0.10451, 0.12831, 0.38372, 0.30586, 0.13676], best = 0.3837\n9GFT_A3: TM-scores per model = [0.11507, 0.10777, 0.13035, 0.15241, 0.12083], best = 0.1524\n9GFT_AU: TM-scores per model = [0.15393, 0.13646, 0.11624, 0.13623, 0.11147], best = 0.1539\n9GHF_Z: TM-scores per model = [0.11298, 0.15804, 0.12356, 0.12958, 0.11548], best = 0.1580\n9HNY_CA: TM-scores per model = [0.3032, 0.39094, 0.22368, 0.20177, 0.39177], best = 0.3918\n9IS7_B: TM-scores per model = [0.13961, 0.1514, 0.161, 0.20473, 0.16849], best = 0.2047\n9ISV_A: TM-scores per model = [0.17176, 0.15234, 0.17936, 0.17087, 0.17281], best = 0.1794\n9J3R_B: TM-scores per model = [0.18939, 0.17691, 0.14307, 0.16323, 0.14347], best = 0.1894\n9J3T_B: TM-scores per model = [0.09167, 0.08689, 0.09498, 0.09223, 0.10097], best = 0.1010\n9J6Y_E: TM-scores per model = [0.10617, 0.11242, 0.10525, 0.09396, 0.12415], best = 0.1241\n9KPO_B: TM-scores per model = [0.103, 0.11861, 0.10794, 0.11713, 0.06785], best = 0.1186\n","output_type":"stream"}],"execution_count":null}]}