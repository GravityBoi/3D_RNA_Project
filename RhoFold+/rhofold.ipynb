{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-06-08T22:02:43.453619Z","iopub.status.busy":"2025-06-08T22:02:43.453303Z","iopub.status.idle":"2025-06-08T22:03:12.898581Z","shell.execute_reply":"2025-06-08T22:03:12.896681Z","shell.execute_reply.started":"2025-06-08T22:02:43.453591Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: ../input/requirements\n","Processing /kaggle/input/requirements/multimolecule-0.0.6-py3-none-any.whl (from -r ../input/requirements/requirements.txt (line 1))\n","Processing /kaggle/input/requirements/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r ../input/requirements/requirements.txt (line 2))\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r ../input/requirements/requirements.txt (line 3)) (4.47.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r ../input/requirements/requirements.txt (line 4)) (2.5.1+cu121)\n","Requirement already satisfied: python-box in /usr/local/lib/python3.10/dist-packages (from -r ../input/requirements/requirements.txt (line 5)) (7.3.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from -r ../input/requirements/requirements.txt (line 6)) (0.1.8)\n","Processing /kaggle/input/requirements/ml_collections-1.0.0-py3-none-any.whl (from -r ../input/requirements/requirements.txt (line 7))\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r ../input/requirements/requirements.txt (line 8)) (0.8.0)\n","Processing /kaggle/input/requirements/OpenMM-8.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from -r ../input/requirements/requirements.txt (line 9))\n","Processing /kaggle/input/requirements/ViennaRNA-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r ../input/requirements/requirements.txt (line 10))\n","Processing /kaggle/input/requirements/pytest_runner-6.0.1-py3-none-any.whl (from -r ../input/requirements/requirements.txt (line 11))\n","Processing /kaggle/input/requirements/deepspeed-0.16.4.tar.gz (from -r ../input/requirements/requirements.txt (line 12))\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r ../input/requirements/requirements.txt (line 13)) (3.20.3)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from multimolecule->-r ../input/requirements/requirements.txt (line 1)) (1.2.1)\n","Processing /kaggle/input/requirements/chanfig-0.0.107-py3-none-any.whl (from multimolecule->-r ../input/requirements/requirements.txt (line 1))\n","Processing /kaggle/input/requirements/danling-0.3.13-py3-none-any.whl (from danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1))\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from multimolecule->-r ../input/requirements/requirements.txt (line 1)) (3.3.1)\n","Requirement already satisfied: StrEnum in /usr/local/lib/python3.10/dist-packages (from multimolecule->-r ../input/requirements/requirements.txt (line 1)) (0.4.15)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from BioPython->-r ../input/requirements/requirements.txt (line 2)) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (0.29.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r ../input/requirements/requirements.txt (line 3)) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../input/requirements/requirements.txt (line 4)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r ../input/requirements/requirements.txt (line 4)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../input/requirements/requirements.txt (line 4)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r ../input/requirements/requirements.txt (line 4)) (2024.12.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../input/requirements/requirements.txt (line 4)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r ../input/requirements/requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections->-r ../input/requirements/requirements.txt (line 7)) (1.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections->-r ../input/requirements/requirements.txt (line 7)) (1.17.0)\n","Processing /kaggle/input/requirements/hjson-3.1.0-py3-none-any.whl (from deepspeed->-r ../input/requirements/requirements.txt (line 12))\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r ../input/requirements/requirements.txt (line 12)) (1.1.0)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r ../input/requirements/requirements.txt (line 12)) (1.11.1.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r ../input/requirements/requirements.txt (line 12)) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r ../input/requirements/requirements.txt (line 12)) (9.0.0)\n","Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r ../input/requirements/requirements.txt (line 12)) (2.11.0a2)\n","Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from danling>=0.3.11->danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (3.1.43)\n","Processing /kaggle/input/requirements/lazy_imports-0.4.0-py3-none-any.whl (from danling>=0.3.11->danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1))\n","Processing /kaggle/input/requirements/torcheval-0.0.7-py3-none-any.whl (from danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1))\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (1.6.1)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (2025.0.1)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (2022.0.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (2.4.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r ../input/requirements/requirements.txt (line 12)) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r ../input/requirements/requirements.txt (line 12)) (2.29.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (19.0.1)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (2.2.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (3.11.12)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r ../input/requirements/requirements.txt (line 3)) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r ../input/requirements/requirements.txt (line 3)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r ../input/requirements/requirements.txt (line 3)) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r ../input/requirements/requirements.txt (line 3)) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r ../input/requirements/requirements.txt (line 4)) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (5.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (1.18.3)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->danling>=0.3.11->danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (4.0.11)\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (2022.0.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (1.2.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (2024.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (2025.1)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics->danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (0.12.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->danling>=0.3.11->danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (5.0.1)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->BioPython->-r ../input/requirements/requirements.txt (line 2)) (2024.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics->danling[torch]>=0.3.11->multimolecule->-r ../input/requirements/requirements.txt (line 1)) (75.1.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.16.4-py3-none-any.whl size=1562530 sha256=b735bc642bb3600fab3b2e468c63179c7882a7dce94de2d40f2d06b48ef84687\n","  Stored in directory: /root/.cache/pip/wheels/0e/cf/5f/8a3c04b97b00603ccf19a153c1bbefc2a1a75a0bbe6673a8c9\n","Successfully built deepspeed\n","Installing collected packages: hjson, viennarna, torcheval, pytest-runner, ml_collections, lazy-imports, chanfig, danling, openmm, multimolecule, deepspeed, BioPython\n","Successfully installed BioPython-1.85 chanfig-0.0.107 danling-0.3.13 deepspeed-0.16.4 hjson-3.1.0 lazy-imports-0.4.0 ml_collections-1.0.0 multimolecule-0.0.6 openmm-8.2.0 pytest-runner-6.0.1 torcheval-0.0.7 viennarna-2.7.0\n"]}],"source":["# Requires the rhofold dependency library.\n","# Although it includes some unnecessary libraries, you can also use my experimental requirements.\n","# https://www.kaggle.com/code/shosukesuzuki/requirements\n","! python -m pip install --no-index --find-links=../input/requirements -r ../input/requirements/requirements.txt"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-06-08T22:03:19.147621Z","iopub.status.busy":"2025-06-08T22:03:19.147210Z","iopub.status.idle":"2025-06-08T22:03:19.811864Z","shell.execute_reply":"2025-06-08T22:03:19.810572Z","shell.execute_reply.started":"2025-06-08T22:03:19.147589Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import math\n","import time\n","import pickle\n","import subprocess\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from Bio.PDB import PDBParser\n","\n","from scipy.spatial import procrustes\n","from scipy.spatial.transform import Rotation\n","from scipy.linalg import orthogonal_procrustes\n","\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib.animation import FuncAnimation"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-06-08T22:03:23.258240Z","iopub.status.busy":"2025-06-08T22:03:23.257666Z","iopub.status.idle":"2025-06-08T22:03:23.262800Z","shell.execute_reply":"2025-06-08T22:03:23.261583Z","shell.execute_reply.started":"2025-06-08T22:03:23.258206Z"},"trusted":true},"outputs":[],"source":["# Please download and upload the checkpoint from the RhoFold repository.\n","ckpt_path = \"/kaggle/input/model-ckpt/RhoFold/pretrained/rhofold_pretrained_params.pt\"\n","\n","# The source code for RhoFold+ can be imported from Input → GitHub.\n","# I have named the repository 'rhofold'.\n","# When running, either import the source as 'rhofold' or adjust the corresponding parts accordingly."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-06-08T22:03:25.695157Z","iopub.status.busy":"2025-06-08T22:03:25.694780Z","iopub.status.idle":"2025-06-08T22:03:25.711400Z","shell.execute_reply":"2025-06-08T22:03:25.709859Z","shell.execute_reply.started":"2025-06-08T22:03:25.695128Z"},"trusted":true},"outputs":[],"source":["# merge multi segments for long sequences\n","def align_and_merge_segments(results, original_length, overlap=20):\n","    results = sorted(results, key=lambda x: x[0])\n","    \n","    full_coords = np.zeros((original_length, 3))\n","    weights = np.zeros(original_length)\n","    \n","    ref_start, ref_names, ref_ids, ref_coords = results[0]\n","    \n","    for i in range(len(ref_ids)):\n","        global_idx = ref_start + i\n","        if global_idx < original_length:\n","            full_coords[global_idx] = ref_coords[i]\n","            weights[global_idx] = 1.0\n","    \n","    for i in range(1, len(results)):\n","        curr_start, curr_names, curr_ids, curr_coords = results[i]\n","\n","        prev_start = results[i-1][0]\n","        prev_coords = results[i-1][3]\n","\n","        overlap_start = max(curr_start, prev_start)\n","        prev_end = prev_start + len(prev_coords)\n","        overlap_end = min(curr_start + len(curr_coords), prev_end)\n","\n","        if overlap_end > overlap_start:\n","            prev_overlap_indices = [j - prev_start for j in range(overlap_start, overlap_end)]\n","            curr_overlap_indices = [j - curr_start for j in range(overlap_start, overlap_end)]\n","            \n","            prev_overlap_coords = prev_coords[prev_overlap_indices]\n","            curr_overlap_coords = curr_coords[curr_overlap_indices]\n","            \n","            if len(prev_overlap_coords) >= 3 and len(curr_overlap_coords) >= 3:\n","                _, curr_overlap_aligned, transform_scale = procrustes(prev_overlap_coords, curr_overlap_coords)\n","\n","                mtx1, mtx2, disparity = procrustes(prev_overlap_coords, curr_overlap_coords)\n","\n","                aligned_coords = curr_coords.copy()\n","                \n","                curr_centroid = np.mean(curr_overlap_coords, axis=0)\n","                prev_centroid = np.mean(prev_overlap_coords, axis=0)\n","                translation = prev_centroid - curr_centroid\n","                \n","                curr_centered = curr_coords - curr_centroid\n","\n","                _, _, rotation_matrix, scale = _get_procrustes_transformation(prev_overlap_coords, curr_overlap_coords)\n","                aligned_coords = scale * (curr_centered @ rotation_matrix.T) + prev_centroid\n","                \n","                for j in range(len(curr_ids)):\n","                    global_idx = curr_start + j\n","                    if global_idx < original_length:\n","                        in_overlap = global_idx >= overlap_start and global_idx < overlap_end\n","                        if in_overlap:\n","                            pos_in_overlap = (global_idx - overlap_start) / (overlap_end - overlap_start)\n","                            weight = 1.0 - pos_in_overlap\n","                            if weights[global_idx] > 0:\n","                                full_coords[global_idx] = full_coords[global_idx] * weights[global_idx] + aligned_coords[j] * weight\n","                                full_coords[global_idx] /= (weights[global_idx] + weight)\n","                                weights[global_idx] += weight\n","                            else:\n","                                full_coords[global_idx] = aligned_coords[j]\n","                                weights[global_idx] = weight\n","                        elif weights[global_idx] == 0:\n","                            full_coords[global_idx] = aligned_coords[j]\n","                            weights[global_idx] = 1.0\n","            else:\n","                for j in range(len(curr_ids)):\n","                    global_idx = curr_start + j\n","                    if global_idx < original_length and weights[global_idx] == 0:\n","                        full_coords[global_idx] = curr_coords[j]\n","                        weights[global_idx] = 1.0\n","        else:\n","            for j in range(len(curr_ids)):\n","                global_idx = curr_start + j\n","                if global_idx < original_length and weights[global_idx] == 0:\n","                    full_coords[global_idx] = curr_coords[j]\n","                    weights[global_idx] = 1.0\n","    \n","    missing_indices = np.where(weights == 0)[0]\n","    if len(missing_indices) > 0:\n","        valid_indices = np.where(weights > 0)[0]\n","        if len(valid_indices) > 0:\n","            for dim in range(3):\n","                full_coords[missing_indices, dim] = np.interp(\n","                    missing_indices, valid_indices, full_coords[valid_indices, dim]\n","                )\n","    \n","    return full_coords\n","\n","\n","def _get_procrustes_transformation(target, source):\n","    target_centroid = np.mean(target, axis=0)\n","    source_centroid = np.mean(source, axis=0)\n","    \n","    target_centered = target - target_centroid\n","    source_centered = source - source_centroid\n","    \n","    rotation_matrix, _ = orthogonal_procrustes(source_centered, target_centered)\n","    \n","    target_norm = np.linalg.norm(target_centered)\n","    source_norm = np.linalg.norm(source_centered)\n","    scale = target_norm / source_norm if source_norm > 0 else 1.0\n","    \n","    return target_centroid, source_centroid, rotation_matrix, scale"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-06-08T22:04:23.038348Z","iopub.status.busy":"2025-06-08T22:04:23.037974Z","iopub.status.idle":"2025-06-08T22:04:23.151597Z","shell.execute_reply":"2025-06-08T22:04:23.150420Z","shell.execute_reply.started":"2025-06-08T22:04:23.038321Z"},"trusted":true},"outputs":[],"source":["# run rhofold inference helper\n","def create_fasta_files(df, fasta_dir):\n","    \"\"\"Create individual FASTA files for each target_id\"\"\"\n","    print(f\"Creating FASTA files in {fasta_dir}...\")\n","    os.makedirs(fasta_dir, exist_ok=True)\n","    for idx, row in df.iterrows():\n","        target_id = row[\"target_id\"]\n","        sequence = row[\"sequence\"]\n","        fasta_path = os.path.join(fasta_dir, f\"{target_id}.fasta\")\n","        with open(fasta_path, \"w\") as f:\n","            f.write(f\">{target_id}\\n{sequence}\\n\")\n","    print(f\"Created {len(df)} FASTA files.\")\n","    gc.collect()\n","\n","\n","def run_rhofold(target_id, fasta_path, output_dir, ckpt_path):\n","    \"\"\"Run RhoFold on a single FASTA file.\"\"\"\n","    with open(fasta_path, 'r') as f:\n","        f.readline()\n","        sequence = f.readline().strip()\n","    sequence_length = len(sequence)\n","    device = \"cpu\"\n","    \n","    result_dir = os.path.join(output_dir, target_id)\n","    os.makedirs(result_dir, exist_ok=True)\n","    \n","    abs_fasta_path = os.path.abspath(fasta_path)\n","    abs_result_dir = os.path.abspath(result_dir)\n","    abs_checkpoint = os.path.abspath(ckpt_path)\n","    \n","    current_dir = os.getcwd()\n","    cmd = [\n","        \"python\", \"inference.py\",\n","        \"--input_fas\", abs_fasta_path,\n","        \"--single_seq_pred\", \"True\",\n","        \"--output_dir\", abs_result_dir,\n","        \"--device\", device,\n","        \"--ckpt\", abs_checkpoint,\n","        \"--relax_steps\", \"0\",\n","    ]\n","    \n","    print(f\"Running RhoFold on {target_id} (length: {sequence_length}, device: {device})...\")\n","    start_time = time.time()\n","    try:\n","        os.chdir(\"/kaggle/input/rhofold\")\n","        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n","        print(f\"  Completed in {time.time() - start_time:.2f}s\")\n","        print(f\"  RhoFold stdout: {result.stdout[:200]}...\" if len(result.stdout) > 200 else f\"  RhoFold stdout: {result.stdout}\")\n","        success = True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"  Error running RhoFold on {target_id}:\")\n","        print(f\"  RhoFold stderr: {e.stderr}\")\n","        success = False\n","    except Exception as e:\n","        print(f\"  Unexpected error running RhoFold on {target_id}: {str(e)}\")\n","        success = False\n","    finally:\n","        os.chdir(current_dir)\n","        if 'result' in locals():\n","            del result\n","        gc.collect()\n","    return success\n","\n","\n","def extract_c1_coordinates(pdb_file):\n","    \"\"\"Extract C1' coordinates from PDB file\"\"\"\n","    parser = PDBParser(QUIET=True)\n","    c1_coordinates = []\n","    residue_names = []\n","    residue_ids = []\n","    try:\n","        structure = parser.get_structure('RNA_structure', pdb_file)\n","        for model in structure:\n","            for chain in model:\n","                for residue in chain:\n","                    if residue.get_resname() in ['A', 'U', 'G', 'C']:\n","                        try:\n","                            c1_atom = residue[\"C1'\"]\n","                            residue_names.append(residue.get_resname())\n","                            residue_ids.append(residue.get_id()[1])\n","                            c1_coordinates.append(c1_atom.get_coord())\n","                        except KeyError:\n","                            print(f\"C1' atom not found in residue {residue.get_resname()}{residue.get_id()[1]}\")\n","        coords_array = np.array(c1_coordinates)\n","        del structure\n","        gc.collect()\n","        return residue_names, residue_ids, coords_array\n","    except Exception as e:\n","        print(f\"Error parsing PDB file {pdb_file}: {e}\")\n","        return [], [], np.array([])\n","\n","\n","def interpolate_coordinates(coords, target_length):\n","    \"\"\"Interpolate coordinates to target length\"\"\"\n","    n = coords.shape[0]\n","    full_coords = np.empty((target_length, 3))\n","    old_indices = np.linspace(0, 1, n)\n","    new_indices = np.linspace(0, 1, target_length)\n","    for dim in range(3):\n","        full_coords[:, dim] = np.interp(new_indices, old_indices, coords[:, dim])\n","    return full_coords\n","\n","\n","def generate_variants(base_coords, n_variants=5, noise_scale=0.5):\n","    \"\"\"Generate conformation variants\"\"\"\n","    variants = [None] * n_variants\n","    variants[0] = base_coords.copy()\n","    \n","    for i in range(1, n_variants):\n","        noise = np.random.normal(scale=noise_scale, size=base_coords.shape)\n","        variants[i] = base_coords + noise\n","    \n","    return variants\n","\n","\n","def run_rhofold_with_truncation(target_id, sequence, fasta_dir, output_dir, ckpt_path):\n","    \"\"\"Run RhoFold directly without truncation, using random coordinates as fallback\"\"\"\n","    original_length = len(sequence)\n","    \n","    temp_fasta = os.path.join(fasta_dir, f\"{target_id}_full.fasta\")\n","    with open(temp_fasta, \"w\") as f:\n","        f.write(f\">{target_id}\\n{sequence}\\n\")\n","    \n","    print(f\"Running inference for {target_id} with full sequence length {original_length}...\")\n","    \n","    # Try running RhoFold once with full sequence\n","    if run_rhofold(target_id, temp_fasta, output_dir, ckpt_path):\n","        pdb_file = os.path.join(output_dir, target_id, \"unrelaxed_model.pdb\")\n","        if Path(pdb_file).exists():\n","            res_names, res_ids, coords = extract_c1_coordinates(pdb_file)\n","            if coords.size > 0:\n","                print(f\"Successfully predicted structure for full sequence.\")\n","                os.remove(temp_fasta)\n","                return res_names, res_ids, coords\n","    \n","    # If failed, use random coordinates\n","    print(f\"Inference failed for {target_id}. Using random coordinates.\")\n","    if os.path.exists(temp_fasta):\n","        os.remove(temp_fasta)\n","    \n","    # Generate random coordinates with reasonable RNA dimensions\n","    random_coords = np.random.rand(original_length, 3) * 20  # Scale to typical RNA dimensions\n","    res_names_full = list(sequence)\n","    res_ids_full = list(range(1, original_length + 1))\n","    \n","    return res_names_full, res_ids_full, random_coords\n","\n","\n","def run_rhofold_with_multi_segment(target_id, sequence, fasta_dir, output_dir, ckpt_path, overlap=20):\n","    \"\"\"Split sequence into multiple segments and merge results, ensuring each segment is manageable\"\"\"\n","    original_length = len(sequence)\n","    \n","    # Calculate optimal number of segments to keep each segment under 100bp (including overlap)\n","    max_segment_size = 200\n","    effective_segment_size = max_segment_size - (2 * overlap)\n","    n_segments = max(3, math.ceil(original_length / effective_segment_size))\n","    \n","    # Recalculate segment size with fixed number of segments\n","    segment_length = original_length // n_segments\n","    \n","    print(f\"Using multi-segment approach for {target_id} with {n_segments} segments, each ~{segment_length}bp + {overlap}bp overlap\")\n","    \n","    # Calculate segment boundaries\n","    segments = []\n","    start_positions = []\n","    \n","    for i in range(n_segments):\n","        start = max(0, i * segment_length - overlap)\n","        end = min(original_length, (i + 1) * segment_length + overlap)\n","        current_segment = sequence[start:end]\n","        \n","        # Ensure segment is not too long\n","        if len(current_segment) > max_segment_size:\n","            print(f\"Segment {i+1} is {len(current_segment)}bp, trimming to {max_segment_size}bp\")\n","            middle = len(current_segment) // 2\n","            half_max = max_segment_size // 2\n","            current_segment = current_segment[max(0, middle-half_max):min(len(current_segment), middle+half_max)]\n","            # Adjust start position\n","            start = start + max(0, middle-half_max)\n","        \n","        segments.append(current_segment)\n","        start_positions.append(start)\n","        print(f\"Segment {i+1}: positions {start}-{start+len(current_segment)} (length: {len(current_segment)}bp)\")\n","    \n","    # Process each segment\n","    results = []\n","    \n","    for i, (seg, start_pos) in enumerate(zip(segments, start_positions)):\n","        seg_id = f\"{target_id}_seg{i+1}\"\n","        print(f\"Processing segment {i+1}/{n_segments} for {target_id} (length: {len(seg)})\")\n","        \n","        # Create segment FASTA file\n","        seg_fasta = os.path.join(fasta_dir, f\"{seg_id}.fasta\")\n","        with open(seg_fasta, \"w\") as f:\n","            f.write(f\">{seg_id}\\n{seg}\\n\")\n","        \n","        # Try running RhoFold on this segment\n","        if run_rhofold(seg_id, seg_fasta, output_dir, ckpt_path):\n","            pdb_file = os.path.join(output_dir, seg_id, \"unrelaxed_model.pdb\")\n","            if Path(pdb_file).exists():\n","                res_names, res_ids, coords = extract_c1_coordinates(pdb_file)\n","                if coords.size > 0:\n","                    results.append((start_pos, res_names, res_ids, coords))\n","                else:\n","                    print(f\"No coordinates extracted for segment {i+1}\")\n","                    # Generate random coordinates for this segment\n","                    random_coords = np.random.rand(len(seg), 3) * 20\n","                    res_names = list(seg)\n","                    res_ids = list(range(1, len(seg) + 1))\n","                    results.append((start_pos, res_names, res_ids, random_coords))\n","            else:\n","                print(f\"PDB file not found for segment {i+1}\")\n","                # Generate random coordinates for this segment\n","                random_coords = np.random.rand(len(seg), 3) * 20\n","                res_names = list(seg)\n","                res_ids = list(range(1, len(seg) + 1))\n","                results.append((start_pos, res_names, res_ids, random_coords))\n","        else:\n","            print(f\"RhoFold failed for segment {i+1}\")\n","            # Generate random coordinates for this segment\n","            random_coords = np.random.rand(len(seg), 3) * 20\n","            res_names = list(seg)\n","            res_ids = list(range(1, len(seg) + 1))\n","            results.append((start_pos, res_names, res_ids, random_coords))\n","        \n","        # Clean up segment FASTA\n","        if os.path.exists(seg_fasta):\n","            os.remove(seg_fasta)\n","        \n","        gc.collect()\n","    \n","    # integrate setments\n","    try:\n","        print(f\"Aligning and merging {len(results)} segments...\")\n","        full_coords = align_and_merge_segments(results, original_length, overlap)\n","        print(f\"Successfully merged segments using procrustes alignment\")\n","    except Exception as e:\n","        print(f\"Error during segment alignment: {e}\")\n","        print(f\"Falling back to simple weighted average merging\")\n","        \n","        full_coords = np.zeros((original_length, 3))\n","        weights = np.zeros(original_length)\n","        \n","        for start_pos, res_names, res_ids, coords in results:\n","            # Map coordinates to their positions in the full sequence\n","            for i in range(len(res_ids)):\n","                global_idx = start_pos + i\n","                if global_idx < original_length:\n","                    # For overlap regions, use weighted averaging\n","                    if weights[global_idx] > 0:\n","                        # Calculate weight based on distance from segment edge\n","                        seg_len = len(res_ids)\n","                        edge_dist = min(i, seg_len - i)\n","                        edge_weight = min(1.0, edge_dist / overlap) if overlap > 0 else 1.0\n","                        \n","                        # Update weighted average\n","                        full_coords[global_idx] = (full_coords[global_idx] * weights[global_idx] + \n","                                                 coords[i] * edge_weight) / (weights[global_idx] + edge_weight)\n","                        weights[global_idx] += edge_weight\n","                    else:\n","                        full_coords[global_idx] = coords[i]\n","                        weights[global_idx] = 1.0\n","    \n","        # Check for missing positions\n","        missing_count = np.sum(weights == 0)\n","        if missing_count > 0:\n","            print(f\"Warning: {missing_count} positions have no coordinates. Filling with interpolation.\")\n","            \n","            # Try to fill small gaps first\n","            for _ in range(3):  # Repeat a few times to fill larger gaps\n","                valid_mask = weights > 0\n","                for i in range(1, original_length-1):\n","                    if weights[i] == 0 and weights[i-1] > 0 and weights[i+1] > 0:\n","                        full_coords[i] = (full_coords[i-1] + full_coords[i+1]) / 2\n","                        weights[i] = 0.5\n","            \n","            # Fill any remaining gaps with global interpolation\n","            valid_indices = np.where(weights > 0)[0]\n","            zero_indices = np.where(weights == 0)[0]\n","            \n","            if len(valid_indices) > 0 and len(zero_indices) > 0:\n","                for dim in range(3):\n","                    full_coords[zero_indices, dim] = np.interp(\n","                        zero_indices, valid_indices, full_coords[valid_indices, dim]\n","                    )\n","            else:\n","                print(f\"Error: Cannot interpolate coordinates. Using random model as fallback.\")\n","                full_coords = np.random.rand(original_length, 3) * 20\n","    \n","    res_names_full = list(sequence)\n","    res_ids_full = list(range(1, original_length + 1))\n","    \n","    return res_names_full, res_ids_full, full_coords\n","\n","\n","def create_submission_dataframe(target_ids, all_results):\n","    \"\"\"Create submission DataFrame from results with 5 conformation variants\"\"\"\n","    rows = []\n","    for target_id in target_ids:\n","        if target_id not in all_results:\n","            print(f\"Warning: No results for {target_id}\")\n","            continue\n","        \n","        res_names, res_ids, variants = all_results[target_id]\n","        \n","        for i in range(len(res_ids)):\n","            row = {\n","                \"ID\": f\"{target_id}_{res_ids[i]}\",\n","                \"resname\": res_names[i],\n","                \"resid\": res_ids[i]\n","            }\n","            \n","            for model in range(1, 6):\n","                variant_idx = model - 1\n","                if variant_idx < len(variants):\n","                    variant_coords = variants[variant_idx]\n","                    if i < len(variant_coords):\n","                        row[f\"x_{model}\"] = variant_coords[i][0]\n","                        row[f\"y_{model}\"] = variant_coords[i][1]\n","                        row[f\"z_{model}\"] = variant_coords[i][2]\n","            \n","            rows.append(row)\n","    \n","    df_submission = pd.DataFrame.from_records(rows)\n","    return df_submission"]},{"cell_type":"markdown","metadata":{},"source":["## Run Inference (multi-segment run)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-06-08T22:21:29.076817Z","iopub.status.busy":"2025-06-08T22:21:29.076372Z","iopub.status.idle":"2025-06-08T22:21:29.088244Z","shell.execute_reply":"2025-06-08T22:21:29.086885Z","shell.execute_reply.started":"2025-06-08T22:21:29.076779Z"},"trusted":true},"outputs":[],"source":["def load_ground_truth_structures(df_labels):\n","    \"\"\"Load ground truth structures from validation labels.\"\"\"\n","    ground_truth = {}\n","    \n","    # Group by target_id (extract from ID field)\n","    for target_id in df_labels['ID'].str.split('_').str[0].unique():\n","        target_data = df_labels[df_labels['ID'].str.startswith(target_id + '_')]\n","        target_data = target_data.sort_values('resid')  # Ensure correct order\n","        \n","        # Extract coordinates (using x_1, y_1, z_1 as primary structure)\n","        coords = target_data[['x_1', 'y_1', 'z_1']].values\n","        ground_truth[target_id] = coords\n","        print(f\"Loaded ground truth for {target_id}: {len(coords)} residues\")\n","    \n","    return ground_truth\n","\n","def calculate_tm_score_fallback(pred_coords, ref_coords):\n","    \"\"\"Calculate TM-score using simplified formula with Procrustes alignment.\"\"\"\n","    try:\n","        if len(pred_coords) != len(ref_coords):\n","            print(f\"Length mismatch: pred={len(pred_coords)}, ref={len(ref_coords)}\")\n","            return None\n","        \n","        pred_coords = np.array(pred_coords)\n","        ref_coords = np.array(ref_coords)\n","        \n","        if len(pred_coords) == 1:\n","            distance = np.linalg.norm(pred_coords[0] - ref_coords[0])\n","            d0 = max(1.24 * max(1, 1 - 15) ** (1/3) - 1.8, 0.5)\n","            return 1 / (1 + (distance / d0) ** 2)\n","        \n","        # Center both structures\n","        pred_centered = pred_coords - np.mean(pred_coords, axis=0)\n","        ref_centered = ref_coords - np.mean(ref_coords, axis=0)\n","        \n","        # Procrustes alignment\n","        try:\n","            H = pred_centered.T @ ref_centered\n","            U, _, Vt = np.linalg.svd(H)\n","            R = Vt.T @ U.T\n","            if np.linalg.det(R) < 0:\n","                Vt[-1, :] *= -1\n","                R = Vt.T @ U.T\n","            pred_aligned = pred_centered @ R\n","            distances = np.linalg.norm(pred_aligned - ref_centered, axis=1)\n","        except:\n","            distances = np.linalg.norm(pred_centered - ref_centered, axis=1)\n","        \n","        # TM-score formula\n","        L_target = len(ref_coords)\n","        d0 = max(1.24 * max(1, L_target - 15) ** (1/3) - 1.8, 0.5)\n","        tm_score = np.mean(1 / (1 + (distances / d0) ** 2))\n","        return tm_score\n","    except Exception as e:\n","        print(f\"Error calculating TM-score: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-06-08T22:21:57.739435Z","iopub.status.busy":"2025-06-08T22:21:57.739036Z","iopub.status.idle":"2025-06-08T22:21:58.387970Z","shell.execute_reply":"2025-06-08T22:21:58.386588Z","shell.execute_reply.started":"2025-06-08T22:21:57.739405Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting RNA folding pipeline... (segment length: 400)\n","Creating FASTA files in test_fasta...\n","Created 12 FASTA files.\n","Loaded 10 results from checkpoint\n","\n","[1/2] Processing R1189 (length: 118)...\n","Sequence length 118 < 400, using direct method\n","Running inference for R1189 with full sequence length 118...\n","Running RhoFold on R1189 (length: 118, device: cpu)...\n","  Unexpected error running RhoFold on R1189: [Errno 2] No such file or directory: '/kaggle/input/rhofold'\n","Inference failed for R1189. Using random coordinates.\n","Completed R1189: 118 residues\n","\n","[2/2] Processing R1190 (length: 118)...\n","Sequence length 118 < 400, using direct method\n","Running inference for R1190 with full sequence length 118...\n","Running RhoFold on R1190 (length: 118, device: cpu)...\n","  Unexpected error running RhoFold on R1190: [Errno 2] No such file or directory: '/kaggle/input/rhofold'\n","Inference failed for R1190. Using random coordinates.\n","Completed R1190: 118 residues\n","\n","Loading ground truth structures...\n","Loaded ground truth for R1107: 69 residues\n","Loaded ground truth for R1108: 69 residues\n","Loaded ground truth for R1116: 157 residues\n","Loaded ground truth for R1117v2: 30 residues\n","Loaded ground truth for R1126: 363 residues\n","Loaded ground truth for R1128: 238 residues\n","Loaded ground truth for R1136: 374 residues\n","Loaded ground truth for R1138: 720 residues\n","Loaded ground truth for R1149: 124 residues\n","Loaded ground truth for R1156: 135 residues\n","Loaded ground truth for R1189: 118 residues\n","Loaded ground truth for R1190: 118 residues\n","\n","Calculating TM-scores against ground truth...\n","  R1107 variant 1: TM = 0.0357\n","  R1107 variant 2: TM = 0.0324\n","  R1107 variant 3: TM = 0.0330\n","  R1107 variant 4: TM = 0.0349\n","  R1107 variant 5: TM = 0.0380\n","  R1107: Best TM = 0.0380, Mean TM = 0.0348\n","  R1108 variant 1: TM = 0.0378\n","  R1108 variant 2: TM = 0.0347\n","  R1108 variant 3: TM = 0.0254\n","  R1108 variant 4: TM = 0.0290\n","  R1108 variant 5: TM = 0.0330\n","  R1108: Best TM = 0.0378, Mean TM = 0.0320\n","  R1116 variant 1: TM = 0.0000\n","  R1116 variant 2: TM = 0.0000\n","  R1116 variant 3: TM = 0.0000\n","  R1116 variant 4: TM = 0.0000\n","  R1116 variant 5: TM = 0.0000\n","  R1116: Best TM = 0.0000, Mean TM = 0.0000\n","  R1117v2 variant 1: TM = 0.0000\n","  R1117v2 variant 2: TM = 0.0000\n","  R1117v2 variant 3: TM = 0.0000\n","  R1117v2 variant 4: TM = 0.0000\n","  R1117v2 variant 5: TM = 0.0000\n","  R1117v2: Best TM = 0.0000, Mean TM = 0.0000\n","  R1126 variant 1: TM = 0.0371\n","  R1126 variant 2: TM = 0.0379\n","  R1126 variant 3: TM = 0.0376\n","  R1126 variant 4: TM = 0.0404\n","  R1126 variant 5: TM = 0.0379\n","  R1126: Best TM = 0.0404, Mean TM = 0.0382\n","  R1128 variant 1: TM = 0.0453\n","  R1128 variant 2: TM = 0.0457\n","  R1128 variant 3: TM = 0.0420\n","  R1128 variant 4: TM = 0.0438\n","  R1128 variant 5: TM = 0.0431\n","  R1128: Best TM = 0.0457, Mean TM = 0.0440\n","  R1136 variant 1: TM = 0.0472\n","  R1136 variant 2: TM = 0.0466\n","  R1136 variant 3: TM = 0.0446\n","  R1136 variant 4: TM = 0.0469\n","  R1136 variant 5: TM = 0.0502\n","  R1136: Best TM = 0.0502, Mean TM = 0.0471\n","  R1138 variant 1: TM = 0.0534\n","  R1138 variant 2: TM = 0.0534\n","  R1138 variant 3: TM = 0.0535\n","  R1138 variant 4: TM = 0.0539\n","  R1138 variant 5: TM = 0.0540\n","  R1138: Best TM = 0.0540, Mean TM = 0.0536\n","  R1149 variant 1: TM = 0.0313\n","  R1149 variant 2: TM = 0.0319\n","  R1149 variant 3: TM = 0.0326\n","  R1149 variant 4: TM = 0.0328\n","  R1149 variant 5: TM = 0.0324\n","  R1149: Best TM = 0.0328, Mean TM = 0.0322\n","  R1156 variant 1: TM = 0.0391\n","  R1156 variant 2: TM = 0.0360\n","  R1156 variant 3: TM = 0.0362\n","  R1156 variant 4: TM = 0.0426\n","  R1156 variant 5: TM = 0.0377\n","  R1156: Best TM = 0.0426, Mean TM = 0.0383\n","  R1189 variant 1: TM = 0.0272\n","  R1189 variant 2: TM = 0.0290\n","  R1189 variant 3: TM = 0.0275\n","  R1189 variant 4: TM = 0.0288\n","  R1189 variant 5: TM = 0.0260\n","  R1189: Best TM = 0.0290, Mean TM = 0.0277\n","  R1190 variant 1: TM = 0.0287\n","  R1190 variant 2: TM = 0.0253\n","  R1190 variant 3: TM = 0.0297\n","  R1190 variant 4: TM = 0.0280\n","  R1190 variant 5: TM = 0.0278\n","  R1190: Best TM = 0.0297, Mean TM = 0.0279\n","\n","============================================================\n","GROUND TRUTH TM-SCORE RESULTS\n","============================================================\n","Targets evaluated: 12\n","Overall mean TM-score (all variants): 0.0313 ± 0.0159\n","Mean of best TM-scores: 0.0333\n","Best TM-scores: ['0.038', '0.038', '0.000', '0.000', '0.040', '0.046', '0.050', '0.054', '0.033', '0.043', '0.029', '0.030']\n","\n","Competition Score (average of best-of-5): 0.0333\n","\n","Quality distribution (best scores):\n","  High quality (TM > 0.5): 0/12 (0.0%)\n","  Medium quality (0.3-0.5): 0/12 (0.0%)\n","  Low quality (TM < 0.3): 12/12 (100.0%)\n","\n","============================================================\n","GROUND TRUTH EVALUATION COMPLETE\n","============================================================\n"]}],"source":["# inference setup\n","fasta_dir = \"test_fasta\"\n","output_dir = \"rhofold_output\"\n","\n","os.makedirs(fasta_dir, exist_ok=True)\n","os.makedirs(output_dir, exist_ok=True)\n","\n","segment_length = 400\n","print(f\"Starting RNA folding pipeline... (segment length: {segment_length})\")\n","\n","df = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv\")\n","df_labels = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding/validation_labels.csv\")\n","create_fasta_files(df, fasta_dir)\n","\n","# Process all sequences\n","all_results = {}\n","checkpoint_file = \"results_checkpoint.pkl\"\n","\n","# Load checkpoint if exists\n","if os.path.exists(checkpoint_file):\n","    try:\n","        with open(checkpoint_file, 'rb') as f:\n","            all_results = pickle.load(f)\n","        print(f\"Loaded {len(all_results)} results from checkpoint\")\n","    except:\n","        print(\"Failed to load checkpoint. Starting fresh.\")\n","\n","# Process remaining sequences\n","target_ids = df[\"target_id\"].tolist()\n","remaining_ids = [id for id in target_ids if id not in all_results]\n","\n","for idx, target_id in enumerate(remaining_ids):\n","    sequence = df[df[\"target_id\"] == target_id][\"sequence\"].values[0]\n","    print(f\"\\n[{idx+1}/{len(remaining_ids)}] Processing {target_id} (length: {len(sequence)})...\")\n","    \n","    try:\n","        # Select appropriate method based on sequence length\n","        if len(sequence) >= segment_length:\n","            print(f\"Sequence length {len(sequence)} >= {segment_length}, using multi-segment method\")\n","            res_names, res_ids, base_coords = run_rhofold_with_multi_segment(\n","                target_id, sequence, fasta_dir, output_dir, ckpt_path\n","            )\n","        else:\n","            print(f\"Sequence length {len(sequence)} < {segment_length}, using direct method\")\n","            res_names, res_ids, base_coords = run_rhofold_with_truncation(\n","                target_id, sequence, fasta_dir, output_dir, ckpt_path\n","            )\n","        \n","        # Generate 5 different conformation variants\n","        variants = generate_variants(base_coords, n_variants=5, noise_scale=2.5)\n","        all_results[target_id] = (res_names, res_ids, variants)\n","        \n","        print(f\"Completed {target_id}: {len(res_ids)} residues\")\n","        \n","        # Clean up temp files\n","        for f in os.listdir(fasta_dir):\n","            if f.startswith(f\"{target_id}_\") and f.endswith(\".fasta\"):\n","                try:\n","                    os.remove(os.path.join(fasta_dir, f))\n","                except:\n","                    pass\n","    except Exception as e:\n","        print(f\"Error processing {target_id}: {e}\")\n","        # Create fallback result in case of errors\n","        print(f\"Using random coordinates as fallback for {target_id}\")\n","        res_names = list(sequence)\n","        res_ids = list(range(1, len(sequence) + 1))\n","        random_coords = np.random.rand(len(sequence), 3) * 20\n","        variants = [random_coords.copy() + np.random.normal(scale=0.5, size=random_coords.shape) for _ in range(5)]\n","        all_results[target_id] = (res_names, res_ids, variants)\n","    \n","    # Save checkpoint after each sequence\n","    if (idx + 1) % 5 == 0:\n","        with open(checkpoint_file, 'wb') as f:\n","            pickle.dump(all_results, f)\n","        print(f\"Saved checkpoint with {len(all_results)}/{len(df)} sequences processed\")\n","    \n","    gc.collect()\n","\n","# Load ground truth structures\n","print(\"\\nLoading ground truth structures...\")\n","ground_truth = load_ground_truth_structures(df_labels)\n","\n","# Calculate TM-scores against ground truth\n","print(\"\\nCalculating TM-scores against ground truth...\")\n","real_tm_scores = []\n","results_summary = []\n","\n","for target_id, (res_names, res_ids, variants) in all_results.items():\n","    if target_id in ground_truth:\n","        ref_coords = ground_truth[target_id]\n","        \n","        # Calculate TM-scores for all 5 variants against ground truth\n","        tm_scores = []\n","        for i, variant in enumerate(variants):\n","            tm_score = calculate_tm_score_fallback(variant, ref_coords)\n","            if tm_score is not None:\n","                tm_scores.append(tm_score)\n","                print(f\"  {target_id} variant {i+1}: TM = {tm_score:.4f}\")\n","        \n","        if tm_scores:\n","            best_tm = max(tm_scores)\n","            mean_tm = np.mean(tm_scores)\n","            \n","            results_summary.append({\n","                'target_id': target_id,\n","                'best_tm_score': best_tm,\n","                'mean_tm_score': mean_tm,\n","                'sequence_length': len(res_names)\n","            })\n","            \n","            real_tm_scores.extend(tm_scores)\n","            \n","            print(f\"  {target_id}: Best TM = {best_tm:.4f}, Mean TM = {mean_tm:.4f}\")\n","        else:\n","            print(f\"  Failed to calculate TM-scores for {target_id}\")\n","    else:\n","        print(f\"  No ground truth found for {target_id}\")\n","\n","# Calculate overall statistics\n","if real_tm_scores:\n","    overall_mean = np.mean(real_tm_scores)\n","    overall_std = np.std(real_tm_scores)\n","    best_tm_scores = [result['best_tm_score'] for result in results_summary]\n","    mean_of_best = np.mean(best_tm_scores)\n","    \n","    print(f\"\\n{'='*60}\")\n","    print(\"GROUND TRUTH TM-SCORE RESULTS\")\n","    print(f\"{'='*60}\")\n","    print(f\"Targets evaluated: {len(results_summary)}\")\n","    print(f\"Overall mean TM-score (all variants): {overall_mean:.4f} ± {overall_std:.4f}\")\n","    print(f\"Mean of best TM-scores: {mean_of_best:.4f}\")\n","    print(f\"Best TM-scores: {[f'{x:.3f}' for x in best_tm_scores]}\")\n","    \n","    # Competition-style scoring (best of 5 per target)\n","    print(f\"\\nCompetition Score (average of best-of-5): {mean_of_best:.4f}\")\n","    \n","    # Quality analysis\n","    high_quality = len([x for x in best_tm_scores if x > 0.5])\n","    medium_quality = len([x for x in best_tm_scores if 0.3 <= x <= 0.5])\n","    low_quality = len([x for x in best_tm_scores if x < 0.3])\n","    \n","    print(f\"\\nQuality distribution (best scores):\")\n","    print(f\"  High quality (TM > 0.5): {high_quality}/{len(best_tm_scores)} ({high_quality/len(best_tm_scores)*100:.1f}%)\")\n","    print(f\"  Medium quality (0.3-0.5): {medium_quality}/{len(best_tm_scores)} ({medium_quality/len(best_tm_scores)*100:.1f}%)\")\n","    print(f\"  Low quality (TM < 0.3): {low_quality}/{len(best_tm_scores)} ({low_quality/len(best_tm_scores)*100:.1f}%)\")\n","else:\n","    print(\"No TM-scores calculated against ground truth!\")\n","\n","print(f\"\\n{'='*60}\")\n","print(\"GROUND TRUTH EVALUATION COMPLETE\")\n","print(f\"{'='*60}\")\n","\n","# Create submission file\n","submission_df = create_submission_dataframe(df[\"target_id\"].tolist(), all_results)"]},{"cell_type":"markdown","metadata":{},"source":["## Check Predicted RNA Structures"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:09:22.201720Z","iopub.status.busy":"2025-05-14T21:09:22.201367Z","iopub.status.idle":"2025-05-14T21:09:22.211478Z","shell.execute_reply":"2025-05-14T21:09:22.209944Z","shell.execute_reply.started":"2025-05-14T21:09:22.201691Z"},"trusted":true},"outputs":[],"source":["# check predicted structures (5 conformations)\n","def plot_rna_structure(rna_id):\n","    filtered_df = submission_df[submission_df['ID'].str.startswith(f\"{rna_id}_\")]\n","    \n","    if len(filtered_df) == 0:\n","        print(f\"ID '{rna_id} is not found/\")\n","        return\n","\n","    base_colors = {'A': 'red', 'U': 'blue', 'G': 'green', 'C': 'orange'}\n","    fig = plt.figure(figsize=(20, 15))\n","    for variant in range(1, 6):\n","        ax = fig.add_subplot(2, 3, variant, projection='3d')\n","        \n","        x = filtered_df[f'x_{variant}'].values\n","        y = filtered_df[f'y_{variant}'].values\n","        z = filtered_df[f'z_{variant}'].values\n","        \n","        colors = [base_colors.get(base, 'gray') for base in filtered_df['resname']]\n","        \n","        ax.scatter(x, y, z, c=colors, s=5, alpha=1.0)\n","        ax.plot(x, y, z, 'k-', alpha=1.0, linewidth=0.1)\n","        \n","        ax.set_xlabel('X')\n","        ax.set_ylabel('Y')\n","        ax.set_zlabel('Z')\n","        \n","        ax.set_title(f'Random Noise {variant} - {rna_id}')\n","        for base, color in base_colors.items():\n","            ax.scatter([], [], [], c=color, label=base)\n","        ax.legend()\n","    \n","    plt.suptitle(f'RNA Structure: {rna_id} (Length: {len(filtered_df)})', fontsize=16)\n","    plt.tight_layout()\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:09:26.284704Z","iopub.status.busy":"2025-05-14T21:09:26.284331Z","iopub.status.idle":"2025-05-14T21:09:28.014229Z","shell.execute_reply":"2025-05-14T21:09:28.013020Z","shell.execute_reply.started":"2025-05-14T21:09:26.284675Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1107')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:09:31.718131Z","iopub.status.busy":"2025-05-14T21:09:31.717721Z","iopub.status.idle":"2025-05-14T21:09:33.341634Z","shell.execute_reply":"2025-05-14T21:09:33.338804Z","shell.execute_reply.started":"2025-05-14T21:09:31.718098Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1108')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:09:36.486783Z","iopub.status.busy":"2025-05-14T21:09:36.486444Z","iopub.status.idle":"2025-05-14T21:09:38.290595Z","shell.execute_reply":"2025-05-14T21:09:38.288875Z","shell.execute_reply.started":"2025-05-14T21:09:36.486757Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1116')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:09:42.063895Z","iopub.status.busy":"2025-05-14T21:09:42.063566Z","iopub.status.idle":"2025-05-14T21:09:43.656559Z","shell.execute_reply":"2025-05-14T21:09:43.655085Z","shell.execute_reply.started":"2025-05-14T21:09:42.063868Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1117v2')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:00.413579Z","iopub.status.busy":"2025-05-14T21:10:00.413191Z","iopub.status.idle":"2025-05-14T21:10:02.300606Z","shell.execute_reply":"2025-05-14T21:10:02.298133Z","shell.execute_reply.started":"2025-05-14T21:10:00.413546Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1126')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:06.098545Z","iopub.status.busy":"2025-05-14T21:10:06.098030Z","iopub.status.idle":"2025-05-14T21:10:08.013146Z","shell.execute_reply":"2025-05-14T21:10:08.011730Z","shell.execute_reply.started":"2025-05-14T21:10:06.098509Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1128')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:13.462738Z","iopub.status.busy":"2025-05-14T21:10:13.462276Z","iopub.status.idle":"2025-05-14T21:10:15.315316Z","shell.execute_reply":"2025-05-14T21:10:15.313990Z","shell.execute_reply.started":"2025-05-14T21:10:13.462593Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1136')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:20.012830Z","iopub.status.busy":"2025-05-14T21:10:20.012499Z","iopub.status.idle":"2025-05-14T21:10:21.903290Z","shell.execute_reply":"2025-05-14T21:10:21.901894Z","shell.execute_reply.started":"2025-05-14T21:10:20.012804Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1138')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:26.356800Z","iopub.status.busy":"2025-05-14T21:10:26.356485Z","iopub.status.idle":"2025-05-14T21:10:27.924611Z","shell.execute_reply":"2025-05-14T21:10:27.922885Z","shell.execute_reply.started":"2025-05-14T21:10:26.356776Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1149')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:32.280133Z","iopub.status.busy":"2025-05-14T21:10:32.279706Z","iopub.status.idle":"2025-05-14T21:10:33.863476Z","shell.execute_reply":"2025-05-14T21:10:33.862085Z","shell.execute_reply.started":"2025-05-14T21:10:32.280099Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1156')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:37.461337Z","iopub.status.busy":"2025-05-14T21:10:37.460921Z","iopub.status.idle":"2025-05-14T21:10:39.093546Z","shell.execute_reply":"2025-05-14T21:10:39.092131Z","shell.execute_reply.started":"2025-05-14T21:10:37.461307Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1189')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:43.230847Z","iopub.status.busy":"2025-05-14T21:10:43.230502Z","iopub.status.idle":"2025-05-14T21:10:44.793165Z","shell.execute_reply":"2025-05-14T21:10:44.791074Z","shell.execute_reply.started":"2025-05-14T21:10:43.230819Z"},"trusted":true},"outputs":[],"source":["plot_rna_structure('R1190')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Check Submission Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:48.844556Z","iopub.status.busy":"2025-05-14T21:10:48.844196Z","iopub.status.idle":"2025-05-14T21:10:48.882552Z","shell.execute_reply":"2025-05-14T21:10:48.881440Z","shell.execute_reply.started":"2025-05-14T21:10:48.844530Z"},"trusted":true},"outputs":[],"source":["# submission data\n","submission_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-14T21:10:51.961908Z","iopub.status.busy":"2025-05-14T21:10:51.961471Z","iopub.status.idle":"2025-05-14T21:10:52.042318Z","shell.execute_reply":"2025-05-14T21:10:52.041125Z","shell.execute_reply.started":"2025-05-14T21:10:51.961872Z"},"trusted":true},"outputs":[],"source":["# Save submission\n","submission_path = \"/kaggle/working/submission.csv\"\n","submission_df.to_csv(submission_path, index=False)\n","print(f\"Saved submission to {submission_path} with {len(submission_df)} rows\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":12276181,"sourceId":87793,"sourceType":"competition"},{"datasetId":6813663,"sourceId":10953211,"sourceType":"datasetVersion"},{"datasetId":6813673,"sourceId":10953224,"sourceType":"datasetVersion"},{"sourceId":227750413,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
