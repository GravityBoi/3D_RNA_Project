{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87793,"databundleVersionId":12276181,"sourceType":"competition"},{"sourceId":10855324,"sourceType":"datasetVersion","datasetId":6742586},{"sourceId":10880419,"sourceType":"datasetVersion","datasetId":6760509},{"sourceId":11065669,"sourceType":"datasetVersion","datasetId":6889817},{"sourceId":11752460,"sourceType":"datasetVersion","datasetId":7356947},{"sourceId":11913080,"sourceType":"datasetVersion","datasetId":7489544},{"sourceId":12199811,"sourceType":"datasetVersion","datasetId":7684811},{"sourceId":12199817,"sourceType":"datasetVersion","datasetId":7684816}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os,sys\n\nimport pandas as pd\npd.set_option('display.max_columns', 20)\npd.set_option('display.expand_frame_repr', False)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom timeit import default_timer as timer\nimport re\nimport optuna\nimport matplotlib \nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nimport os, sys, shutil\n\n# 1) make sure you have a writable copy\nshutil.copytree(\n  '/kaggle/input/drfold2/DRfold2',\n  '/kaggle/working/drfold',\n  dirs_exist_ok=True\n)\n\n# 2) point Python at the cfg_97 folder under that copy\nBASE = '/kaggle/working/drfold'\nsys.path.insert(0, os.path.join(BASE, 'cfg_97'))\nfrom EvoMSA2XYZ import MSA2XYZ\nfrom RNALM2.Model import RNA2nd\nfrom data import parse_seq, Get_base\nfrom util import outpdb\n# parse_pdb_to_xyz write_frame_coor_to_pdb\n\nimport numpy as np\n\n\n# helper--\nclass dotdict(dict):\n\t__setattr__ = dict.__setitem__\n\t__delattr__ = dict.__delitem__\n\n\tdef __getattr__(self, name):\n\t\ttry:\n\t\t\treturn self[name]\n\t\texcept KeyError:\n\t\t\traise AttributeError(name)\n\ndef time_to_str(t, mode='min'):\n\tif mode=='min':\n\t\tt  = int(t)/60\n\t\thr = t//60\n\t\tmin = t%60\n\t\treturn '%2d hr %02d min'%(hr,min) \n\telif mode=='sec':\n\t\tt   = int(t)\n\t\tmin = t//60\n\t\tsec = t%60\n\t\treturn '%2d min %02d sec'%(min,sec)\n\n\telse:\n\t\traise NotImplementedError\n\ndef gpu_memory_use():\n    if torch.cuda.is_available():\n        device = torch.device(0)\n        free, total = torch.cuda.mem_get_info(device)\n        used= (total - free) / 1024 ** 3\n        return int(round(used))\n    else:\n        return 0\n\ndef set_aspect_equal(ax):\n\tx_limits = ax.get_xlim()\n\ty_limits = ax.get_ylim()\n\tz_limits = ax.get_zlim()\n\n\t# Compute the mean of each axis\n\tx_middle = np.mean(x_limits)\n\ty_middle = np.mean(y_limits)\n\tz_middle = np.mean(z_limits)\n\n\t# Compute the max range across all axes\n\tmax_range = max(x_limits[1] - x_limits[0],\n\t\t\t\t\ty_limits[1] - y_limits[0],\n\t\t\t\t\tz_limits[1] - z_limits[0]) / 2.0\n\n\t# Set the new limits to ensure equal scaling\n\tax.set_xlim(x_middle - max_range, x_middle + max_range)\n\tax.set_ylim(y_middle - max_range, y_middle + max_range)\n\tax.set_zlim(z_middle - max_range, z_middle + max_range)\n\n\nprint('torch',torch.__version__)\nprint('torch.cuda',torch.version.cuda)\n\nprint('IMPORT OK!!!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:41:44.044107Z","iopub.execute_input":"2025-06-22T21:41:44.044414Z","iopub.status.idle":"2025-06-22T21:42:05.662050Z","shell.execute_reply.started":"2025-06-22T21:41:44.044382Z","shell.execute_reply":"2025-06-22T21:42:05.661121Z"}},"outputs":[{"name":"stdout","text":"will do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/drfold/cfg_97/EvoMSA2XYZ.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  RNAlm.load_state_dict(torch.load(saved_model,map_location=torch.device('cpu')),strict=False)\n","output_type":"stream"},{"name":"stdout","text":"torch 2.5.1+cu121\ntorch.cuda 12.1\nIMPORT OK!!!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ─── TOP-OF-NOTEBOOK INTEGRATION CELL ───\nCFG_FOLDER  = \"/kaggle/input/drfold2/DRfold2/model_hub/cfg_97\"\nROOT_FOLDER = \"/kaggle/input/drfold2/DRfold2\"\ndef write_frame_coor_to_pdb(coord, seq, savefile):\n    \"\"\"\n    coord: np.ndarray of shape (L,3,3) from your model (P, sugar, N)\n    seq:   string of length L\n    savefile: path to write PDB with only C1' atom per residue\n    \"\"\"\n    L = coord.shape[0]\n    with open(savefile, 'w') as f:\n        count = 1\n        for i, res in enumerate(seq):\n            x, y, z = coord[i, 1]   # channel=1 → sugar atom\n            # PDB ATOM line building:\n            # atom serial, atom name, residue name, chain A, residue seq, x,y,z, occup,temp, element\n            f.write(\n                f\"ATOM  {count:5d}  C1' {res:>3s} A{ i+1:4d}\"\n                f\"{x:8.3f}{y:8.3f}{z:8.3f}  1.00  0.00           C\\n\"\n            )\n            count += 1\n        f.write(\"TER\\n\")\n\n\n# ── pure-Python FASTA I/O ──\ndef parse_fasta(path):\n    recs, h, seqs = [], None, []\n    with open(path) as f:\n        for line in f:\n            line = line.rstrip(\"\\n\")\n            if line.startswith(\">\"):\n                if h is not None:\n                    recs.append((h, \"\".join(seqs)))\n                h = line[1:].split()[0]\n                seqs = []\n            else:\n                seqs.append(line)\n        if h is not None:\n            recs.append((h, \"\".join(seqs)))\n    return recs\n\n# ── load model & weights ──\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nconfig = dict(seq_dim=6, msa_dim=7, N_ensemble=1, N_cycle=8, m_dim=64, s_dim=64, z_dim=64)\nmodel = MSA2XYZ(**config).to(device)\n\nweights_path = os.path.join(CFG_FOLDER, \"model_0\")\nmodel.load_state_dict(torch.load(weights_path, map_location=device), strict=False)\nmodel.eval()\n\nprint(\"✅ DRfold2 + MSA integration ready. Device:\", device)\nprint(\"   parse_seq → (L,6); parse_msa → (N,L,7); model loaded.\")\n# ─── end of integration cell ───\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:05.663035Z","iopub.execute_input":"2025-06-22T21:42:05.663505Z","iopub.status.idle":"2025-06-22T21:42:08.024158Z","shell.execute_reply.started":"2025-06-22T21:42:05.663479Z","shell.execute_reply":"2025-06-22T21:42:08.023310Z"}},"outputs":[{"name":"stdout","text":"will do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-02d836b58166>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(weights_path, map_location=device), strict=False)\n","output_type":"stream"},{"name":"stdout","text":"✅ DRfold2 + MSA integration ready. Device: cuda\n   parse_seq → (L,6); parse_msa → (N,L,7); model loaded.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install /kaggle/input/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:08.024938Z","iopub.execute_input":"2025-06-22T21:42:08.025237Z","iopub.status.idle":"2025-06-22T21:42:14.155760Z","shell.execute_reply.started":"2025-06-22T21:42:08.025201Z","shell.execute_reply":"2025-06-22T21:42:14.154851Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython==1.85) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython==1.85) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython==1.85) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->biopython==1.85) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->biopython==1.85) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->biopython==1.85) (2024.2.0)\nInstalling collected packages: biopython\nSuccessfully installed biopython-1.85\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#tuning\nclass SequenceStructureDataset(Dataset):\n    def __init__(self, csv_file, data_dir, transform=None):\n        self.df = pd.read_csv(csv_file)\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        features = np.load(self.data_dir / f\"{row['ID']}_features.npy\")\n        coords   = np.load(self.data_dir / f\"{row['ID']}_coords.npy\")\n        sample = {'features': torch.from_numpy(features).float(),\n                  'coords':   torch.from_numpy(coords).float()}\n        return self.transform(sample) if self.transform else sample\n\n\nDATA_KAGGLE_DIR = '/kaggle/input/stanford-rna-3d-folding'\n\ntrain_dataset = SequenceStructureDataset(\n    csv_file=f\"{DATA_KAGGLE_DIR}/train_labels.csv\",\n    data_dir=f\"{DATA_KAGGLE_DIR}/train_data\"\n)\nval_dataset = SequenceStructureDataset(\n    csv_file=f\"{DATA_KAGGLE_DIR}/validation_labels.csv\",\n    data_dir=f\"{DATA_KAGGLE_DIR}/validation_data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:14.156962Z","iopub.execute_input":"2025-06-22T21:42:14.157306Z","iopub.status.idle":"2025-06-22T21:42:14.512656Z","shell.execute_reply.started":"2025-06-22T21:42:14.157273Z","shell.execute_reply":"2025-06-22T21:42:14.511930Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# from datetime import datetime\n# import pytz\n# print('LOGGING TIME OF START:',  datetime.strftime(datetime.now(pytz.timezone('Asia/Singapore')), \"%Y-%m-%d %H:%M:%S\"))\n\n\n# try:\n#     import Bio\n# except:\n#     #for drfold2 --------\n#     #!pip install biopython\n#     !pip install /kaggle/input/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# !pip install /kaggle/input/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# !pip install biopython\n# print('PIP INSTALL OK !!!!')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:14.513450Z","iopub.execute_input":"2025-06-22T21:42:14.513668Z","iopub.status.idle":"2025-06-22T21:42:14.516883Z","shell.execute_reply.started":"2025-06-22T21:42:14.513650Z","shell.execute_reply":"2025-06-22T21:42:14.516177Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# MODE = 'local' #'local' # submit\nMODE = 'local'\nMSA_DIR = '/kaggle/input/stanford-rna-3d-folding/MSA_v2'\n\nDATA_KAGGLE_DIR = '/kaggle/input/stanford-rna-3d-folding'\nif MODE == 'local':\n    valid_df = pd.read_csv(\"/kaggle/input/validation-sequences-clean-csv/validation_sequences_clean.csv\")\n    label_df = pd.read_csv(\"/kaggle/input/validation-labels-clean-csv/validation_labels_clean.csv\")\n    label_df['target_id'] = label_df['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n\nif MODE == 'submit':\n\tvalid_df = pd.read_csv(f'{DATA_KAGGLE_DIR}/test_sequences.csv')\n\nprint('len(valid_df)',len(valid_df))\nprint(valid_df.iloc[0])\nprint('')\n\n\n# cfg = dotdict(\n#     num_conf = 5,\n#     max_length=480,\n# )\nNUM_CONF=5\nMAX_LENGTH=480\nDEVICE='cuda' #'cpu'\n\nprint('MODE:', MODE)\nprint('SETTING OK!!!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:14.519037Z","iopub.execute_input":"2025-06-22T21:42:14.519286Z","iopub.status.idle":"2025-06-22T21:42:14.597449Z","shell.execute_reply.started":"2025-06-22T21:42:14.519267Z","shell.execute_reply":"2025-06-22T21:42:14.596771Z"}},"outputs":[{"name":"stdout","text":"len(valid_df) 94\ntarget_id                                                     9L5R_2\nsequence           AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUU...\ntemporal_cutoff                                           2025-03-12\ndescription        Cryo-EM structure of the thermophile spliceoso...\nall_sequences      >9L5R_1|Chain A[auth 2]|U2 snRNA|Chaetomium th...\nName: 0, dtype: object\n\nMODE: local\nSETTING OK!!!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nos.makedirs('/kaggle/working/drfold', exist_ok=True)\n!cp -r /kaggle/input/drfold2/DRfold2/* /kaggle/working/drfold/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:14.598877Z","iopub.execute_input":"2025-06-22T21:42:14.599146Z","iopub.status.idle":"2025-06-22T21:42:16.808953Z","shell.execute_reply.started":"2025-06-22T21:42:14.599124Z","shell.execute_reply":"2025-06-22T21:42:16.807890Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# import sys, os\n# CFG97 = os.path.join('/kaggle/input/drfold/DRfold2/DRfold2', 'cfg_97')\n# assert os.path.isdir(CFG97), f\"{CFG97} not found!\"\n# sys.path.insert(0, CFG97)\n\n# # now this should succeed:\n# from EvoMSA2XYZ import MSA2XYZ\n# from RNALM2.Model import RNA2nd\n# from data         import parse_seq, Get_base, BASE_COOR\n# from data         import write_frame_coor_to_pdb, parse_pdb_to_xyz\n\n# print(\"imported!\", MSA2XYZ, RNA2nd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:16.810075Z","iopub.execute_input":"2025-06-22T21:42:16.810358Z","iopub.status.idle":"2025-06-22T21:42:16.813843Z","shell.execute_reply.started":"2025-06-22T21:42:16.810328Z","shell.execute_reply":"2025-06-22T21:42:16.813078Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\nimport numpy as np\n\n# 1) Point at the cfg_97 folder (adjust this to your actual path)\nCFG97 = '/kaggle/input/drfold2/DRfold2/cfg_97'\n\n# 2) Load the base.npy file once into a Python variable\nBASE_COOR = np.load(os.path.join(CFG97, 'base.npy'))\n\n# 3) Now import your parsing and model code\nimport sys\nsys.path.insert(0, CFG97)\nfrom data import parse_seq, Get_base\n# (no BASE_COOR to import from data.py)\n\n# 4) When you need the 3×3 base coordinates for a sequence, call:\nsequence = \"ACGUACGUA\"\nbase_coords = Get_base(sequence, BASE_COOR)\n# base_coords.shape == (len(sequence), 3, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:16.814820Z","iopub.execute_input":"2025-06-22T21:42:16.815111Z","iopub.status.idle":"2025-06-22T21:42:16.830812Z","shell.execute_reply.started":"2025-06-22T21:42:16.815083Z","shell.execute_reply":"2025-06-22T21:42:16.830084Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def parse_pdb_to_xyz(pdb_file, atom_name=\" P  \"):\n    coords, resid, resname = [], [], []\n    with open(pdb_file) as f:\n        for line in f:\n            if line.startswith(\"ATOM\") and line[12:16].strip() == atom_name.strip():\n                x = float(line[30:38]); y = float(line[38:46]); z = float(line[46:54])\n                coords.append((x,y,z))\n                resid.append(int(line[22:26]))\n                resname.append(line[17:20].strip())\n    return np.array(coords, dtype=np.float32), resname, resid\n\n\ndef parse_msa(path: str, max_seqs: int = 100) -> np.ndarray:\n    \"\"\"\n    Reads a FASTA/A3M MSA, caps at max_seqs rows, one-hots each → (L,6),\n    and stacks into (N,L,6) - no target marker channel for now.\n    \"\"\"\n    recs = parse_fasta(path)[:max_seqs]\n    if not recs:\n        raise ValueError(f\"no sequences in MSA at {path}\")\n    N = len(recs)\n    L = len(recs[0][1])\n    msa = np.zeros((N, L, 6), dtype=np.float32)  # Changed from 7 to 6\n    \n    for i, (_, seq) in enumerate(recs):\n        # Always use our one-hot function for MSA\n        sq = parse_seq_onehot(seq)\n        if i < 5:  # Only print first 5 rows to reduce spam\n            print(f\"   Row {i}: seq length {len(seq)}, parse_seq_onehot shape {sq.shape}, expected L={L}\")\n        \n        if sq.shape[0] != L:\n            raise ValueError(f\"row {i} length {sq.shape[0]} != expected {L}\")\n        if sq.shape[1] != 6:\n            raise ValueError(f\"row {i} features {sq.shape[1]} != expected 6\")\n        \n        msa[i,:,:6] = sq\n        # Remove the target marker for now\n    return msa\n\n# ── one-hot encoder for a single RNA sequence ──\ndef parse_seq_onehot(seq: str) -> np.ndarray:\n    \"\"\"\n    Returns (L,6) float32 array:\n      ch0–3 = one-hot A/G/C/U,\n      ch4   = everything else (gaps/unknown),\n      ch5   = mask = 1.0 everywhere.\n    \"\"\"\n    L = len(seq)\n    feat = np.zeros((L,6), dtype=np.float32)\n    cmap = {'A':0, 'G':1, 'C':2, 'U':3, 'T':3}\n    for i, r in enumerate(seq):\n        feat[i, cmap.get(r,4)] = 1.0\n    feat[:,5] = 1.0\n    return feat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:16.831600Z","iopub.execute_input":"2025-06-22T21:42:16.831864Z","iopub.status.idle":"2025-06-22T21:42:16.841689Z","shell.execute_reply.started":"2025-06-22T21:42:16.831845Z","shell.execute_reply":"2025-06-22T21:42:16.840781Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\n###########################################################3\nKAGGLE_TRUTH_PDB_DIR ='/kaggle/working/drfold/kaggle-casp15-truth'\nUSALIGN = '/kaggle/working/USalign' \nos.system('cp /kaggle/input/usalign/USalign /kaggle/working/')\nos.system('sudo chmod u+x /kaggle/working/USalign')\n\n# evaluate helper\ndef get_truth_df(target_id, label_df):\n    truth_df = label_df[label_df['target_id'] == target_id]\n    truth_df = truth_df.reset_index(drop=True)\n    return truth_df\n\ndef parse_usalign_for_tm_score(output):\n    # Extract TM-score based on length of reference structure (second)\n    tm_score_match = re.findall(r'TM-score=\\s+([\\d.]+)', output)[1]\n    if not tm_score_match:\n        raise ValueError('No TM score found')\n    return float(tm_score_match)\n\ndef parse_usalign_for_transform(output):\n    # Locate the rotation matrix section\n    matrix_lines = []\n    found_matrix = False\n\n    for line in output.splitlines():\n        if \"The rotation matrix to rotate Structure_1 to Structure_2\" in line:\n            found_matrix = True\n        elif found_matrix and re.match(r'^\\d+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+$', line):\n            matrix_lines.append(line)\n        elif found_matrix and not line.strip():\n            break  # Stop parsing if an empty line is encountered after the matrix\n\n    # Parse the rotation matrix values\n    rotation_matrix = []\n    for line in matrix_lines:\n        parts = line.split()\n        row_values = list(map(float, parts[1:]))  # Skip the first column (index)\n        rotation_matrix.append(row_values)\n    return np.array(rotation_matrix)\n\n\n\n# data helper\ndef make_data(seq, target_id, msa_dir):\n    base = Get_base(seq, BASE_COOR)\n    seq_idx = np.arange(len(seq)) + 1\n    \n    # Try to load MSA, fallback to single sequence if not found\n    msa_file = os.path.join(msa_dir, f\"{target_id}.MSA.fasta\")\n    \n    if os.path.exists(msa_file):\n        print(f\"✅ Loading MSA from {msa_file}\")\n        print(f\"   Target sequence length: {len(seq)}\")\n        try:\n            msa_data = parse_msa(msa_file, max_seqs=100)  # Shape: (N,L,7)\n            print(f\"   MSA loaded: {msa_data.shape[0]} sequences, {msa_data.shape[1]} positions\")\n            \n            # Verify MSA matches sequence length\n            if msa_data.shape[1] != len(seq):\n                print(f\"⚠️ MSA length {msa_data.shape[1]} != sequence length {len(seq)}, using single sequence\")\n                msa_data = create_single_sequence_msa(seq)\n            else:\n                # Verify target sequence matches first row of MSA\n                target_from_msa = msa_to_sequence(msa_data[0, :, :6])\n                if not sequences_match(target_from_msa, seq):\n                    print(f\"⚠️ Target sequence mismatch with MSA, using single sequence\")\n                    msa_data = create_single_sequence_msa(seq)\n        except Exception as e:\n            print(f\"⚠️ Error loading MSA: {e}, using single sequence\")\n            msa_data = create_single_sequence_msa(seq)\n    else:\n        print(f\"⚠️ MSA file not found: {msa_file}, using single sequence\")\n        msa_data = create_single_sequence_msa(seq)\n    \n    msa = torch.from_numpy(msa_data).float()  # Shape: (N,L,7)\n    base_x = torch.from_numpy(base).float()\n    seq_idx = torch.from_numpy(seq_idx).long()\n    \n    return msa, base_x, seq_idx\n\ndef create_single_sequence_msa(seq):\n    \"\"\"Create a fake MSA with just the target sequence duplicated\"\"\"\n    # Always use our one-hot function\n    aa_type = parse_seq_onehot(seq)  \n    print(f\"   aa_type shape: {aa_type.shape}\")\n    \n    # Create MSA with 2 identical sequences, 6 channels only\n    msa_data = np.zeros((2, len(seq), 6), dtype=np.float32)  # Changed from 7 to 6\n    msa_data[0, :, :6] = aa_type  # First sequence\n    msa_data[1, :, :6] = aa_type  # Duplicate\n    # Remove target marker for now\n    return msa_data\n\ndef msa_to_sequence(msa_row):\n    \"\"\"Convert one-hot MSA row back to sequence string\"\"\"\n    seq = \"\"\n    for i in range(msa_row.shape[0]):\n        if msa_row[i, 0] == 1.0: seq += \"A\"\n        elif msa_row[i, 1] == 1.0: seq += \"G\"\n        elif msa_row[i, 2] == 1.0: seq += \"C\"\n        elif msa_row[i, 3] == 1.0: seq += \"U\"\n        else: seq += \"N\"  # Unknown/gap\n    return seq\n\ndef sequences_match(seq1, seq2, gap_chars=\"-N\"):\n    \"\"\"Check if sequences match, ignoring gaps\"\"\"\n    s1 = ''.join(c for c in seq1.upper() if c not in gap_chars)\n    s2 = ''.join(c for c in seq2.upper() if c not in gap_chars)\n    return s1 == s2\n    \ndef make_dummy_solution():\n    solution=dotdict()\n    for i, row in valid_df.iterrows():\n        target_id = row.target_id\n        sequence = row.sequence\n        solution[target_id]=dotdict(\n            target_id=target_id,\n            sequence=sequence,\n            coord=[],\n        )\n    return solution\n\ndef solution_to_submit_df(solution):\n    submit_df = []\n    for k,s in solution.items():\n        df = coord_to_df(s.sequence, s.coord, s.target_id)\n        submit_df.append(df)\n    \n    submit_df = pd.concat(submit_df)\n    return submit_df\n \n\ndef coord_to_df(sequence, coord, target_id):\n    L = len(sequence)\n    df = pd.DataFrame()\n    df['ID'] = [f'{target_id}_{i + 1}' for i in range(L)]\n    df['resname'] = [s for s in sequence]\n    df['resid'] = [i + 1 for i in range(L)]\n\n    num_coord = len(coord)\n    for j in range(num_coord):\n        df[f'x_{j+1}'] = coord[j][:, 0]\n        df[f'y_{j+1}'] = coord[j][:, 1]\n        df[f'z_{j+1}'] = coord[j][:, 2]\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:16.842826Z","iopub.execute_input":"2025-06-22T21:42:16.843132Z","iopub.status.idle":"2025-06-22T21:42:16.952137Z","shell.execute_reply.started":"2025-06-22T21:42:16.843102Z","shell.execute_reply":"2025-06-22T21:42:16.951137Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Add this diagnostic code to check MSA availability\nimport os\n\nmsa_directory = \"/kaggle/input/stanford-rna-3d-folding/MSA_v2\"\ntest_data = pd.read_csv(\"/kaggle/input/validation-sequences-clean-csv/validation_sequences_clean.csv\")\n\nprint(f\"MSA directory exists: {os.path.exists(msa_directory)}\")\n\nif os.path.exists(msa_directory):\n    msa_files = os.listdir(msa_directory)\n    print(f\"Number of MSA files found: {len(msa_files)}\")\n    print(f\"First 10 MSA files: {msa_files[:10]}\")\n    \n    # Check what target IDs we're looking for\n    target_ids = test_data['target_id'].tolist()\n    print(f\"First 10 target IDs: {target_ids}\")\n    \n    # Check if any MSA files match our target IDs\n    matches = []\n    for target_id in target_ids:\n        expected_file = f\"{target_id}.MSA.fasta\"\n        if expected_file in msa_files:\n            matches.append(expected_file)\n    \n    print(f\"Matching MSA files found: {len(matches)}\")\n\nelse:\n    print(\"MSA directory does not exist!\")\n    print(\"Available directories in /kaggle/input/stanford-rna-3d-folding/:\")\n    if os.path.exists(\"/kaggle/input/stanford-rna-3d-folding/\"):\n        print(os.listdir(\"/kaggle/input/stanford-rna-3d-folding/\"))\n    else:\n        print(\"Stanford RNA 3D folding dataset not available!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:16.953095Z","iopub.execute_input":"2025-06-22T21:42:16.953419Z","iopub.status.idle":"2025-06-22T21:42:17.024969Z","shell.execute_reply.started":"2025-06-22T21:42:16.953390Z","shell.execute_reply":"2025-06-22T21:42:17.024112Z"}},"outputs":[{"name":"stdout","text":"MSA directory exists: True\nNumber of MSA files found: 2534\nFirst 10 MSA files: ['3JCS_6.MSA.fasta', '7MSF_R.MSA.fasta', '2OOM_B.MSA.fasta', '1ZDI_S.MSA.fasta', '5FJ1_H.MSA.fasta', '6UF1_C.MSA.fasta', '4V8A_AB.MSA.fasta', '6JDG_G.MSA.fasta', '5NCO_1.MSA.fasta', '5DI4_A.MSA.fasta']\nFirst 10 target IDs: ['9L5R_2', '9GFT_AU', '9L0R_K', '9GFT_A3', '9B2K_B', '9B0S_Et', '9J3T_B', '9LCR_B', '8KEB_A', '9L5S_5', '8VXZ_C', '9J6Y_E', '8QHU_5', '9GHF_Z', '9KPO_B', '9N2B_5', '9N2C_Pt', '9B1Y_4', '9G06_a', '9DE8_A', '9B83_C', '8ZMH_A', '9E2Y_F', '9DE7_A', '8Y9L_B', '9FIB_Y', '9J3R_B', '9DPB_C', '8XTP_A', '8ZTV_Y', '8Y9M_B', '8ZQ9_A', '8XTP_B', '9B89_C', '8SYK_C', '9FN3_B', '8QHU_3', '9DRS_C', '8XTR_A', '9LMF_F', '9DE6_B', '8SYK_B', '9DE6_A', '8R7N_A', '8K85_A', '9FCV_B', '9DPA_C', '9DE5_C', '8VZ6_S', '8YIG_C', '9B84_F', '9C8K_2', '9B0Q_AP', '9E2Z_F', '8Z8Q_B', '9E2W_F', '8KHH_A', '8Z8U_B', '8ZTU_Y', '9GCL_A', '8RRI_Ax', '9L5S_6', '9GCM_A', '8Z9K_B', '9MTY_C', '8QHU_7', '9GBW_R', '8T5O_A', '9DPL_C', '8WFA_B', '9ISV_A', '9AR6_B', '9DE5_D', '8ZDR_A', '8WFB_B', '9L5R_6', '9IS7_B', '9GC0_Q', '8YIH_C', '9HNY_CA', '8VK7_B', '8WF8_B', '8QHU_S4', '8ZAU_A', '9AR7_B', '9AR4_B', '8Y9N_B', '8QHU_4', '8RWG_C', '8YII_C', '9DCF_C', '9L5T_6', '8WF9_B', '9GBZ_R']\nMatching MSA files found: 60\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"################### start here !!! #######################################################3\nout_dir = '/kaggle/working/model-output'\nos.makedirs(out_dir, exist_ok=True)\nsolution = make_dummy_solution()\n\n\n#load model (these are moified versions, not the same from their github repo)\nrnalm = RNA2nd(dict(\n    s_in_dim=5,\n    z_in_dim=2,\n    s_dim= 512,\n    z_dim= 128,\n    N_elayers=18,\n))\nrnalm_file = '/kaggle/working/drfold/model_hub/RCLM/epoch_67000'\nprint(rnalm_file)\nprint(\n    rnalm.load_state_dict(torch.load(rnalm_file, map_location='cpu', weights_only=True), strict=False)\n    #Unexpected key(s) in state_dict: \"ss_head.linear.weight\", \"ss_head.linear.bias\".\n)\nrnalm = rnalm.to(DEVICE)\nrnalm = rnalm.eval()\ntotal_time_taken = 0\nmax_gpu_mem_used = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:17.025864Z","iopub.execute_input":"2025-06-22T21:42:17.026170Z","iopub.status.idle":"2025-06-22T21:42:18.054075Z","shell.execute_reply.started":"2025-06-22T21:42:17.026138Z","shell.execute_reply":"2025-06-22T21:42:18.053286Z"}},"outputs":[{"name":"stdout","text":"will do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n/kaggle/working/drfold/model_hub/RCLM/epoch_67000\n_IncompatibleKeys(missing_keys=[], unexpected_keys=['ss_head.linear.weight', 'ss_head.linear.bias'])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"cfg = dict(\n    seq_dim=6,\n    msa_dim=7,\n    N_ensemble=1,   # how many ensemble members\n    N_cycle=8,      # how many recycling cycles\n    m_dim=64,\n    s_dim=64,\n    z_dim=64,\n)\nfor c in range(NUM_CONF): \n    msa2xyz = MSA2XYZ(**cfg)\n    msa2xyz_file = [\n        f'/kaggle/working/drfold/model_hub/cfg_97/model_{k}' for k in [0,1,2,8,9]\n    ][c]\n    print(msa2xyz_file)\n    print(\n        msa2xyz.load_state_dict(torch.load(msa2xyz_file, map_location='cpu', weights_only=True), strict=True)\n    )\n    msa2xyz.msaxyzone.premsa.rnalm = rnalm\n    msa2xyz = msa2xyz.to(DEVICE)\n    msa2xyz = msa2xyz.eval()\n \n    for i,row in valid_df.iterrows():\n        start_timer = timer()\n        \n        target_id = row.target_id\n        sequence = row.sequence\n        seq = row.sequence    \n        \n        L = len(sequence)\n        if L>MAX_LENGTH:\n            i0 = np.random.choice(L-MAX_LENGTH+1)\n            i1 = i0 + MAX_LENGTH\n        else:\n            i0 = 0\n            i1 = L\n        \n        seq = sequence[i0:i1]\n        print(c,i,target_id, L, seq[:75]+'...')\n        \n        msa, base_x, seq_idx = make_data(seq, target_id, MSA_DIR)\n        msa, base_x, seq_idx = msa.to(DEVICE), base_x.to(DEVICE), seq_idx.to(DEVICE)\n        \n        # MSA usage verification\n        print(f\"   MSA tensor shape: {msa.shape}\")\n        if msa.shape[0] > 2:\n            print(f\"   ✅ Using real MSA with {msa.shape[0]} sequences\")\n        else:\n            print(f\"   ⚠️ Using single sequence (duplicated)\")\n            \n        # Extract sequence features (first 6 channels) for the model\n        # The model expects seq to be (L, 6), not (L, 7)\n        seq_features = msa[0, :, :6]  # Take first row, first 6 channels\n        print(f\"   Seq features shape: {seq_features.shape}\")\n            \n        secondary = None #secondary structure\n    \n        with torch.no_grad(): \n            out = msa2xyz.pred(msa, seq_idx, secondary, base_x, np.array(list(seq)))\n\n        # key = list(out.keys()) # plddt(L,L), coor(L,3,3), dist_p(L,L,38), dist_c, dist_n,\n        # for k in key:\n        #     print(k, type(out[k]), out[k].shape)\n \n        \n        if L!=len(seq):\n             out['coor'] = np.pad(out['coor'] ,((i0, L - i1), (0, 0), (0, 0)), 'constant', constant_values=0)\n\n\n        print('out:',  out['coor'].shape)\n        \n        # Log MSA usage statistics\n        msa_depth = msa.shape[0]\n        if msa_depth > 2:\n            print(f\"   📊 Processed with MSA depth: {msa_depth}\")\n        else:\n            print(f\"   📊 Processed with single sequence (no MSA)\")\n            \n        time_taken = timer()-start_timer\n        time_taken = timer()-start_timer\n        total_time_taken += time_taken\n        print('time_taken:', time_to_str(time_taken, mode='sec')) \n        \n        gpu_mem_used = gpu_memory_use()\n        max_gpu_mem_used = max(max_gpu_mem_used,gpu_mem_used)\n        print('gpu_mem_used:', gpu_mem_used, 'GB')\n\n        torch.cuda.empty_cache() \n        sugar_xyz = out['coor'][:, 1, :]   # shape (L,3)\n        solution[target_id].coord.append(sugar_xyz)\n    print('')\n    \n#-----end of conformation generation ----\nprint('MAX_LENGTH', MAX_LENGTH)\nprint('### total_time_taken:', time_to_str(total_time_taken, mode='min'))\nprint('### max_gpu_mem_used:', max_gpu_mem_used, 'GB')\n\n# MSA usage summary\nmsa_used_count = 0\ntotal_targets = len(valid_df)\nfor i, row in valid_df.iterrows():\n    target_id = row.target_id\n    msa_file = os.path.join(MSA_DIR, f\"{target_id}.MSA.fasta\")\n    if os.path.exists(msa_file):\n        msa_used_count += 1\n\nprint(f'### MSA usage: {msa_used_count}/{total_targets} targets had MSA files')\nprint(f'### MSA coverage: {msa_used_count/total_targets*100:.1f}%')\nprint('')\n\nsubmission = solution_to_submit_df(solution)\nsubmission.to_csv(f'submission.csv', index=False)\nprint(submission)\nprint('SUBMIT OK!!!!!!')\nprint('')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T21:42:18.054799Z","iopub.execute_input":"2025-06-22T21:42:18.055010Z","iopub.status.idle":"2025-06-22T22:50:50.239024Z","shell.execute_reply.started":"2025-06-22T21:42:18.054993Z","shell.execute_reply":"2025-06-22T22:50:50.238305Z"}},"outputs":[{"name":"stdout","text":"will do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n/kaggle/working/drfold/model_hub/cfg_97/model_0\n<All keys matched successfully>\n0 0 9L5R_2 193 AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUUUUCAGUUUAAUCUCUGAAACUGCUCUACG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_2.MSA.fasta\n   Target sequence length: 193\n   Row 0: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 1: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 2: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 3: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 4: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   MSA loaded: 100 sequences, 193 positions\n   MSA tensor shape: torch.Size([100, 193, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([193, 6])\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"out: (193, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 13 sec\ngpu_mem_used: 4 GB\n0 1 9GFT_AU 76 GGGGCUAUAGCUCAGCUGGGAGAGCGCUUGCAUGGCAUGCAAGAGGUCAGCGGUUCGAUCCCGCUUAGCUCCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_AU.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 2 9L0R_K 700 CAGAUAAUCCAUAGCGAUAUGGGAAAGCUUUUGUAGGUGUAUCAACAAGAGCGCCAGUGAUGGUCAAUCUAAGCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L0R_K.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 3 9GFT_A3 77 GGCUACGUAGCUCAGUUGGUUAGAGCACAUCACUCAUAAUGAUGGGGUCACAGGUUCGAAUCCCGUCGUAGCCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_A3.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 4 9B2K_B 70 AAACAGCAUAGCAAGUUAAAAUAAGGCUAGUCCGUUAUCAACUUGAAAAAGUGGCACCGAGUCGGUGCUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B2K_B.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 5 9B0S_Et 75 GCCCGGAUAGCUCAGCUGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0S_Et.MSA.fasta\n   Target sequence length: 75\n   Row 0: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 1: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 2: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 3: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 4: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   MSA loaded: 100 sequences, 75 positions\n   MSA tensor shape: torch.Size([100, 75, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([75, 6])\nout: (75, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 6 9J3T_B 580 AUUGAAAAAUCAAUAGAUUUAAACCUAGUGAAGAGCAUUUGAACAAUGUGCUAGGGUAGUAUGGGAUAAGUCGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3T_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 1: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 2: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 3: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 4: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   MSA loaded: 100 sequences, 580 positions\n⚠️ MSA length 580 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 7 9LCR_B 578 UCUGGUGCUGGCUUACGACGCCCAGUUGUGGGCUGGUGCUGGGAGAAAGAAGGCUGGGGCAAUCACAAAGGCAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LCR_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 1: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 2: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 3: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 4: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   MSA loaded: 100 sequences, 578 positions\n⚠️ MSA length 578 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (578, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 8 8KEB_A 72 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KEB_A.MSA.fasta\n   Target sequence length: 72\n   Row 0: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 1: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 2: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 3: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 4: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   MSA loaded: 100 sequences, 72 positions\n   MSA tensor shape: torch.Size([100, 72, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([72, 6])\nout: (72, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 9 9L5S_5 116 UUGGAGUAGGCCAGCUCAGACCGAACUCAUUUCCUGCCUUUUACCGGAUGUGACCGUGAGUUGGCCUGAAAUACU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_5.MSA.fasta\n   Target sequence length: 116\n   Row 0: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 1: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 2: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 3: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 4: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   MSA loaded: 100 sequences, 116 positions\n   MSA tensor shape: torch.Size([100, 116, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([116, 6])\nout: (116, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n0 10 8VXZ_C 36 GCGUACGAAGGAGAGGAGAGGAAGAGGAGAGUACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VXZ_C.MSA.fasta\n   Target sequence length: 36\n   Row 0: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 1: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 2: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 3: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 4: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   MSA loaded: 100 sequences, 36 positions\n   MSA tensor shape: torch.Size([100, 36, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 11 9J6Y_E 550 AAGUGUACCGAUGAAGCUAGUGGAUAAGGUGUGACAAGCCGCCUAGCCAUACGUCUCUUAAUAACUACUAUGACG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J6Y_E.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 1: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 2: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 3: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 4: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   MSA loaded: 63 sequences, 550 positions\n⚠️ MSA length 550 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (550, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 12 8QHU_5 135 UUACGUCCCUCUCCAAACGAGAGAACAUGCAUGGGCUGGCAUGAGCGGCAUGCUUCACUUCGGUGGGGCUCGAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_5.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 64 sequences, 135 positions\n   MSA tensor shape: torch.Size([64, 135, 6])\n   ✅ Using real MSA with 64 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 64\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n0 13 9GHF_Z 77 CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GHF_Z.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 14 9KPO_B 255 AAAAAACUUACUAUUAUAUUUGUAACAAAUUUUAUACAUAAGAUAAAUUCGUAUGUAUAGCCGUUCUGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9KPO_B.MSA.fasta\n   Target sequence length: 255\n   Row 0: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 1: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 2: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 3: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 4: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   MSA loaded: 94 sequences, 255 positions\n   MSA tensor shape: torch.Size([94, 255, 6])\n   ✅ Using real MSA with 94 sequences\n   Seq features shape: torch.Size([255, 6])\nout: (255, 3, 3)\n   📊 Processed with MSA depth: 94\ntime_taken:  0 min 17 sec\ngpu_mem_used: 6 GB\n0 15 9N2B_5 120 UGCCUGGCGGCAGUAGCGCGGUGGUCCCACCUGACCCCAUGCCGAACUCAGAAGUGAAACGCCGUAGCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2B_5.MSA.fasta\n   Target sequence length: 120\n   Row 0: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 1: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 2: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 3: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 4: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   MSA loaded: 100 sequences, 120 positions\n   MSA tensor shape: torch.Size([100, 120, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n0 16 9N2C_Pt 77 CGCGGGGUGGAGCAGCCCGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2C_Pt.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 17 9B1Y_4 81 ACGGUUUGUGUAGGAUAGGUGGGAGACUGUGAAGCUCACACGCCAGUGUGGGUGGAGUCGUUGUUGAAAUACCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B1Y_4.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 18 9G06_a 73 GCGGGGUGGAGCAGGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9G06_a.MSA.fasta\n   Target sequence length: 73\n   Row 0: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 1: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 2: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 3: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 4: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   MSA loaded: 100 sequences, 73 positions\n   MSA tensor shape: torch.Size([100, 73, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([73, 6])\nout: (73, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 19 9DE8_A 57 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE8_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 20 9B83_C 39 GGGAGCCCCCCXGCUUCACUGCAUGGAAGCUAAAGGGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B83_C.MSA.fasta\n   Target sequence length: 39\n   Row 0: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 1: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 2: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 3: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 4: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   MSA loaded: 23 sequences, 39 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (39, 6)\n   MSA tensor shape: torch.Size([2, 39, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([39, 6])\nout: (39, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 21 8ZMH_A 169 CGUGGUUGACACGCAGACCUCUUACAAGAGUGUCUAGGUGCCUUUGAGAGUUACUCUUUGCUCUCUUCGGAAGAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZMH_A.MSA.fasta, using single sequence\n   aa_type shape: (169, 6)\n   MSA tensor shape: torch.Size([2, 169, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([169, 6])\nout: (169, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n0 22 9E2Y_F 35 AUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Y_F.MSA.fasta, using single sequence\n   aa_type shape: (35, 6)\n   MSA tensor shape: torch.Size([2, 35, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 23 9DE7_A 58 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE7_A.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 100 sequences, 58 positions\n   MSA tensor shape: torch.Size([100, 58, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 24 8Y9L_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9L_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 25 9FIB_Y 15 AAUGUUUGAAAAAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FIB_Y.MSA.fasta\n   Target sequence length: 15\n   Row 0: seq length 15, parse_seq_onehot shape (15, 6), expected L=15\n   MSA loaded: 1 sequences, 15 positions\n   MSA tensor shape: torch.Size([1, 15, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([15, 6])\nout: (15, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 26 9J3R_B 580 GUGAAGAGCAUUUGAACAAUGUGCUAGGGUAGUAUGGGAUAAGUCGAUAACUAAAAUGAAUUGGGAUACUGAUUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3R_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 27 9DPB_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPB_C.MSA.fasta, using single sequence\n   aa_type shape: (76, 6)\n   MSA tensor shape: torch.Size([2, 76, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 28 8XTP_A 133 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_A.MSA.fasta, using single sequence\n   aa_type shape: (133, 6)\n   MSA tensor shape: torch.Size([2, 133, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([133, 6])\nout: (133, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 29 8ZTV_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTV_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 30 8Y9M_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9M_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 31 8ZQ9_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZQ9_A.MSA.fasta, using single sequence\n   aa_type shape: (159, 6)\n   MSA tensor shape: torch.Size([2, 159, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n0 32 8XTP_B 595 GAGCGCAACACCUGCCGCACAGGAUGGCUUCUGAGGUACCGGUGACGGUACAGAACGCGGAGGGGAAACCUGGAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (595, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 33 9B89_C 35 GGGCUUUCGUUUUCCUAUAUAGGAAAAUGAACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B89_C.MSA.fasta\n   Target sequence length: 35\n   Row 0: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 1: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 2: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 3: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 4: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   MSA loaded: 100 sequences, 35 positions\n   MSA tensor shape: torch.Size([100, 35, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 34 8SYK_C 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_C.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n0 35 9FN3_B 58 GGAGUCAUGGCUCAGGGCUGUUCGCAGCCGCUGCAGUCAGUCGAAAGACUGXGACUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FN3_B.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 34 sequences, 58 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (58, 6)\n   MSA tensor shape: torch.Size([2, 58, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 36 8QHU_3 216 UAUUAGUGGUAAUGCGAAACACUUGCCAGGUAACAAAUCAAUCCUCCCACGGUGAGCUUUCUUUUCACCAUAAUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_3.MSA.fasta, using single sequence\n   aa_type shape: (216, 6)\n   MSA tensor shape: torch.Size([2, 216, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([216, 6])\nout: (216, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 07 sec\ngpu_mem_used: 3 GB\n0 37 9DRS_C 77 GGCCAGGUAGCUCAGUCGGUAUGAGCGUCCGCCUGAAAAGCGGAAGGUCGGCGGUUCGAUCCCGCCCCUGGCCAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DRS_C.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 38 8XTR_A 145 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTR_A.MSA.fasta\n   Target sequence length: 145\n   Row 0: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 1: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 2: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 3: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 4: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   MSA loaded: 100 sequences, 145 positions\n   MSA tensor shape: torch.Size([100, 145, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([145, 6])\nout: (145, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 06 sec\ngpu_mem_used: 3 GB\n0 39 9LMF_F 700 GUAGGUGUAUCAACAAGAGCGCCAGUGAUGGUCAAUCUAAGCAAACCAAUCUUUAUGUAAUCAGUAAUGGUUACA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LMF_F.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 40 9DE6_B 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_B.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 41 8SYK_B 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_B.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n0 42 9DE6_A 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 43 8R7N_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8R7N_A.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 100 sequences, 135 positions\n   MSA tensor shape: torch.Size([100, 135, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n0 44 8K85_A 56 GGAGACGGUCGGGUCCAGUCGCAACGAUGUUGGCUGUUGAGUAGUGUGUGGGCUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8K85_A.MSA.fasta, using single sequence\n   aa_type shape: (56, 6)\n   MSA tensor shape: torch.Size([2, 56, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([56, 6])\nout: (56, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 45 9FCV_B 81 CGUCGCCGUCCAGCUCGACCAGGAUGGGAAGUUGCAUCUGCCUUCUUUUUGAAAGGUAAAAACAACAUCGUCCAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FCV_B.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 46 9DPA_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPA_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 47 9DE5_C 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_C.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 48 8VZ6_S 50 ACUAGAACCCGCCAAGCCUCUCAACGAUGCUCAAAUGUGCGGGUCGUUUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VZ6_S.MSA.fasta\n   Target sequence length: 50\n   Row 0: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 1: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 2: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 3: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 4: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   MSA loaded: 100 sequences, 50 positions\n   MSA tensor shape: torch.Size([100, 50, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([50, 6])\nout: (50, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 49 8YIG_C 104 GGUUAGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIG_C.MSA.fasta\n   Target sequence length: 104\n   Row 0: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 1: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 2: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 3: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 4: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   MSA loaded: 100 sequences, 104 positions\n   MSA tensor shape: torch.Size([100, 104, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n0 50 9B84_F 66 GGGCUXUUCGUUUUCCUAUUGAGCAUAGCCGCUUCUUCGGCUAUGCUCAAUAGGAAAACGAACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B84_F.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 51 9C8K_2 27 GCCCUGUGGGACCCAGGCCUGGGGGGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9C8K_2.MSA.fasta\n   Target sequence length: 27\n   Row 0: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 1: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 2: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 3: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 4: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   MSA loaded: 29 sequences, 27 positions\n   MSA tensor shape: torch.Size([29, 27, 6])\n   ✅ Using real MSA with 29 sequences\n   Seq features shape: torch.Size([27, 6])\nout: (27, 3, 3)\n   📊 Processed with MSA depth: 29\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 52 9B0Q_AP 71 GUCUCCGUAGUGUAGGGUAUCACGUUCGCCUAACACGCGAAAGGUCCUCGGUUCGAAACCGGGCGGAAACA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0Q_AP.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 53 9E2Z_F 40 GUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Z_F.MSA.fasta\n   Target sequence length: 40\n   Row 0: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 1: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 2: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 3: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 4: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   MSA loaded: 100 sequences, 40 positions\n   MSA tensor shape: torch.Size([100, 40, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([40, 6])\nout: (40, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 54 8Z8Q_B 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAAACCACAAAUUUCUUACUGCGAAUUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8Q_B.MSA.fasta, using single sequence\n   aa_type shape: (71, 6)\n   MSA tensor shape: torch.Size([2, 71, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 55 9E2W_F 48 UCGUGCUGAGUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2W_F.MSA.fasta, using single sequence\n   aa_type shape: (48, 6)\n   MSA tensor shape: torch.Size([2, 48, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([48, 6])\nout: (48, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 56 8KHH_A 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KHH_A.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 57 8Z8U_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8U_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 58 8ZTU_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTU_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 59 9GCL_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGGGCU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCL_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 60 8RRI_Ax 70 UAGGGAAUAGUUUAAAAAACAUCUGACUCACAUUCAGAAGAUGGAGGUUCAAUUCCUCCUUCCCUAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RRI_Ax.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 61 9L5S_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 62 9GCM_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCM_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 63 8Z9K_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGCCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z9K_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 64 9MTY_C 36 AGCCAUGCAAGCCCUGAAACCCAUUGCCUUAGUGCG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9MTY_C.MSA.fasta, using single sequence\n   aa_type shape: (36, 6)\n   MSA tensor shape: torch.Size([2, 36, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 65 8QHU_7 171 AACGUGUCGCGAUGGAUGACUUGGCUUCCUAUUUCGUUGAAGAACGCAGUAAAGUGCGAUAAGUGGUAUCAAUUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_7.MSA.fasta, using single sequence\n   aa_type shape: (171, 6)\n   MSA tensor shape: torch.Size([2, 171, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([171, 6])\nout: (171, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n0 66 9GBW_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBW_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 67 8T5O_A 124 GUGAGGUGCAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8T5O_A.MSA.fasta\n   Target sequence length: 124\n   Row 0: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 1: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 2: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 3: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 4: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   MSA loaded: 100 sequences, 124 positions\n   MSA tensor shape: torch.Size([100, 124, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([124, 6])\nout: (124, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n0 68 9DPL_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPL_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 69 8WFA_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFA_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 70 9ISV_A 580 AAUGUGCUAGGGUAGUAUGGGAUAAGUCGAUAACUAAAAUGAAUUGGGAUACUGAUUGAUUUUAGUGGUGGAUUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9ISV_A.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 71 9AR6_B 147 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR6_B.MSA.fasta\n   Target sequence length: 147\n   Row 0: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 1: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 2: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 3: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 4: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   MSA loaded: 50 sequences, 147 positions\n   MSA tensor shape: torch.Size([50, 147, 6])\n   ✅ Using real MSA with 50 sequences\n   Seq features shape: torch.Size([147, 6])\nout: (147, 3, 3)\n   📊 Processed with MSA depth: 50\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n0 72 9DE5_D 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_D.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 73 8ZDR_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZDR_A.MSA.fasta\n   Target sequence length: 159\n   Row 0: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 1: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 2: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 3: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 4: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   MSA loaded: 45 sequences, 159 positions\n   MSA tensor shape: torch.Size([45, 159, 6])\n   ✅ Using real MSA with 45 sequences\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with MSA depth: 45\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n0 74 8WFB_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFB_B.MSA.fasta\n   Target sequence length: 66\n   Row 0: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 1: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 2: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 3: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 4: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   MSA loaded: 20 sequences, 66 positions\n   MSA tensor shape: torch.Size([20, 66, 6])\n   ✅ Using real MSA with 20 sequences\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with MSA depth: 20\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 75 9L5R_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 76 9IS7_B 582 GCCUAGCGAAAGUGGGCGACGAGUGCAACACCUGGAGGCUGAGGGCGUCAGUGAUGGCGUCAGGCAUGCGGGAGA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9IS7_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (582, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 77 9GC0_Q 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GC0_Q.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 78 8YIH_C 96 GUAGCAUGAUCAUCCGAAUCCUCUACAACGAUUUUUUCCCCAUUAUUGAAUAAUGGCAAAAAAUCCUUGUAGUGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIH_C.MSA.fasta\n   Target sequence length: 96\n   Row 0: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 1: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 2: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 3: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 4: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   MSA loaded: 100 sequences, 96 positions\n   MSA tensor shape: torch.Size([100, 96, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([96, 6])\nout: (96, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 79 9HNY_CA 620 UUUGUAUAAAAUUUUAGGAAUAGUUAAUAAUAAUUUAUAAUUUUGAUUAGAUUGUUUUGUUAAUGCUAUUAGAUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9HNY_CA.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (620, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n0 80 8VK7_B 118 GUUACGGCGGUCCAUAGCGGCAGGGAAACGCCCGGUCCCAUCCCGAACCCGGAAGCUAAGCCUGCCAGCGCCGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VK7_B.MSA.fasta\n   Target sequence length: 118\n   Row 0: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 1: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 2: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 3: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 4: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   MSA loaded: 100 sequences, 118 positions\n   MSA tensor shape: torch.Size([100, 118, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([118, 6])\nout: (118, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n0 81 8WF8_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF8_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 82 8QHU_S4 76 GCGCGGAUAGCUCAGUCGGUAGAGCAGGGGAUUGAAAAUCCCCGUGUCCUUGGUUCGAUUCCGAGUCCGCGCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_S4.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 83 8ZAU_A 69 GCGGGCUGACCGACCCCCCGAGUUCGCUUGGGGACAACUAGACAUACAGUAUGAAAAUGCUGAGCCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZAU_A.MSA.fasta, using single sequence\n   aa_type shape: (69, 6)\n   MSA tensor shape: torch.Size([2, 69, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([69, 6])\nout: (69, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 84 9AR7_B 120 GUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR7_B.MSA.fasta, using single sequence\n   aa_type shape: (120, 6)\n   MSA tensor shape: torch.Size([2, 120, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 85 9AR4_B 149 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAUA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR4_B.MSA.fasta, using single sequence\n   aa_type shape: (149, 6)\n   MSA tensor shape: torch.Size([2, 149, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([149, 6])\nout: (149, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n0 86 8Y9N_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9N_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 2 sequences, 62 positions\n   MSA tensor shape: torch.Size([2, 62, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 87 8QHU_4 184 GUGAGAUUGUGAAGGGAUCUCGCAGGUAUCGUGAGGGAAGUAUGGGGUAGUACGAGAGGAACUCCCAUGCCGUGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_4.MSA.fasta\n   Target sequence length: 184\n   Row 0: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 1: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 2: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 3: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 4: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   MSA loaded: 100 sequences, 184 positions\n   MSA tensor shape: torch.Size([100, 184, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([184, 6])\nout: (184, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 10 sec\ngpu_mem_used: 4 GB\n0 88 8RWG_C 121 UGCUUGACGAUCAUAGAGCGUUGGAACCACCUGAUCCCUUCCCGAACUCAGAAGUGAAACGACGCAUCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RWG_C.MSA.fasta\n   Target sequence length: 121\n   Row 0: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 1: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 2: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 3: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 4: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   MSA loaded: 100 sequences, 121 positions\n   MSA tensor shape: torch.Size([100, 121, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([121, 6])\nout: (121, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n0 89 8YII_C 104 GGUUCGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YII_C.MSA.fasta, using single sequence\n   aa_type shape: (104, 6)\n   MSA tensor shape: torch.Size([2, 104, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 90 9DCF_C 90 GGUAAAACAGCCUGUGGGUUGAUCCCACCCACAGGGCCCAUUGGGCGCUAGCACUCUGGUAUCACGGUACCUUUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DCF_C.MSA.fasta\n   Target sequence length: 90\n   Row 0: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 1: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 2: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 3: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 4: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   MSA loaded: 100 sequences, 90 positions\n   MSA tensor shape: torch.Size([100, 90, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([90, 6])\nout: (90, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 91 9L5T_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5T_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n0 92 8WF9_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF9_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n0 93 9GBZ_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBZ_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n/kaggle/working/drfold/model_hub/cfg_97/model_1\n<All keys matched successfully>\n1 0 9L5R_2 193 AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUUUUCAGUUUAAUCUCUGAAACUGCUCUACG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_2.MSA.fasta\n   Target sequence length: 193\n   Row 0: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 1: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 2: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 3: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 4: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   MSA loaded: 100 sequences, 193 positions\n   MSA tensor shape: torch.Size([100, 193, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([193, 6])\nout: (193, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 10 sec\ngpu_mem_used: 4 GB\n1 1 9GFT_AU 76 GGGGCUAUAGCUCAGCUGGGAGAGCGCUUGCAUGGCAUGCAAGAGGUCAGCGGUUCGAUCCCGCUUAGCUCCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_AU.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 2 9L0R_K 700 AGGUGCUGUGUCAGCAGUACGUGCGACGAGUGUGUAGGAGGUAUAAACUGACGAAGUUCAAGGGUGGUACCAAGA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L0R_K.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 3 9GFT_A3 77 GGCUACGUAGCUCAGUUGGUUAGAGCACAUCACUCAUAAUGAUGGGGUCACAGGUUCGAAUCCCGUCGUAGCCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_A3.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 4 9B2K_B 70 AAACAGCAUAGCAAGUUAAAAUAAGGCUAGUCCGUUAUCAACUUGAAAAAGUGGCACCGAGUCGGUGCUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B2K_B.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 5 9B0S_Et 75 GCCCGGAUAGCUCAGCUGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0S_Et.MSA.fasta\n   Target sequence length: 75\n   Row 0: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 1: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 2: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 3: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 4: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   MSA loaded: 100 sequences, 75 positions\n   MSA tensor shape: torch.Size([100, 75, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([75, 6])\nout: (75, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 6 9J3T_B 580 AGAUUUAAACCUAGUGAAGAGCAUUUGAACAAUGUGCUAGGGUAGUAUGGGAUAAGUCGAUAACUAAAAUGAAUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3T_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 1: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 2: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 3: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 4: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   MSA loaded: 100 sequences, 580 positions\n⚠️ MSA length 580 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 7 9LCR_B 578 GUGCUGGCUUACGACGCCCAGUUGUGGGCUGGUGCUGGGAGAAAGAAGGCUGGGGCAAUCACAAAGGCAUGUGGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LCR_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 1: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 2: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 3: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 4: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   MSA loaded: 100 sequences, 578 positions\n⚠️ MSA length 578 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (578, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 8 8KEB_A 72 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KEB_A.MSA.fasta\n   Target sequence length: 72\n   Row 0: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 1: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 2: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 3: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 4: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   MSA loaded: 100 sequences, 72 positions\n   MSA tensor shape: torch.Size([100, 72, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([72, 6])\nout: (72, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 9 9L5S_5 116 UUGGAGUAGGCCAGCUCAGACCGAACUCAUUUCCUGCCUUUUACCGGAUGUGACCGUGAGUUGGCCUGAAAUACU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_5.MSA.fasta\n   Target sequence length: 116\n   Row 0: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 1: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 2: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 3: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 4: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   MSA loaded: 100 sequences, 116 positions\n   MSA tensor shape: torch.Size([100, 116, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([116, 6])\nout: (116, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n1 10 8VXZ_C 36 GCGUACGAAGGAGAGGAGAGGAAGAGGAGAGUACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VXZ_C.MSA.fasta\n   Target sequence length: 36\n   Row 0: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 1: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 2: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 3: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 4: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   MSA loaded: 100 sequences, 36 positions\n   MSA tensor shape: torch.Size([100, 36, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 11 9J6Y_E 550 AUACGUCUCUUAAUAACUACUAUGACGAAAUAUACGGAUACGUUUAUUUUUUCUAAUUUCCACUUGGGUAGUACU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J6Y_E.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 1: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 2: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 3: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 4: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   MSA loaded: 63 sequences, 550 positions\n⚠️ MSA length 550 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (550, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 12 8QHU_5 135 UUACGUCCCUCUCCAAACGAGAGAACAUGCAUGGGCUGGCAUGAGCGGCAUGCUUCACUUCGGUGGGGCUCGAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_5.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 64 sequences, 135 positions\n   MSA tensor shape: torch.Size([64, 135, 6])\n   ✅ Using real MSA with 64 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 64\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n1 13 9GHF_Z 77 CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GHF_Z.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 14 9KPO_B 255 AAAAAACUUACUAUUAUAUUUGUAACAAAUUUUAUACAUAAGAUAAAUUCGUAUGUAUAGCCGUUCUGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9KPO_B.MSA.fasta\n   Target sequence length: 255\n   Row 0: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 1: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 2: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 3: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 4: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   MSA loaded: 94 sequences, 255 positions\n   MSA tensor shape: torch.Size([94, 255, 6])\n   ✅ Using real MSA with 94 sequences\n   Seq features shape: torch.Size([255, 6])\nout: (255, 3, 3)\n   📊 Processed with MSA depth: 94\ntime_taken:  0 min 17 sec\ngpu_mem_used: 6 GB\n1 15 9N2B_5 120 UGCCUGGCGGCAGUAGCGCGGUGGUCCCACCUGACCCCAUGCCGAACUCAGAAGUGAAACGCCGUAGCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2B_5.MSA.fasta\n   Target sequence length: 120\n   Row 0: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 1: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 2: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 3: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 4: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   MSA loaded: 100 sequences, 120 positions\n   MSA tensor shape: torch.Size([100, 120, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n1 16 9N2C_Pt 77 CGCGGGGUGGAGCAGCCCGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2C_Pt.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 17 9B1Y_4 81 ACGGUUUGUGUAGGAUAGGUGGGAGACUGUGAAGCUCACACGCCAGUGUGGGUGGAGUCGUUGUUGAAAUACCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B1Y_4.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 18 9G06_a 73 GCGGGGUGGAGCAGGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9G06_a.MSA.fasta\n   Target sequence length: 73\n   Row 0: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 1: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 2: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 3: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 4: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   MSA loaded: 100 sequences, 73 positions\n   MSA tensor shape: torch.Size([100, 73, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([73, 6])\nout: (73, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 19 9DE8_A 57 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE8_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 20 9B83_C 39 GGGAGCCCCCCXGCUUCACUGCAUGGAAGCUAAAGGGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B83_C.MSA.fasta\n   Target sequence length: 39\n   Row 0: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 1: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 2: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 3: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 4: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   MSA loaded: 23 sequences, 39 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (39, 6)\n   MSA tensor shape: torch.Size([2, 39, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([39, 6])\nout: (39, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 21 8ZMH_A 169 CGUGGUUGACACGCAGACCUCUUACAAGAGUGUCUAGGUGCCUUUGAGAGUUACUCUUUGCUCUCUUCGGAAGAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZMH_A.MSA.fasta, using single sequence\n   aa_type shape: (169, 6)\n   MSA tensor shape: torch.Size([2, 169, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([169, 6])\nout: (169, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n1 22 9E2Y_F 35 AUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Y_F.MSA.fasta, using single sequence\n   aa_type shape: (35, 6)\n   MSA tensor shape: torch.Size([2, 35, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 23 9DE7_A 58 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE7_A.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 100 sequences, 58 positions\n   MSA tensor shape: torch.Size([100, 58, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 24 8Y9L_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9L_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 25 9FIB_Y 15 AAUGUUUGAAAAAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FIB_Y.MSA.fasta\n   Target sequence length: 15\n   Row 0: seq length 15, parse_seq_onehot shape (15, 6), expected L=15\n   MSA loaded: 1 sequences, 15 positions\n   MSA tensor shape: torch.Size([1, 15, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([15, 6])\nout: (15, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 26 9J3R_B 580 AUGAAUUGGGAUACUGAUUGAUUUUAGUGGUGGAUUUUACAGCAAUGUAAAAAGGACUAAUAGUAAAAGCUAUUA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3R_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 27 9DPB_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPB_C.MSA.fasta, using single sequence\n   aa_type shape: (76, 6)\n   MSA tensor shape: torch.Size([2, 76, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 28 8XTP_A 133 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_A.MSA.fasta, using single sequence\n   aa_type shape: (133, 6)\n   MSA tensor shape: torch.Size([2, 133, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([133, 6])\nout: (133, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 29 8ZTV_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTV_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 30 8Y9M_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9M_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 31 8ZQ9_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZQ9_A.MSA.fasta, using single sequence\n   aa_type shape: (159, 6)\n   MSA tensor shape: torch.Size([2, 159, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n1 32 8XTP_B 595 GGUGAAAGUCGGUGAAAGACCGACCGGUGGGGCGUAUCGAAAGAGCGCAACACCUGCCGCACAGGAUGGCUUCUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (595, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 33 9B89_C 35 GGGCUUUCGUUUUCCUAUAUAGGAAAAUGAACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B89_C.MSA.fasta\n   Target sequence length: 35\n   Row 0: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 1: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 2: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 3: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 4: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   MSA loaded: 100 sequences, 35 positions\n   MSA tensor shape: torch.Size([100, 35, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 34 8SYK_C 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_C.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n1 35 9FN3_B 58 GGAGUCAUGGCUCAGGGCUGUUCGCAGCCGCUGCAGUCAGUCGAAAGACUGXGACUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FN3_B.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 34 sequences, 58 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (58, 6)\n   MSA tensor shape: torch.Size([2, 58, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 36 8QHU_3 216 UAUUAGUGGUAAUGCGAAACACUUGCCAGGUAACAAAUCAAUCCUCCCACGGUGAGCUUUCUUUUCACCAUAAUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_3.MSA.fasta, using single sequence\n   aa_type shape: (216, 6)\n   MSA tensor shape: torch.Size([2, 216, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([216, 6])\nout: (216, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 07 sec\ngpu_mem_used: 3 GB\n1 37 9DRS_C 77 GGCCAGGUAGCUCAGUCGGUAUGAGCGUCCGCCUGAAAAGCGGAAGGUCGGCGGUUCGAUCCCGCCCCUGGCCAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DRS_C.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 38 8XTR_A 145 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTR_A.MSA.fasta\n   Target sequence length: 145\n   Row 0: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 1: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 2: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 3: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 4: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   MSA loaded: 100 sequences, 145 positions\n   MSA tensor shape: torch.Size([100, 145, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([145, 6])\nout: (145, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n1 39 9LMF_F 700 AUAAACUGACGAAGUUCAAGGGUGGUACCAAGAGCGGUCAGCAUGUUGUGCAACGCUGGGAGGAUAUCCCAGUCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LMF_F.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 40 9DE6_B 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_B.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 41 8SYK_B 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_B.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n1 42 9DE6_A 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 43 8R7N_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8R7N_A.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 100 sequences, 135 positions\n   MSA tensor shape: torch.Size([100, 135, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n1 44 8K85_A 56 GGAGACGGUCGGGUCCAGUCGCAACGAUGUUGGCUGUUGAGUAGUGUGUGGGCUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8K85_A.MSA.fasta, using single sequence\n   aa_type shape: (56, 6)\n   MSA tensor shape: torch.Size([2, 56, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([56, 6])\nout: (56, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 45 9FCV_B 81 CGUCGCCGUCCAGCUCGACCAGGAUGGGAAGUUGCAUCUGCCUUCUUUUUGAAAGGUAAAAACAACAUCGUCCAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FCV_B.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 46 9DPA_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPA_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 47 9DE5_C 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_C.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 48 8VZ6_S 50 ACUAGAACCCGCCAAGCCUCUCAACGAUGCUCAAAUGUGCGGGUCGUUUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VZ6_S.MSA.fasta\n   Target sequence length: 50\n   Row 0: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 1: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 2: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 3: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 4: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   MSA loaded: 100 sequences, 50 positions\n   MSA tensor shape: torch.Size([100, 50, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([50, 6])\nout: (50, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 49 8YIG_C 104 GGUUAGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIG_C.MSA.fasta\n   Target sequence length: 104\n   Row 0: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 1: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 2: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 3: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 4: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   MSA loaded: 100 sequences, 104 positions\n   MSA tensor shape: torch.Size([100, 104, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n1 50 9B84_F 66 GGGCUXUUCGUUUUCCUAUUGAGCAUAGCCGCUUCUUCGGCUAUGCUCAAUAGGAAAACGAACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B84_F.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 51 9C8K_2 27 GCCCUGUGGGACCCAGGCCUGGGGGGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9C8K_2.MSA.fasta\n   Target sequence length: 27\n   Row 0: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 1: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 2: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 3: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 4: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   MSA loaded: 29 sequences, 27 positions\n   MSA tensor shape: torch.Size([29, 27, 6])\n   ✅ Using real MSA with 29 sequences\n   Seq features shape: torch.Size([27, 6])\nout: (27, 3, 3)\n   📊 Processed with MSA depth: 29\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 52 9B0Q_AP 71 GUCUCCGUAGUGUAGGGUAUCACGUUCGCCUAACACGCGAAAGGUCCUCGGUUCGAAACCGGGCGGAAACA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0Q_AP.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 53 9E2Z_F 40 GUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Z_F.MSA.fasta\n   Target sequence length: 40\n   Row 0: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 1: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 2: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 3: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 4: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   MSA loaded: 100 sequences, 40 positions\n   MSA tensor shape: torch.Size([100, 40, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([40, 6])\nout: (40, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 54 8Z8Q_B 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAAACCACAAAUUUCUUACUGCGAAUUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8Q_B.MSA.fasta, using single sequence\n   aa_type shape: (71, 6)\n   MSA tensor shape: torch.Size([2, 71, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 55 9E2W_F 48 UCGUGCUGAGUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2W_F.MSA.fasta, using single sequence\n   aa_type shape: (48, 6)\n   MSA tensor shape: torch.Size([2, 48, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([48, 6])\nout: (48, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 56 8KHH_A 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KHH_A.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 57 8Z8U_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8U_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 58 8ZTU_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTU_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 59 9GCL_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGGGCU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCL_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 60 8RRI_Ax 70 UAGGGAAUAGUUUAAAAAACAUCUGACUCACAUUCAGAAGAUGGAGGUUCAAUUCCUCCUUCCCUAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RRI_Ax.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 61 9L5S_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 62 9GCM_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCM_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 63 8Z9K_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGCCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z9K_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 64 9MTY_C 36 AGCCAUGCAAGCCCUGAAACCCAUUGCCUUAGUGCG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9MTY_C.MSA.fasta, using single sequence\n   aa_type shape: (36, 6)\n   MSA tensor shape: torch.Size([2, 36, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 65 8QHU_7 171 AACGUGUCGCGAUGGAUGACUUGGCUUCCUAUUUCGUUGAAGAACGCAGUAAAGUGCGAUAAGUGGUAUCAAUUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_7.MSA.fasta, using single sequence\n   aa_type shape: (171, 6)\n   MSA tensor shape: torch.Size([2, 171, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([171, 6])\nout: (171, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n1 66 9GBW_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBW_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 67 8T5O_A 124 GUGAGGUGCAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8T5O_A.MSA.fasta\n   Target sequence length: 124\n   Row 0: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 1: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 2: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 3: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 4: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   MSA loaded: 100 sequences, 124 positions\n   MSA tensor shape: torch.Size([100, 124, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([124, 6])\nout: (124, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n1 68 9DPL_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPL_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 69 8WFA_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFA_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 70 9ISV_A 580 AAUUGGGAUACUGAUUGAUUUUAGUGGUGGAUUUUACAGCAAUGUAAAAAGGACUAAUAGUAAAAGCUAUUAAUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9ISV_A.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 71 9AR6_B 147 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR6_B.MSA.fasta\n   Target sequence length: 147\n   Row 0: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 1: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 2: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 3: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 4: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   MSA loaded: 50 sequences, 147 positions\n   MSA tensor shape: torch.Size([50, 147, 6])\n   ✅ Using real MSA with 50 sequences\n   Seq features shape: torch.Size([147, 6])\nout: (147, 3, 3)\n   📊 Processed with MSA depth: 50\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n1 72 9DE5_D 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_D.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 73 8ZDR_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZDR_A.MSA.fasta\n   Target sequence length: 159\n   Row 0: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 1: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 2: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 3: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 4: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   MSA loaded: 45 sequences, 159 positions\n   MSA tensor shape: torch.Size([45, 159, 6])\n   ✅ Using real MSA with 45 sequences\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with MSA depth: 45\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n1 74 8WFB_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFB_B.MSA.fasta\n   Target sequence length: 66\n   Row 0: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 1: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 2: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 3: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 4: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   MSA loaded: 20 sequences, 66 positions\n   MSA tensor shape: torch.Size([20, 66, 6])\n   ✅ Using real MSA with 20 sequences\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with MSA depth: 20\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 75 9L5R_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 76 9IS7_B 582 GUCGGUGAAAGCCCGACCCUCGGGGCCUAGCGAAAGUGGGCGACGAGUGCAACACCUGGAGGCUGAGGGCGUCAG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9IS7_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (582, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 77 9GC0_Q 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GC0_Q.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 78 8YIH_C 96 GUAGCAUGAUCAUCCGAAUCCUCUACAACGAUUUUUUCCCCAUUAUUGAAUAAUGGCAAAAAAUCCUUGUAGUGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIH_C.MSA.fasta\n   Target sequence length: 96\n   Row 0: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 1: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 2: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 3: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 4: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   MSA loaded: 100 sequences, 96 positions\n   MSA tensor shape: torch.Size([100, 96, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([96, 6])\nout: (96, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 79 9HNY_CA 620 UGUUUUGUUAAUGCUAUUAGAUGGGUGUGGAAAAAUAAAAAAAAUAAUUAAUAUAUAUCAAUAAUAAAUUAAAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9HNY_CA.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (620, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n1 80 8VK7_B 118 GUUACGGCGGUCCAUAGCGGCAGGGAAACGCCCGGUCCCAUCCCGAACCCGGAAGCUAAGCCUGCCAGCGCCGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VK7_B.MSA.fasta\n   Target sequence length: 118\n   Row 0: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 1: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 2: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 3: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 4: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   MSA loaded: 100 sequences, 118 positions\n   MSA tensor shape: torch.Size([100, 118, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([118, 6])\nout: (118, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n1 81 8WF8_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF8_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 82 8QHU_S4 76 GCGCGGAUAGCUCAGUCGGUAGAGCAGGGGAUUGAAAAUCCCCGUGUCCUUGGUUCGAUUCCGAGUCCGCGCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_S4.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 83 8ZAU_A 69 GCGGGCUGACCGACCCCCCGAGUUCGCUUGGGGACAACUAGACAUACAGUAUGAAAAUGCUGAGCCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZAU_A.MSA.fasta, using single sequence\n   aa_type shape: (69, 6)\n   MSA tensor shape: torch.Size([2, 69, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([69, 6])\nout: (69, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 84 9AR7_B 120 GUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR7_B.MSA.fasta, using single sequence\n   aa_type shape: (120, 6)\n   MSA tensor shape: torch.Size([2, 120, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 85 9AR4_B 149 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAUA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR4_B.MSA.fasta, using single sequence\n   aa_type shape: (149, 6)\n   MSA tensor shape: torch.Size([2, 149, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([149, 6])\nout: (149, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n1 86 8Y9N_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9N_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 2 sequences, 62 positions\n   MSA tensor shape: torch.Size([2, 62, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 87 8QHU_4 184 GUGAGAUUGUGAAGGGAUCUCGCAGGUAUCGUGAGGGAAGUAUGGGGUAGUACGAGAGGAACUCCCAUGCCGUGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_4.MSA.fasta\n   Target sequence length: 184\n   Row 0: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 1: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 2: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 3: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 4: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   MSA loaded: 100 sequences, 184 positions\n   MSA tensor shape: torch.Size([100, 184, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([184, 6])\nout: (184, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 10 sec\ngpu_mem_used: 4 GB\n1 88 8RWG_C 121 UGCUUGACGAUCAUAGAGCGUUGGAACCACCUGAUCCCUUCCCGAACUCAGAAGUGAAACGACGCAUCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RWG_C.MSA.fasta\n   Target sequence length: 121\n   Row 0: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 1: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 2: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 3: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 4: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   MSA loaded: 100 sequences, 121 positions\n   MSA tensor shape: torch.Size([100, 121, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([121, 6])\nout: (121, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n1 89 8YII_C 104 GGUUCGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YII_C.MSA.fasta, using single sequence\n   aa_type shape: (104, 6)\n   MSA tensor shape: torch.Size([2, 104, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 90 9DCF_C 90 GGUAAAACAGCCUGUGGGUUGAUCCCACCCACAGGGCCCAUUGGGCGCUAGCACUCUGGUAUCACGGUACCUUUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DCF_C.MSA.fasta\n   Target sequence length: 90\n   Row 0: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 1: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 2: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 3: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 4: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   MSA loaded: 100 sequences, 90 positions\n   MSA tensor shape: torch.Size([100, 90, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([90, 6])\nout: (90, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 91 9L5T_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5T_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n1 92 8WF9_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF9_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n1 93 9GBZ_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBZ_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n/kaggle/working/drfold/model_hub/cfg_97/model_2\n<All keys matched successfully>\n2 0 9L5R_2 193 AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUUUUCAGUUUAAUCUCUGAAACUGCUCUACG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_2.MSA.fasta\n   Target sequence length: 193\n   Row 0: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 1: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 2: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 3: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 4: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   MSA loaded: 100 sequences, 193 positions\n   MSA tensor shape: torch.Size([100, 193, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([193, 6])\nout: (193, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 11 sec\ngpu_mem_used: 4 GB\n2 1 9GFT_AU 76 GGGGCUAUAGCUCAGCUGGGAGAGCGCUUGCAUGGCAUGCAAGAGGUCAGCGGUUCGAUCCCGCUUAGCUCCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_AU.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 2 9L0R_K 700 UAGCGAUAUGGGAAAGCUUUUGUAGGUGUAUCAACAAGAGCGCCAGUGAUGGUCAAUCUAAGCAAACCAAUCUUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L0R_K.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 3 9GFT_A3 77 GGCUACGUAGCUCAGUUGGUUAGAGCACAUCACUCAUAAUGAUGGGGUCACAGGUUCGAAUCCCGUCGUAGCCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_A3.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 4 9B2K_B 70 AAACAGCAUAGCAAGUUAAAAUAAGGCUAGUCCGUUAUCAACUUGAAAAAGUGGCACCGAGUCGGUGCUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B2K_B.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 5 9B0S_Et 75 GCCCGGAUAGCUCAGCUGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0S_Et.MSA.fasta\n   Target sequence length: 75\n   Row 0: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 1: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 2: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 3: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 4: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   MSA loaded: 100 sequences, 75 positions\n   MSA tensor shape: torch.Size([100, 75, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([75, 6])\nout: (75, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 6 9J3T_B 580 UUGAACAAUGUGCUAGGGUAGUAUGGGAUAAGUCGAUAACUAAAAUGAAUUGGGAUACUGAUUGAUUUUAGUGGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3T_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 1: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 2: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 3: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 4: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   MSA loaded: 100 sequences, 580 positions\n⚠️ MSA length 580 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 7 9LCR_B 578 UGCAGUAUUCUAGUCAGGUAAGUAUAUCUUGAAGGCGGGGCUAAAAAUCCGCUAAAGGGCACAUCGAUGAAGCUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LCR_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 1: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 2: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 3: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 4: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   MSA loaded: 100 sequences, 578 positions\n⚠️ MSA length 578 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (578, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 8 8KEB_A 72 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KEB_A.MSA.fasta\n   Target sequence length: 72\n   Row 0: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 1: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 2: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 3: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 4: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   MSA loaded: 100 sequences, 72 positions\n   MSA tensor shape: torch.Size([100, 72, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([72, 6])\nout: (72, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 9 9L5S_5 116 UUGGAGUAGGCCAGCUCAGACCGAACUCAUUUCCUGCCUUUUACCGGAUGUGACCGUGAGUUGGCCUGAAAUACU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_5.MSA.fasta\n   Target sequence length: 116\n   Row 0: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 1: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 2: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 3: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 4: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   MSA loaded: 100 sequences, 116 positions\n   MSA tensor shape: torch.Size([100, 116, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([116, 6])\nout: (116, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n2 10 8VXZ_C 36 GCGUACGAAGGAGAGGAGAGGAAGAGGAGAGUACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VXZ_C.MSA.fasta\n   Target sequence length: 36\n   Row 0: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 1: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 2: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 3: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 4: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   MSA loaded: 100 sequences, 36 positions\n   MSA tensor shape: torch.Size([100, 36, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 11 9J6Y_E 550 UGUACCGAUGAAGCUAGUGGAUAAGGUGUGACAAGCCGCCUAGCCAUACGUCUCUUAAUAACUACUAUGACGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J6Y_E.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 1: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 2: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 3: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 4: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   MSA loaded: 63 sequences, 550 positions\n⚠️ MSA length 550 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (550, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 12 8QHU_5 135 UUACGUCCCUCUCCAAACGAGAGAACAUGCAUGGGCUGGCAUGAGCGGCAUGCUUCACUUCGGUGGGGCUCGAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_5.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 64 sequences, 135 positions\n   MSA tensor shape: torch.Size([64, 135, 6])\n   ✅ Using real MSA with 64 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 64\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n2 13 9GHF_Z 77 CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GHF_Z.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 14 9KPO_B 255 AAAAAACUUACUAUUAUAUUUGUAACAAAUUUUAUACAUAAGAUAAAUUCGUAUGUAUAGCCGUUCUGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9KPO_B.MSA.fasta\n   Target sequence length: 255\n   Row 0: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 1: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 2: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 3: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 4: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   MSA loaded: 94 sequences, 255 positions\n   MSA tensor shape: torch.Size([94, 255, 6])\n   ✅ Using real MSA with 94 sequences\n   Seq features shape: torch.Size([255, 6])\nout: (255, 3, 3)\n   📊 Processed with MSA depth: 94\ntime_taken:  0 min 18 sec\ngpu_mem_used: 6 GB\n2 15 9N2B_5 120 UGCCUGGCGGCAGUAGCGCGGUGGUCCCACCUGACCCCAUGCCGAACUCAGAAGUGAAACGCCGUAGCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2B_5.MSA.fasta\n   Target sequence length: 120\n   Row 0: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 1: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 2: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 3: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 4: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   MSA loaded: 100 sequences, 120 positions\n   MSA tensor shape: torch.Size([100, 120, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n2 16 9N2C_Pt 77 CGCGGGGUGGAGCAGCCCGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2C_Pt.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 17 9B1Y_4 81 ACGGUUUGUGUAGGAUAGGUGGGAGACUGUGAAGCUCACACGCCAGUGUGGGUGGAGUCGUUGUUGAAAUACCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B1Y_4.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 18 9G06_a 73 GCGGGGUGGAGCAGGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9G06_a.MSA.fasta\n   Target sequence length: 73\n   Row 0: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 1: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 2: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 3: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 4: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   MSA loaded: 100 sequences, 73 positions\n   MSA tensor shape: torch.Size([100, 73, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([73, 6])\nout: (73, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 19 9DE8_A 57 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE8_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 20 9B83_C 39 GGGAGCCCCCCXGCUUCACUGCAUGGAAGCUAAAGGGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B83_C.MSA.fasta\n   Target sequence length: 39\n   Row 0: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 1: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 2: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 3: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 4: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   MSA loaded: 23 sequences, 39 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (39, 6)\n   MSA tensor shape: torch.Size([2, 39, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([39, 6])\nout: (39, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 21 8ZMH_A 169 CGUGGUUGACACGCAGACCUCUUACAAGAGUGUCUAGGUGCCUUUGAGAGUUACUCUUUGCUCUCUUCGGAAGAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZMH_A.MSA.fasta, using single sequence\n   aa_type shape: (169, 6)\n   MSA tensor shape: torch.Size([2, 169, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([169, 6])\nout: (169, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n2 22 9E2Y_F 35 AUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Y_F.MSA.fasta, using single sequence\n   aa_type shape: (35, 6)\n   MSA tensor shape: torch.Size([2, 35, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 23 9DE7_A 58 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE7_A.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 100 sequences, 58 positions\n   MSA tensor shape: torch.Size([100, 58, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 24 8Y9L_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9L_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 25 9FIB_Y 15 AAUGUUUGAAAAAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FIB_Y.MSA.fasta\n   Target sequence length: 15\n   Row 0: seq length 15, parse_seq_onehot shape (15, 6), expected L=15\n   MSA loaded: 1 sequences, 15 positions\n   MSA tensor shape: torch.Size([1, 15, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([15, 6])\nout: (15, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 26 9J3R_B 580 UUGAAAAAUCAAUAGAUUUAAACCUAGUGAAGAGCAUUUGAACAAUGUGCUAGGGUAGUAUGGGAUAAGUCGAUA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3R_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 57 sec\ngpu_mem_used: 14 GB\n2 27 9DPB_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPB_C.MSA.fasta, using single sequence\n   aa_type shape: (76, 6)\n   MSA tensor shape: torch.Size([2, 76, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 28 8XTP_A 133 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_A.MSA.fasta, using single sequence\n   aa_type shape: (133, 6)\n   MSA tensor shape: torch.Size([2, 133, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([133, 6])\nout: (133, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n2 29 8ZTV_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTV_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 30 8Y9M_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9M_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 31 8ZQ9_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZQ9_A.MSA.fasta, using single sequence\n   aa_type shape: (159, 6)\n   MSA tensor shape: torch.Size([2, 159, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n2 32 8XTP_B 595 AGUCGGUGAAAGACCGACCGGUGGGGCGUAUCGAAAGAGCGCAACACCUGCCGCACAGGAUGGCUUCUGAGGUAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (595, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 33 9B89_C 35 GGGCUUUCGUUUUCCUAUAUAGGAAAAUGAACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B89_C.MSA.fasta\n   Target sequence length: 35\n   Row 0: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 1: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 2: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 3: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 4: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   MSA loaded: 100 sequences, 35 positions\n   MSA tensor shape: torch.Size([100, 35, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 34 8SYK_C 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_C.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n2 35 9FN3_B 58 GGAGUCAUGGCUCAGGGCUGUUCGCAGCCGCUGCAGUCAGUCGAAAGACUGXGACUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FN3_B.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 34 sequences, 58 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (58, 6)\n   MSA tensor shape: torch.Size([2, 58, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 36 8QHU_3 216 UAUUAGUGGUAAUGCGAAACACUUGCCAGGUAACAAAUCAAUCCUCCCACGGUGAGCUUUCUUUUCACCAUAAUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_3.MSA.fasta, using single sequence\n   aa_type shape: (216, 6)\n   MSA tensor shape: torch.Size([2, 216, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([216, 6])\nout: (216, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 07 sec\ngpu_mem_used: 3 GB\n2 37 9DRS_C 77 GGCCAGGUAGCUCAGUCGGUAUGAGCGUCCGCCUGAAAAGCGGAAGGUCGGCGGUUCGAUCCCGCCCCUGGCCAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DRS_C.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 38 8XTR_A 145 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTR_A.MSA.fasta\n   Target sequence length: 145\n   Row 0: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 1: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 2: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 3: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 4: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   MSA loaded: 100 sequences, 145 positions\n   MSA tensor shape: torch.Size([100, 145, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([145, 6])\nout: (145, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 06 sec\ngpu_mem_used: 3 GB\n2 39 9LMF_F 700 AACUGACGAAGUUCAAGGGUGGUACCAAGAGCGGUCAGCAUGUUGUGCAACGCUGGGAGGAUAUCCCAGUCAAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LMF_F.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 40 9DE6_B 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_B.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 41 8SYK_B 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_B.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n2 42 9DE6_A 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 43 8R7N_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8R7N_A.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 100 sequences, 135 positions\n   MSA tensor shape: torch.Size([100, 135, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n2 44 8K85_A 56 GGAGACGGUCGGGUCCAGUCGCAACGAUGUUGGCUGUUGAGUAGUGUGUGGGCUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8K85_A.MSA.fasta, using single sequence\n   aa_type shape: (56, 6)\n   MSA tensor shape: torch.Size([2, 56, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([56, 6])\nout: (56, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 45 9FCV_B 81 CGUCGCCGUCCAGCUCGACCAGGAUGGGAAGUUGCAUCUGCCUUCUUUUUGAAAGGUAAAAACAACAUCGUCCAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FCV_B.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 46 9DPA_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPA_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 47 9DE5_C 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_C.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 48 8VZ6_S 50 ACUAGAACCCGCCAAGCCUCUCAACGAUGCUCAAAUGUGCGGGUCGUUUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VZ6_S.MSA.fasta\n   Target sequence length: 50\n   Row 0: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 1: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 2: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 3: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 4: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   MSA loaded: 100 sequences, 50 positions\n   MSA tensor shape: torch.Size([100, 50, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([50, 6])\nout: (50, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 49 8YIG_C 104 GGUUAGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIG_C.MSA.fasta\n   Target sequence length: 104\n   Row 0: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 1: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 2: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 3: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 4: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   MSA loaded: 100 sequences, 104 positions\n   MSA tensor shape: torch.Size([100, 104, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n2 50 9B84_F 66 GGGCUXUUCGUUUUCCUAUUGAGCAUAGCCGCUUCUUCGGCUAUGCUCAAUAGGAAAACGAACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B84_F.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 51 9C8K_2 27 GCCCUGUGGGACCCAGGCCUGGGGGGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9C8K_2.MSA.fasta\n   Target sequence length: 27\n   Row 0: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 1: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 2: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 3: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 4: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   MSA loaded: 29 sequences, 27 positions\n   MSA tensor shape: torch.Size([29, 27, 6])\n   ✅ Using real MSA with 29 sequences\n   Seq features shape: torch.Size([27, 6])\nout: (27, 3, 3)\n   📊 Processed with MSA depth: 29\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 52 9B0Q_AP 71 GUCUCCGUAGUGUAGGGUAUCACGUUCGCCUAACACGCGAAAGGUCCUCGGUUCGAAACCGGGCGGAAACA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0Q_AP.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 53 9E2Z_F 40 GUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Z_F.MSA.fasta\n   Target sequence length: 40\n   Row 0: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 1: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 2: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 3: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 4: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   MSA loaded: 100 sequences, 40 positions\n   MSA tensor shape: torch.Size([100, 40, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([40, 6])\nout: (40, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 54 8Z8Q_B 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAAACCACAAAUUUCUUACUGCGAAUUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8Q_B.MSA.fasta, using single sequence\n   aa_type shape: (71, 6)\n   MSA tensor shape: torch.Size([2, 71, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 55 9E2W_F 48 UCGUGCUGAGUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2W_F.MSA.fasta, using single sequence\n   aa_type shape: (48, 6)\n   MSA tensor shape: torch.Size([2, 48, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([48, 6])\nout: (48, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 56 8KHH_A 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KHH_A.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 57 8Z8U_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8U_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 58 8ZTU_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTU_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 59 9GCL_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGGGCU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCL_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 60 8RRI_Ax 70 UAGGGAAUAGUUUAAAAAACAUCUGACUCACAUUCAGAAGAUGGAGGUUCAAUUCCUCCUUCCCUAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RRI_Ax.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 61 9L5S_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 62 9GCM_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCM_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 63 8Z9K_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGCCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z9K_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 64 9MTY_C 36 AGCCAUGCAAGCCCUGAAACCCAUUGCCUUAGUGCG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9MTY_C.MSA.fasta, using single sequence\n   aa_type shape: (36, 6)\n   MSA tensor shape: torch.Size([2, 36, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 65 8QHU_7 171 AACGUGUCGCGAUGGAUGACUUGGCUUCCUAUUUCGUUGAAGAACGCAGUAAAGUGCGAUAAGUGGUAUCAAUUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_7.MSA.fasta, using single sequence\n   aa_type shape: (171, 6)\n   MSA tensor shape: torch.Size([2, 171, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([171, 6])\nout: (171, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n2 66 9GBW_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBW_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 67 8T5O_A 124 GUGAGGUGCAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8T5O_A.MSA.fasta\n   Target sequence length: 124\n   Row 0: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 1: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 2: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 3: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 4: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   MSA loaded: 100 sequences, 124 positions\n   MSA tensor shape: torch.Size([100, 124, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([124, 6])\nout: (124, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n2 68 9DPL_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPL_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 69 8WFA_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFA_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 70 9ISV_A 580 GAAGAGCAUUUGAACAAUGUGCUAGGGUAGUAUGGGAUAAGUCGAUAACUAAAAUGAAUUGGGAUACUGAUUGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9ISV_A.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 71 9AR6_B 147 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR6_B.MSA.fasta\n   Target sequence length: 147\n   Row 0: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 1: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 2: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 3: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 4: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   MSA loaded: 50 sequences, 147 positions\n   MSA tensor shape: torch.Size([50, 147, 6])\n   ✅ Using real MSA with 50 sequences\n   Seq features shape: torch.Size([147, 6])\nout: (147, 3, 3)\n   📊 Processed with MSA depth: 50\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n2 72 9DE5_D 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_D.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 73 8ZDR_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZDR_A.MSA.fasta\n   Target sequence length: 159\n   Row 0: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 1: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 2: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 3: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 4: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   MSA loaded: 45 sequences, 159 positions\n   MSA tensor shape: torch.Size([45, 159, 6])\n   ✅ Using real MSA with 45 sequences\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with MSA depth: 45\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n2 74 8WFB_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFB_B.MSA.fasta\n   Target sequence length: 66\n   Row 0: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 1: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 2: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 3: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 4: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   MSA loaded: 20 sequences, 66 positions\n   MSA tensor shape: torch.Size([20, 66, 6])\n   ✅ Using real MSA with 20 sequences\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with MSA depth: 20\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 75 9L5R_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 76 9IS7_B 582 GGUAGGAAAUGACCCAGUGACCCUGACAGUUUGGGAAAGUCGGUGAAAGCCCGACCCUCGGGGCCUAGCGAAAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9IS7_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (582, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 77 9GC0_Q 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GC0_Q.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 78 8YIH_C 96 GUAGCAUGAUCAUCCGAAUCCUCUACAACGAUUUUUUCCCCAUUAUUGAAUAAUGGCAAAAAAUCCUUGUAGUGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIH_C.MSA.fasta\n   Target sequence length: 96\n   Row 0: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 1: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 2: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 3: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 4: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   MSA loaded: 100 sequences, 96 positions\n   MSA tensor shape: torch.Size([100, 96, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([96, 6])\nout: (96, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 79 9HNY_CA 620 GUCAAUUGUUAGUAUUCAUAUUAAUUUUUUUAAAUGUUUUAUCAUUUUAUAAAGGUUUAUUUUUGAAAGAUUUUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9HNY_CA.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (620, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n2 80 8VK7_B 118 GUUACGGCGGUCCAUAGCGGCAGGGAAACGCCCGGUCCCAUCCCGAACCCGGAAGCUAAGCCUGCCAGCGCCGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VK7_B.MSA.fasta\n   Target sequence length: 118\n   Row 0: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 1: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 2: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 3: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 4: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   MSA loaded: 100 sequences, 118 positions\n   MSA tensor shape: torch.Size([100, 118, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([118, 6])\nout: (118, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n2 81 8WF8_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF8_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 82 8QHU_S4 76 GCGCGGAUAGCUCAGUCGGUAGAGCAGGGGAUUGAAAAUCCCCGUGUCCUUGGUUCGAUUCCGAGUCCGCGCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_S4.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 83 8ZAU_A 69 GCGGGCUGACCGACCCCCCGAGUUCGCUUGGGGACAACUAGACAUACAGUAUGAAAAUGCUGAGCCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZAU_A.MSA.fasta, using single sequence\n   aa_type shape: (69, 6)\n   MSA tensor shape: torch.Size([2, 69, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([69, 6])\nout: (69, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 84 9AR7_B 120 GUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR7_B.MSA.fasta, using single sequence\n   aa_type shape: (120, 6)\n   MSA tensor shape: torch.Size([2, 120, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 85 9AR4_B 149 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAUA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR4_B.MSA.fasta, using single sequence\n   aa_type shape: (149, 6)\n   MSA tensor shape: torch.Size([2, 149, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([149, 6])\nout: (149, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n2 86 8Y9N_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9N_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 2 sequences, 62 positions\n   MSA tensor shape: torch.Size([2, 62, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 87 8QHU_4 184 GUGAGAUUGUGAAGGGAUCUCGCAGGUAUCGUGAGGGAAGUAUGGGGUAGUACGAGAGGAACUCCCAUGCCGUGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_4.MSA.fasta\n   Target sequence length: 184\n   Row 0: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 1: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 2: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 3: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 4: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   MSA loaded: 100 sequences, 184 positions\n   MSA tensor shape: torch.Size([100, 184, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([184, 6])\nout: (184, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 10 sec\ngpu_mem_used: 4 GB\n2 88 8RWG_C 121 UGCUUGACGAUCAUAGAGCGUUGGAACCACCUGAUCCCUUCCCGAACUCAGAAGUGAAACGACGCAUCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RWG_C.MSA.fasta\n   Target sequence length: 121\n   Row 0: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 1: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 2: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 3: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 4: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   MSA loaded: 100 sequences, 121 positions\n   MSA tensor shape: torch.Size([100, 121, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([121, 6])\nout: (121, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n2 89 8YII_C 104 GGUUCGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YII_C.MSA.fasta, using single sequence\n   aa_type shape: (104, 6)\n   MSA tensor shape: torch.Size([2, 104, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 90 9DCF_C 90 GGUAAAACAGCCUGUGGGUUGAUCCCACCCACAGGGCCCAUUGGGCGCUAGCACUCUGGUAUCACGGUACCUUUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DCF_C.MSA.fasta\n   Target sequence length: 90\n   Row 0: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 1: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 2: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 3: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 4: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   MSA loaded: 100 sequences, 90 positions\n   MSA tensor shape: torch.Size([100, 90, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([90, 6])\nout: (90, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 91 9L5T_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5T_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n2 92 8WF9_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF9_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n2 93 9GBZ_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBZ_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n/kaggle/working/drfold/model_hub/cfg_97/model_8\n<All keys matched successfully>\n3 0 9L5R_2 193 AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUUUUCAGUUUAAUCUCUGAAACUGCUCUACG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_2.MSA.fasta\n   Target sequence length: 193\n   Row 0: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 1: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 2: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 3: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 4: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   MSA loaded: 100 sequences, 193 positions\n   MSA tensor shape: torch.Size([100, 193, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([193, 6])\nout: (193, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 11 sec\ngpu_mem_used: 4 GB\n3 1 9GFT_AU 76 GGGGCUAUAGCUCAGCUGGGAGAGCGCUUGCAUGGCAUGCAAGAGGUCAGCGGUUCGAUCCCGCUUAGCUCCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_AU.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 2 9L0R_K 700 UCAACAAGAGCGCCAGUGAUGGUCAAUCUAAGCAAACCAAUCUUUAUGUAAUCAGUAAUGGUUACAGCACACGAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L0R_K.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 3 9GFT_A3 77 GGCUACGUAGCUCAGUUGGUUAGAGCACAUCACUCAUAAUGAUGGGGUCACAGGUUCGAAUCCCGUCGUAGCCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_A3.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 4 9B2K_B 70 AAACAGCAUAGCAAGUUAAAAUAAGGCUAGUCCGUUAUCAACUUGAAAAAGUGGCACCGAGUCGGUGCUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B2K_B.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 5 9B0S_Et 75 GCCCGGAUAGCUCAGCUGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0S_Et.MSA.fasta\n   Target sequence length: 75\n   Row 0: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 1: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 2: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 3: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 4: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   MSA loaded: 100 sequences, 75 positions\n   MSA tensor shape: torch.Size([100, 75, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([75, 6])\nout: (75, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 6 9J3T_B 580 AGUCGAUAACUAAAAUGAAUUGGGAUACUGAUUGAUUUUAGUGGUGGAUUUUACAGCAAUGUAAAAAGGACUAAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3T_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 1: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 2: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 3: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 4: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   MSA loaded: 100 sequences, 580 positions\n⚠️ MSA length 580 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 7 9LCR_B 578 AAAUCCGCUAAAGGGCACAUCGAUGAAGCUUCUGGUGCUGGCUUACGACGCCCAGUUGUGGGCUGGUGCUGGGAG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LCR_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 1: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 2: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 3: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 4: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   MSA loaded: 100 sequences, 578 positions\n⚠️ MSA length 578 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (578, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 8 8KEB_A 72 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KEB_A.MSA.fasta\n   Target sequence length: 72\n   Row 0: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 1: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 2: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 3: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 4: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   MSA loaded: 100 sequences, 72 positions\n   MSA tensor shape: torch.Size([100, 72, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([72, 6])\nout: (72, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 9 9L5S_5 116 UUGGAGUAGGCCAGCUCAGACCGAACUCAUUUCCUGCCUUUUACCGGAUGUGACCGUGAGUUGGCCUGAAAUACU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_5.MSA.fasta\n   Target sequence length: 116\n   Row 0: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 1: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 2: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 3: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 4: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   MSA loaded: 100 sequences, 116 positions\n   MSA tensor shape: torch.Size([100, 116, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([116, 6])\nout: (116, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n3 10 8VXZ_C 36 GCGUACGAAGGAGAGGAGAGGAAGAGGAGAGUACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VXZ_C.MSA.fasta\n   Target sequence length: 36\n   Row 0: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 1: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 2: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 3: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 4: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   MSA loaded: 100 sequences, 36 positions\n   MSA tensor shape: torch.Size([100, 36, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 11 9J6Y_E 550 GACAAGCCGCCUAGCCAUACGUCUCUUAAUAACUACUAUGACGAAAUAUACGGAUACGUUUAUUUUUUCUAAUUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J6Y_E.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 1: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 2: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 3: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 4: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   MSA loaded: 63 sequences, 550 positions\n⚠️ MSA length 550 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (550, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 12 8QHU_5 135 UUACGUCCCUCUCCAAACGAGAGAACAUGCAUGGGCUGGCAUGAGCGGCAUGCUUCACUUCGGUGGGGCUCGAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_5.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 64 sequences, 135 positions\n   MSA tensor shape: torch.Size([64, 135, 6])\n   ✅ Using real MSA with 64 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 64\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n3 13 9GHF_Z 77 CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GHF_Z.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 14 9KPO_B 255 AAAAAACUUACUAUUAUAUUUGUAACAAAUUUUAUACAUAAGAUAAAUUCGUAUGUAUAGCCGUUCUGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9KPO_B.MSA.fasta\n   Target sequence length: 255\n   Row 0: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 1: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 2: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 3: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 4: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   MSA loaded: 94 sequences, 255 positions\n   MSA tensor shape: torch.Size([94, 255, 6])\n   ✅ Using real MSA with 94 sequences\n   Seq features shape: torch.Size([255, 6])\nout: (255, 3, 3)\n   📊 Processed with MSA depth: 94\ntime_taken:  0 min 17 sec\ngpu_mem_used: 6 GB\n3 15 9N2B_5 120 UGCCUGGCGGCAGUAGCGCGGUGGUCCCACCUGACCCCAUGCCGAACUCAGAAGUGAAACGCCGUAGCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2B_5.MSA.fasta\n   Target sequence length: 120\n   Row 0: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 1: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 2: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 3: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 4: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   MSA loaded: 100 sequences, 120 positions\n   MSA tensor shape: torch.Size([100, 120, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n3 16 9N2C_Pt 77 CGCGGGGUGGAGCAGCCCGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2C_Pt.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 17 9B1Y_4 81 ACGGUUUGUGUAGGAUAGGUGGGAGACUGUGAAGCUCACACGCCAGUGUGGGUGGAGUCGUUGUUGAAAUACCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B1Y_4.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 18 9G06_a 73 GCGGGGUGGAGCAGGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9G06_a.MSA.fasta\n   Target sequence length: 73\n   Row 0: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 1: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 2: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 3: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 4: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   MSA loaded: 100 sequences, 73 positions\n   MSA tensor shape: torch.Size([100, 73, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([73, 6])\nout: (73, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 19 9DE8_A 57 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE8_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 20 9B83_C 39 GGGAGCCCCCCXGCUUCACUGCAUGGAAGCUAAAGGGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B83_C.MSA.fasta\n   Target sequence length: 39\n   Row 0: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 1: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 2: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 3: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 4: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   MSA loaded: 23 sequences, 39 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (39, 6)\n   MSA tensor shape: torch.Size([2, 39, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([39, 6])\nout: (39, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 21 8ZMH_A 169 CGUGGUUGACACGCAGACCUCUUACAAGAGUGUCUAGGUGCCUUUGAGAGUUACUCUUUGCUCUCUUCGGAAGAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZMH_A.MSA.fasta, using single sequence\n   aa_type shape: (169, 6)\n   MSA tensor shape: torch.Size([2, 169, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([169, 6])\nout: (169, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n3 22 9E2Y_F 35 AUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Y_F.MSA.fasta, using single sequence\n   aa_type shape: (35, 6)\n   MSA tensor shape: torch.Size([2, 35, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 23 9DE7_A 58 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE7_A.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 100 sequences, 58 positions\n   MSA tensor shape: torch.Size([100, 58, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 24 8Y9L_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9L_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 25 9FIB_Y 15 AAUGUUUGAAAAAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FIB_Y.MSA.fasta\n   Target sequence length: 15\n   Row 0: seq length 15, parse_seq_onehot shape (15, 6), expected L=15\n   MSA loaded: 1 sequences, 15 positions\n   MSA tensor shape: torch.Size([1, 15, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([15, 6])\nout: (15, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 26 9J3R_B 580 AACAAUGUGCUAGGGUAGUAUGGGAUAAGUCGAUAACUAAAAUGAAUUGGGAUACUGAUUGAUUUUAGUGGUGGA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3R_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 27 9DPB_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPB_C.MSA.fasta, using single sequence\n   aa_type shape: (76, 6)\n   MSA tensor shape: torch.Size([2, 76, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 28 8XTP_A 133 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_A.MSA.fasta, using single sequence\n   aa_type shape: (133, 6)\n   MSA tensor shape: torch.Size([2, 133, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([133, 6])\nout: (133, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 29 8ZTV_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTV_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 30 8Y9M_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9M_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 31 8ZQ9_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZQ9_A.MSA.fasta, using single sequence\n   aa_type shape: (159, 6)\n   MSA tensor shape: torch.Size([2, 159, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n3 32 8XTP_B 595 GGCCCAGUGACCUGGUCAAUGGUGAAAGUCGGUGAAAGACCGACCGGUGGGGCGUAUCGAAAGAGCGCAACACCU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (595, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 33 9B89_C 35 GGGCUUUCGUUUUCCUAUAUAGGAAAAUGAACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B89_C.MSA.fasta\n   Target sequence length: 35\n   Row 0: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 1: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 2: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 3: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 4: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   MSA loaded: 100 sequences, 35 positions\n   MSA tensor shape: torch.Size([100, 35, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 34 8SYK_C 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_C.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n3 35 9FN3_B 58 GGAGUCAUGGCUCAGGGCUGUUCGCAGCCGCUGCAGUCAGUCGAAAGACUGXGACUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FN3_B.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 34 sequences, 58 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (58, 6)\n   MSA tensor shape: torch.Size([2, 58, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 36 8QHU_3 216 UAUUAGUGGUAAUGCGAAACACUUGCCAGGUAACAAAUCAAUCCUCCCACGGUGAGCUUUCUUUUCACCAUAAUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_3.MSA.fasta, using single sequence\n   aa_type shape: (216, 6)\n   MSA tensor shape: torch.Size([2, 216, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([216, 6])\nout: (216, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 07 sec\ngpu_mem_used: 3 GB\n3 37 9DRS_C 77 GGCCAGGUAGCUCAGUCGGUAUGAGCGUCCGCCUGAAAAGCGGAAGGUCGGCGGUUCGAUCCCGCCCCUGGCCAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DRS_C.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 38 8XTR_A 145 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTR_A.MSA.fasta\n   Target sequence length: 145\n   Row 0: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 1: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 2: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 3: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 4: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   MSA loaded: 100 sequences, 145 positions\n   MSA tensor shape: torch.Size([100, 145, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([145, 6])\nout: (145, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 06 sec\ngpu_mem_used: 3 GB\n3 39 9LMF_F 700 GCCGUCUCAAUAGUGGCUUAGCACAGAUAAUCCAUAGCGAUAUGGGAAAGCUUUUGUAGGUGUAUCAACAAGAGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LMF_F.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 40 9DE6_B 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_B.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 41 8SYK_B 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_B.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n3 42 9DE6_A 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 43 8R7N_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8R7N_A.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 100 sequences, 135 positions\n   MSA tensor shape: torch.Size([100, 135, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n3 44 8K85_A 56 GGAGACGGUCGGGUCCAGUCGCAACGAUGUUGGCUGUUGAGUAGUGUGUGGGCUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8K85_A.MSA.fasta, using single sequence\n   aa_type shape: (56, 6)\n   MSA tensor shape: torch.Size([2, 56, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([56, 6])\nout: (56, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 45 9FCV_B 81 CGUCGCCGUCCAGCUCGACCAGGAUGGGAAGUUGCAUCUGCCUUCUUUUUGAAAGGUAAAAACAACAUCGUCCAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FCV_B.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 46 9DPA_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPA_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 47 9DE5_C 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_C.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 48 8VZ6_S 50 ACUAGAACCCGCCAAGCCUCUCAACGAUGCUCAAAUGUGCGGGUCGUUUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VZ6_S.MSA.fasta\n   Target sequence length: 50\n   Row 0: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 1: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 2: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 3: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 4: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   MSA loaded: 100 sequences, 50 positions\n   MSA tensor shape: torch.Size([100, 50, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([50, 6])\nout: (50, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 49 8YIG_C 104 GGUUAGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIG_C.MSA.fasta\n   Target sequence length: 104\n   Row 0: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 1: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 2: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 3: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 4: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   MSA loaded: 100 sequences, 104 positions\n   MSA tensor shape: torch.Size([100, 104, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n3 50 9B84_F 66 GGGCUXUUCGUUUUCCUAUUGAGCAUAGCCGCUUCUUCGGCUAUGCUCAAUAGGAAAACGAACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B84_F.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 51 9C8K_2 27 GCCCUGUGGGACCCAGGCCUGGGGGGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9C8K_2.MSA.fasta\n   Target sequence length: 27\n   Row 0: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 1: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 2: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 3: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 4: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   MSA loaded: 29 sequences, 27 positions\n   MSA tensor shape: torch.Size([29, 27, 6])\n   ✅ Using real MSA with 29 sequences\n   Seq features shape: torch.Size([27, 6])\nout: (27, 3, 3)\n   📊 Processed with MSA depth: 29\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 52 9B0Q_AP 71 GUCUCCGUAGUGUAGGGUAUCACGUUCGCCUAACACGCGAAAGGUCCUCGGUUCGAAACCGGGCGGAAACA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0Q_AP.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 53 9E2Z_F 40 GUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Z_F.MSA.fasta\n   Target sequence length: 40\n   Row 0: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 1: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 2: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 3: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 4: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   MSA loaded: 100 sequences, 40 positions\n   MSA tensor shape: torch.Size([100, 40, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([40, 6])\nout: (40, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 54 8Z8Q_B 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAAACCACAAAUUUCUUACUGCGAAUUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8Q_B.MSA.fasta, using single sequence\n   aa_type shape: (71, 6)\n   MSA tensor shape: torch.Size([2, 71, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 55 9E2W_F 48 UCGUGCUGAGUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2W_F.MSA.fasta, using single sequence\n   aa_type shape: (48, 6)\n   MSA tensor shape: torch.Size([2, 48, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([48, 6])\nout: (48, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 56 8KHH_A 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KHH_A.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 57 8Z8U_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8U_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 58 8ZTU_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTU_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 59 9GCL_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGGGCU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCL_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 60 8RRI_Ax 70 UAGGGAAUAGUUUAAAAAACAUCUGACUCACAUUCAGAAGAUGGAGGUUCAAUUCCUCCUUCCCUAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RRI_Ax.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 61 9L5S_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 62 9GCM_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCM_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 63 8Z9K_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGCCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z9K_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 64 9MTY_C 36 AGCCAUGCAAGCCCUGAAACCCAUUGCCUUAGUGCG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9MTY_C.MSA.fasta, using single sequence\n   aa_type shape: (36, 6)\n   MSA tensor shape: torch.Size([2, 36, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 65 8QHU_7 171 AACGUGUCGCGAUGGAUGACUUGGCUUCCUAUUUCGUUGAAGAACGCAGUAAAGUGCGAUAAGUGGUAUCAAUUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_7.MSA.fasta, using single sequence\n   aa_type shape: (171, 6)\n   MSA tensor shape: torch.Size([2, 171, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([171, 6])\nout: (171, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n3 66 9GBW_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBW_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 67 8T5O_A 124 GUGAGGUGCAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8T5O_A.MSA.fasta\n   Target sequence length: 124\n   Row 0: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 1: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 2: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 3: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 4: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   MSA loaded: 100 sequences, 124 positions\n   MSA tensor shape: torch.Size([100, 124, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([124, 6])\nout: (124, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n3 68 9DPL_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPL_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 69 8WFA_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFA_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 70 9ISV_A 580 AUGGGAUAAGUCGAUAACUAAAAUGAAUUGGGAUACUGAUUGAUUUUAGUGGUGGAUUUUACAGCAAUGUAAAAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9ISV_A.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 71 9AR6_B 147 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR6_B.MSA.fasta\n   Target sequence length: 147\n   Row 0: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 1: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 2: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 3: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 4: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   MSA loaded: 50 sequences, 147 positions\n   MSA tensor shape: torch.Size([50, 147, 6])\n   ✅ Using real MSA with 50 sequences\n   Seq features shape: torch.Size([147, 6])\nout: (147, 3, 3)\n   📊 Processed with MSA depth: 50\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n3 72 9DE5_D 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_D.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 73 8ZDR_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZDR_A.MSA.fasta\n   Target sequence length: 159\n   Row 0: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 1: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 2: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 3: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 4: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   MSA loaded: 45 sequences, 159 positions\n   MSA tensor shape: torch.Size([45, 159, 6])\n   ✅ Using real MSA with 45 sequences\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with MSA depth: 45\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n3 74 8WFB_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFB_B.MSA.fasta\n   Target sequence length: 66\n   Row 0: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 1: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 2: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 3: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 4: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   MSA loaded: 20 sequences, 66 positions\n   MSA tensor shape: torch.Size([20, 66, 6])\n   ✅ Using real MSA with 20 sequences\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with MSA depth: 20\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 75 9L5R_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 76 9IS7_B 582 GGUAGGAAAUGACCCAGUGACCCUGACAGUUUGGGAAAGUCGGUGAAAGCCCGACCCUCGGGGCCUAGCGAAAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9IS7_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (582, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n3 77 9GC0_Q 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GC0_Q.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 78 8YIH_C 96 GUAGCAUGAUCAUCCGAAUCCUCUACAACGAUUUUUUCCCCAUUAUUGAAUAAUGGCAAAAAAUCCUUGUAGUGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIH_C.MSA.fasta\n   Target sequence length: 96\n   Row 0: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 1: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 2: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 3: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 4: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   MSA loaded: 100 sequences, 96 positions\n   MSA tensor shape: torch.Size([100, 96, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([96, 6])\nout: (96, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 79 9HNY_CA 620 UAUUUUUGAAAGAUUUUUUGUAUAAAAUUUUAGGAAUAGUUAAUAAUAAUUUAUAAUUUUGAUUAGAUUGUUUUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9HNY_CA.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (620, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 57 sec\ngpu_mem_used: 14 GB\n3 80 8VK7_B 118 GUUACGGCGGUCCAUAGCGGCAGGGAAACGCCCGGUCCCAUCCCGAACCCGGAAGCUAAGCCUGCCAGCGCCGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VK7_B.MSA.fasta\n   Target sequence length: 118\n   Row 0: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 1: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 2: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 3: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 4: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   MSA loaded: 100 sequences, 118 positions\n   MSA tensor shape: torch.Size([100, 118, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([118, 6])\nout: (118, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n3 81 8WF8_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF8_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 82 8QHU_S4 76 GCGCGGAUAGCUCAGUCGGUAGAGCAGGGGAUUGAAAAUCCCCGUGUCCUUGGUUCGAUUCCGAGUCCGCGCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_S4.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 83 8ZAU_A 69 GCGGGCUGACCGACCCCCCGAGUUCGCUUGGGGACAACUAGACAUACAGUAUGAAAAUGCUGAGCCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZAU_A.MSA.fasta, using single sequence\n   aa_type shape: (69, 6)\n   MSA tensor shape: torch.Size([2, 69, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([69, 6])\nout: (69, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 84 9AR7_B 120 GUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR7_B.MSA.fasta, using single sequence\n   aa_type shape: (120, 6)\n   MSA tensor shape: torch.Size([2, 120, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 85 9AR4_B 149 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAUA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR4_B.MSA.fasta, using single sequence\n   aa_type shape: (149, 6)\n   MSA tensor shape: torch.Size([2, 149, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([149, 6])\nout: (149, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n3 86 8Y9N_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9N_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 2 sequences, 62 positions\n   MSA tensor shape: torch.Size([2, 62, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 87 8QHU_4 184 GUGAGAUUGUGAAGGGAUCUCGCAGGUAUCGUGAGGGAAGUAUGGGGUAGUACGAGAGGAACUCCCAUGCCGUGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_4.MSA.fasta\n   Target sequence length: 184\n   Row 0: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 1: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 2: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 3: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 4: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   MSA loaded: 100 sequences, 184 positions\n   MSA tensor shape: torch.Size([100, 184, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([184, 6])\nout: (184, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 10 sec\ngpu_mem_used: 4 GB\n3 88 8RWG_C 121 UGCUUGACGAUCAUAGAGCGUUGGAACCACCUGAUCCCUUCCCGAACUCAGAAGUGAAACGACGCAUCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RWG_C.MSA.fasta\n   Target sequence length: 121\n   Row 0: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 1: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 2: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 3: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 4: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   MSA loaded: 100 sequences, 121 positions\n   MSA tensor shape: torch.Size([100, 121, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([121, 6])\nout: (121, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n3 89 8YII_C 104 GGUUCGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YII_C.MSA.fasta, using single sequence\n   aa_type shape: (104, 6)\n   MSA tensor shape: torch.Size([2, 104, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 90 9DCF_C 90 GGUAAAACAGCCUGUGGGUUGAUCCCACCCACAGGGCCCAUUGGGCGCUAGCACUCUGGUAUCACGGUACCUUUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DCF_C.MSA.fasta\n   Target sequence length: 90\n   Row 0: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 1: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 2: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 3: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 4: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   MSA loaded: 100 sequences, 90 positions\n   MSA tensor shape: torch.Size([100, 90, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([90, 6])\nout: (90, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 91 9L5T_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5T_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n3 92 8WF9_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF9_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n3 93 9GBZ_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBZ_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n/kaggle/working/drfold/model_hub/cfg_97/model_9\n<All keys matched successfully>\n4 0 9L5R_2 193 AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUUUUCAGUUUAAUCUCUGAAACUGCUCUACG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_2.MSA.fasta\n   Target sequence length: 193\n   Row 0: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 1: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 2: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 3: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 4: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   MSA loaded: 100 sequences, 193 positions\n   MSA tensor shape: torch.Size([100, 193, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([193, 6])\nout: (193, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 11 sec\ngpu_mem_used: 4 GB\n4 1 9GFT_AU 76 GGGGCUAUAGCUCAGCUGGGAGAGCGCUUGCAUGGCAUGCAAGAGGUCAGCGGUUCGAUCCCGCUUAGCUCCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_AU.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 2 9L0R_K 700 GGCUUAGCACAGAUAAUCCAUAGCGAUAUGGGAAAGCUUUUGUAGGUGUAUCAACAAGAGCGCCAGUGAUGGUCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L0R_K.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 3 9GFT_A3 77 GGCUACGUAGCUCAGUUGGUUAGAGCACAUCACUCAUAAUGAUGGGGUCACAGGUUCGAAUCCCGUCGUAGCCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_A3.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 4 9B2K_B 70 AAACAGCAUAGCAAGUUAAAAUAAGGCUAGUCCGUUAUCAACUUGAAAAAGUGGCACCGAGUCGGUGCUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B2K_B.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 5 9B0S_Et 75 GCCCGGAUAGCUCAGCUGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0S_Et.MSA.fasta\n   Target sequence length: 75\n   Row 0: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 1: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 2: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 3: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   Row 4: seq length 75, parse_seq_onehot shape (75, 6), expected L=75\n   MSA loaded: 100 sequences, 75 positions\n   MSA tensor shape: torch.Size([100, 75, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([75, 6])\nout: (75, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 6 9J3T_B 580 UAAAAUGAAUUGGGAUACUGAUUGAUUUUAGUGGUGGAUUUUACAGCAAUGUAAAAAGGACUAAUAGUAAAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3T_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 1: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 2: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 3: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   Row 4: seq length 580, parse_seq_onehot shape (580, 6), expected L=580\n   MSA loaded: 100 sequences, 580 positions\n⚠️ MSA length 580 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 7 9LCR_B 578 UUCUAGUCAGGUAAGUAUAUCUUGAAGGCGGGGCUAAAAAUCCGCUAAAGGGCACAUCGAUGAAGCUUCUGGUGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LCR_B.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 1: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 2: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 3: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   Row 4: seq length 578, parse_seq_onehot shape (578, 6), expected L=578\n   MSA loaded: 100 sequences, 578 positions\n⚠️ MSA length 578 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (578, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 59 sec\ngpu_mem_used: 14 GB\n4 8 8KEB_A 72 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KEB_A.MSA.fasta\n   Target sequence length: 72\n   Row 0: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 1: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 2: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 3: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   Row 4: seq length 72, parse_seq_onehot shape (72, 6), expected L=72\n   MSA loaded: 100 sequences, 72 positions\n   MSA tensor shape: torch.Size([100, 72, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([72, 6])\nout: (72, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 9 9L5S_5 116 UUGGAGUAGGCCAGCUCAGACCGAACUCAUUUCCUGCCUUUUACCGGAUGUGACCGUGAGUUGGCCUGAAAUACU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_5.MSA.fasta\n   Target sequence length: 116\n   Row 0: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 1: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 2: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 3: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   Row 4: seq length 116, parse_seq_onehot shape (116, 6), expected L=116\n   MSA loaded: 100 sequences, 116 positions\n   MSA tensor shape: torch.Size([100, 116, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([116, 6])\nout: (116, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n4 10 8VXZ_C 36 GCGUACGAAGGAGAGGAGAGGAAGAGGAGAGUACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VXZ_C.MSA.fasta\n   Target sequence length: 36\n   Row 0: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 1: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 2: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 3: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   Row 4: seq length 36, parse_seq_onehot shape (36, 6), expected L=36\n   MSA loaded: 100 sequences, 36 positions\n   MSA tensor shape: torch.Size([100, 36, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 11 9J6Y_E 550 ACAAGCCGCCUAGCCAUACGUCUCUUAAUAACUACUAUGACGAAAUAUACGGAUACGUUUAUUUUUUCUAAUUUC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J6Y_E.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 1: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 2: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 3: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   Row 4: seq length 550, parse_seq_onehot shape (550, 6), expected L=550\n   MSA loaded: 63 sequences, 550 positions\n⚠️ MSA length 550 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (550, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 12 8QHU_5 135 UUACGUCCCUCUCCAAACGAGAGAACAUGCAUGGGCUGGCAUGAGCGGCAUGCUUCACUUCGGUGGGGCUCGAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_5.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 64 sequences, 135 positions\n   MSA tensor shape: torch.Size([64, 135, 6])\n   ✅ Using real MSA with 64 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 64\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n4 13 9GHF_Z 77 CGCGGGGUGGAGCAGCCUGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GHF_Z.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 14 9KPO_B 255 AAAAAACUUACUAUUAUAUUUGUAACAAAUUUUAUACAUAAGAUAAAUUCGUAUGUAUAGCCGUUCUGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9KPO_B.MSA.fasta\n   Target sequence length: 255\n   Row 0: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 1: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 2: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 3: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   Row 4: seq length 255, parse_seq_onehot shape (255, 6), expected L=255\n   MSA loaded: 94 sequences, 255 positions\n   MSA tensor shape: torch.Size([94, 255, 6])\n   ✅ Using real MSA with 94 sequences\n   Seq features shape: torch.Size([255, 6])\nout: (255, 3, 3)\n   📊 Processed with MSA depth: 94\ntime_taken:  0 min 17 sec\ngpu_mem_used: 6 GB\n4 15 9N2B_5 120 UGCCUGGCGGCAGUAGCGCGGUGGUCCCACCUGACCCCAUGCCGAACUCAGAAGUGAAACGCCGUAGCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2B_5.MSA.fasta\n   Target sequence length: 120\n   Row 0: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 1: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 2: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 3: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   Row 4: seq length 120, parse_seq_onehot shape (120, 6), expected L=120\n   MSA loaded: 100 sequences, 120 positions\n   MSA tensor shape: torch.Size([100, 120, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n4 16 9N2C_Pt 77 CGCGGGGUGGAGCAGCCCGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9N2C_Pt.MSA.fasta\n   Target sequence length: 77\n   Row 0: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 1: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 2: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 3: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   Row 4: seq length 77, parse_seq_onehot shape (77, 6), expected L=77\n   MSA loaded: 100 sequences, 77 positions\n   MSA tensor shape: torch.Size([100, 77, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 17 9B1Y_4 81 ACGGUUUGUGUAGGAUAGGUGGGAGACUGUGAAGCUCACACGCCAGUGUGGGUGGAGUCGUUGUUGAAAUACCAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B1Y_4.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 18 9G06_a 73 GCGGGGUGGAGCAGGGUAGCUCGUCGGGCUCAUAACCCGAAGGUCGUCGGUUCAAAUCCGGCCCCCGCAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9G06_a.MSA.fasta\n   Target sequence length: 73\n   Row 0: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 1: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 2: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 3: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   Row 4: seq length 73, parse_seq_onehot shape (73, 6), expected L=73\n   MSA loaded: 100 sequences, 73 positions\n   MSA tensor shape: torch.Size([100, 73, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([73, 6])\nout: (73, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 19 9DE8_A 57 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE8_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 20 9B83_C 39 GGGAGCCCCCCXGCUUCACUGCAUGGAAGCUAAAGGGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B83_C.MSA.fasta\n   Target sequence length: 39\n   Row 0: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 1: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 2: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 3: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   Row 4: seq length 39, parse_seq_onehot shape (39, 6), expected L=39\n   MSA loaded: 23 sequences, 39 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (39, 6)\n   MSA tensor shape: torch.Size([2, 39, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([39, 6])\nout: (39, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 21 8ZMH_A 169 CGUGGUUGACACGCAGACCUCUUACAAGAGUGUCUAGGUGCCUUUGAGAGUUACUCUUUGCUCUCUUCGGAAGAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZMH_A.MSA.fasta, using single sequence\n   aa_type shape: (169, 6)\n   MSA tensor shape: torch.Size([2, 169, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([169, 6])\nout: (169, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n4 22 9E2Y_F 35 AUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Y_F.MSA.fasta, using single sequence\n   aa_type shape: (35, 6)\n   MSA tensor shape: torch.Size([2, 35, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 23 9DE7_A 58 GGUCUCUCUGGUUAAGCCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE7_A.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 100 sequences, 58 positions\n   MSA tensor shape: torch.Size([100, 58, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 24 8Y9L_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9L_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 25 9FIB_Y 15 AAUGUUUGAAAAAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FIB_Y.MSA.fasta\n   Target sequence length: 15\n   Row 0: seq length 15, parse_seq_onehot shape (15, 6), expected L=15\n   MSA loaded: 1 sequences, 15 positions\n   MSA tensor shape: torch.Size([1, 15, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([15, 6])\nout: (15, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 26 9J3R_B 580 AAGAGCAUUUGAACAAUGUGCUAGGGUAGUAUGGGAUAAGUCGAUAACUAAAAUGAAUUGGGAUACUGAUUGAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9J3R_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 27 9DPB_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPB_C.MSA.fasta, using single sequence\n   aa_type shape: (76, 6)\n   MSA tensor shape: torch.Size([2, 76, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 28 8XTP_A 133 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_A.MSA.fasta, using single sequence\n   aa_type shape: (133, 6)\n   MSA tensor shape: torch.Size([2, 133, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([133, 6])\nout: (133, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 29 8ZTV_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTV_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 30 8Y9M_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9M_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 2: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 3: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 4 sequences, 62 positions\n   MSA tensor shape: torch.Size([4, 62, 6])\n   ✅ Using real MSA with 4 sequences\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with MSA depth: 4\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 31 8ZQ9_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZQ9_A.MSA.fasta, using single sequence\n   aa_type shape: (159, 6)\n   MSA tensor shape: torch.Size([2, 159, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n4 32 8XTP_B 595 AGCGCAACACCUGCCGCACAGGAUGGCUUCUGAGGUACCGGUGACGGUACAGAACGCGGAGGGGAAACCUGGAAG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTP_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (595, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 33 9B89_C 35 GGGCUUUCGUUUUCCUAUAUAGGAAAAUGAACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B89_C.MSA.fasta\n   Target sequence length: 35\n   Row 0: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 1: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 2: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 3: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   Row 4: seq length 35, parse_seq_onehot shape (35, 6), expected L=35\n   MSA loaded: 100 sequences, 35 positions\n   MSA tensor shape: torch.Size([100, 35, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([35, 6])\nout: (35, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 34 8SYK_C 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_C.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n4 35 9FN3_B 58 GGAGUCAUGGCUCAGGGCUGUUCGCAGCCGCUGCAGUCAGUCGAAAGACUGXGACUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FN3_B.MSA.fasta\n   Target sequence length: 58\n   Row 0: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 1: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 2: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 3: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   Row 4: seq length 58, parse_seq_onehot shape (58, 6), expected L=58\n   MSA loaded: 34 sequences, 58 positions\n⚠️ Target sequence mismatch with MSA, using single sequence\n   aa_type shape: (58, 6)\n   MSA tensor shape: torch.Size([2, 58, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([58, 6])\nout: (58, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 36 8QHU_3 216 UAUUAGUGGUAAUGCGAAACACUUGCCAGGUAACAAAUCAAUCCUCCCACGGUGAGCUUUCUUUUCACCAUAAUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_3.MSA.fasta, using single sequence\n   aa_type shape: (216, 6)\n   MSA tensor shape: torch.Size([2, 216, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([216, 6])\nout: (216, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 07 sec\ngpu_mem_used: 3 GB\n4 37 9DRS_C 77 GGCCAGGUAGCUCAGUCGGUAUGAGCGUCCGCCUGAAAAGCGGAAGGUCGGCGGUUCGAUCCCGCCCCUGGCCAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DRS_C.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 38 8XTR_A 145 CCGGGGCGCCACCCCGGAAGUGAUGCGAGUCGCCAACUCGCAUCACAAGCAAACGCUGUAGCCGCGUGCCUCUAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8XTR_A.MSA.fasta\n   Target sequence length: 145\n   Row 0: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 1: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 2: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 3: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   Row 4: seq length 145, parse_seq_onehot shape (145, 6), expected L=145\n   MSA loaded: 100 sequences, 145 positions\n   MSA tensor shape: torch.Size([100, 145, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([145, 6])\nout: (145, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n4 39 9LMF_F 700 UUUAUGUAAUCAGUAAUGGUUACAGCACACGAAGUCAAAGCGCAAUACCAGAGCCAAUCGGUGAGGUGCUGUGUC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9LMF_F.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (700, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 40 9DE6_B 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_B.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 41 8SYK_B 107 CAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAGGCUCGAAA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8SYK_B.MSA.fasta\n   Target sequence length: 107\n   Row 0: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 1: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 2: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 3: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   Row 4: seq length 107, parse_seq_onehot shape (107, 6), expected L=107\n   MSA loaded: 100 sequences, 107 positions\n   MSA tensor shape: torch.Size([100, 107, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([107, 6])\nout: (107, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n4 42 9DE6_A 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE6_A.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 43 8R7N_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8R7N_A.MSA.fasta\n   Target sequence length: 135\n   Row 0: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 1: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 2: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 3: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   Row 4: seq length 135, parse_seq_onehot shape (135, 6), expected L=135\n   MSA loaded: 100 sequences, 135 positions\n   MSA tensor shape: torch.Size([100, 135, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n4 44 8K85_A 56 GGAGACGGUCGGGUCCAGUCGCAACGAUGUUGGCUGUUGAGUAGUGUGUGGGCUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8K85_A.MSA.fasta, using single sequence\n   aa_type shape: (56, 6)\n   MSA tensor shape: torch.Size([2, 56, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([56, 6])\nout: (56, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 45 9FCV_B 81 CGUCGCCGUCCAGCUCGACCAGGAUGGGAAGUUGCAUCUGCCUUCUUUUUGAAAGGUAAAAACAACAUCGUCCAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9FCV_B.MSA.fasta\n   Target sequence length: 81\n   Row 0: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 1: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 2: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 3: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   Row 4: seq length 81, parse_seq_onehot shape (81, 6), expected L=81\n   MSA loaded: 100 sequences, 81 positions\n   MSA tensor shape: torch.Size([100, 81, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([81, 6])\nout: (81, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 46 9DPA_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPA_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 47 9DE5_C 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_C.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 48 8VZ6_S 50 ACUAGAACCCGCCAAGCCUCUCAACGAUGCUCAAAUGUGCGGGUCGUUUU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VZ6_S.MSA.fasta\n   Target sequence length: 50\n   Row 0: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 1: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 2: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 3: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   Row 4: seq length 50, parse_seq_onehot shape (50, 6), expected L=50\n   MSA loaded: 100 sequences, 50 positions\n   MSA tensor shape: torch.Size([100, 50, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([50, 6])\nout: (50, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 49 8YIG_C 104 GGUUAGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIG_C.MSA.fasta\n   Target sequence length: 104\n   Row 0: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 1: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 2: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 3: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   Row 4: seq length 104, parse_seq_onehot shape (104, 6), expected L=104\n   MSA loaded: 100 sequences, 104 positions\n   MSA tensor shape: torch.Size([100, 104, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 03 sec\ngpu_mem_used: 2 GB\n4 50 9B84_F 66 GGGCUXUUCGUUUUCCUAUUGAGCAUAGCCGCUUCUUCGGCUAUGCUCAAUAGGAAAACGAACAGU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B84_F.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 51 9C8K_2 27 GCCCUGUGGGACCCAGGCCUGGGGGGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9C8K_2.MSA.fasta\n   Target sequence length: 27\n   Row 0: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 1: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 2: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 3: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   Row 4: seq length 27, parse_seq_onehot shape (27, 6), expected L=27\n   MSA loaded: 29 sequences, 27 positions\n   MSA tensor shape: torch.Size([29, 27, 6])\n   ✅ Using real MSA with 29 sequences\n   Seq features shape: torch.Size([27, 6])\nout: (27, 3, 3)\n   📊 Processed with MSA depth: 29\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 52 9B0Q_AP 71 GUCUCCGUAGUGUAGGGUAUCACGUUCGCCUAACACGCGAAAGGUCCUCGGUUCGAAACCGGGCGGAAACA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9B0Q_AP.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 53 9E2Z_F 40 GUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAUA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2Z_F.MSA.fasta\n   Target sequence length: 40\n   Row 0: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 1: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 2: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 3: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   Row 4: seq length 40, parse_seq_onehot shape (40, 6), expected L=40\n   MSA loaded: 100 sequences, 40 positions\n   MSA tensor shape: torch.Size([100, 40, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([40, 6])\nout: (40, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 54 8Z8Q_B 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAAACCACAAAUUUCUUACUGCGAAUUC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8Q_B.MSA.fasta, using single sequence\n   aa_type shape: (71, 6)\n   MSA tensor shape: torch.Size([2, 71, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 55 9E2W_F 48 UCGUGCUGAGUGAUAUCUGCUUUGGGUGGGUGGGUGGGUUGAGGCAAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9E2W_F.MSA.fasta, using single sequence\n   aa_type shape: (48, 6)\n   MSA tensor shape: torch.Size([2, 48, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([48, 6])\nout: (48, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 56 8KHH_A 71 GGAUUCGUAUAUCCUUAAUGAUAUGGUUUAAGGGCAAUACAUAGAGACCACAAAUUUCUUACUGCGAAUUC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8KHH_A.MSA.fasta\n   Target sequence length: 71\n   Row 0: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 1: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 2: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 3: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   Row 4: seq length 71, parse_seq_onehot shape (71, 6), expected L=71\n   MSA loaded: 100 sequences, 71 positions\n   MSA tensor shape: torch.Size([100, 71, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([71, 6])\nout: (71, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 57 8Z8U_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z8U_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 58 8ZTU_Y 77 GGAGCGGUAGUUCAGUCGGUUAGAAUACCUGCCUGUCACGCAGGGGGUCGCGGGUUCGAGUCCCGUCCGUUCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZTU_Y.MSA.fasta, using single sequence\n   aa_type shape: (77, 6)\n   MSA tensor shape: torch.Size([2, 77, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([77, 6])\nout: (77, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 59 9GCL_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGGGCU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCL_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 60 8RRI_Ax 70 UAGGGAAUAGUUUAAAAAACAUCUGACUCACAUUCAGAAGAUGGAGGUUCAAUUCCUCCUUCCCUAACCA...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RRI_Ax.MSA.fasta\n   Target sequence length: 70\n   Row 0: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 1: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 2: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 3: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   Row 4: seq length 70, parse_seq_onehot shape (70, 6), expected L=70\n   MSA loaded: 100 sequences, 70 positions\n   MSA tensor shape: torch.Size([100, 70, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([70, 6])\nout: (70, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 61 9L5S_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5S_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 62 9GCM_A 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GCM_A.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 63 8Z9K_B 41 GGGCAGAGCCCAACACAGCGAAAGCUGUGGCUAGACUGCCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Z9K_B.MSA.fasta, using single sequence\n   aa_type shape: (41, 6)\n   MSA tensor shape: torch.Size([2, 41, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([41, 6])\nout: (41, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 64 9MTY_C 36 AGCCAUGCAAGCCCUGAAACCCAUUGCCUUAGUGCG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9MTY_C.MSA.fasta, using single sequence\n   aa_type shape: (36, 6)\n   MSA tensor shape: torch.Size([2, 36, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([36, 6])\nout: (36, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 65 8QHU_7 171 AACGUGUCGCGAUGGAUGACUUGGCUUCCUAUUUCGUUGAAGAACGCAGUAAAGUGCGAUAAGUGGUAUCAAUUG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_7.MSA.fasta, using single sequence\n   aa_type shape: (171, 6)\n   MSA tensor shape: torch.Size([2, 171, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([171, 6])\nout: (171, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n4 66 9GBW_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBW_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 67 8T5O_A 124 GUGAGGUGCAGGUACAUCCAGCUGAUGAGUCCCAAAUAGGACAAAAAGGGAGAGGUGAAGAAUACGACCACCUAG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8T5O_A.MSA.fasta\n   Target sequence length: 124\n   Row 0: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 1: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 2: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 3: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   Row 4: seq length 124, parse_seq_onehot shape (124, 6), expected L=124\n   MSA loaded: 100 sequences, 124 positions\n   MSA tensor shape: torch.Size([100, 124, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([124, 6])\nout: (124, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n4 68 9DPL_C 76 GCCCGGAUAGCUCAGUCGGUAGAGCAUCAGACUUUUAAUCUGAGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DPL_C.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 69 8WFA_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFA_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 70 9ISV_A 580 AAUGAAUUGGGAUACUGAUUGAUUUUAGUGGUGGAUUUUACAGCAAUGUAAAAAGGACUAAUAGUAAAAGCUAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9ISV_A.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (580, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 71 9AR6_B 147 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR6_B.MSA.fasta\n   Target sequence length: 147\n   Row 0: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 1: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 2: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 3: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   Row 4: seq length 147, parse_seq_onehot shape (147, 6), expected L=147\n   MSA loaded: 50 sequences, 147 positions\n   MSA tensor shape: torch.Size([50, 147, 6])\n   ✅ Using real MSA with 50 sequences\n   Seq features shape: torch.Size([147, 6])\nout: (147, 3, 3)\n   📊 Processed with MSA depth: 50\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n4 72 9DE5_D 57 GGUCUCUCUGGUUAGACCAGAUCUGAGCCGAAAAGCUCUCUGGCUAACUAGGGAACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DE5_D.MSA.fasta\n   Target sequence length: 57\n   Row 0: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 1: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 2: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 3: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   Row 4: seq length 57, parse_seq_onehot shape (57, 6), expected L=57\n   MSA loaded: 100 sequences, 57 positions\n   MSA tensor shape: torch.Size([100, 57, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([57, 6])\nout: (57, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 73 8ZDR_A 159 GGUUCGAAAUUAGGUGCGCUUCGCGUUACAGUUAAGGCUCUGAAAAGAGCCUUAAUUGUAAAACGCCUAUACAGU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZDR_A.MSA.fasta\n   Target sequence length: 159\n   Row 0: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 1: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 2: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 3: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   Row 4: seq length 159, parse_seq_onehot shape (159, 6), expected L=159\n   MSA loaded: 45 sequences, 159 positions\n   MSA tensor shape: torch.Size([45, 159, 6])\n   ✅ Using real MSA with 45 sequences\n   Seq features shape: torch.Size([159, 6])\nout: (159, 3, 3)\n   📊 Processed with MSA depth: 45\ntime_taken:  0 min 05 sec\ngpu_mem_used: 3 GB\n4 74 8WFB_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WFB_B.MSA.fasta\n   Target sequence length: 66\n   Row 0: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 1: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 2: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 3: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   Row 4: seq length 66, parse_seq_onehot shape (66, 6), expected L=66\n   MSA loaded: 20 sequences, 66 positions\n   MSA tensor shape: torch.Size([20, 66, 6])\n   ✅ Using real MSA with 20 sequences\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with MSA depth: 20\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 75 9L5R_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 76 9IS7_B 582 ACAGUUUGGGAAAGUCGGUGAAAGCCCGACCCUCGGGGCCUAGCGAAAGUGGGCGACGAGUGCAACACCUGGAGG...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9IS7_B.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (582, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 77 9GC0_Q 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GC0_Q.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 78 8YIH_C 96 GUAGCAUGAUCAUCCGAAUCCUCUACAACGAUUUUUUCCCCAUUAUUGAAUAAUGGCAAAAAAUCCUUGUAGUGG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YIH_C.MSA.fasta\n   Target sequence length: 96\n   Row 0: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 1: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 2: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 3: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   Row 4: seq length 96, parse_seq_onehot shape (96, 6), expected L=96\n   MSA loaded: 100 sequences, 96 positions\n   MSA tensor shape: torch.Size([100, 96, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([96, 6])\nout: (96, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 79 9HNY_CA 620 AAGGUUUAUUUUUGAAAGAUUUUUUGUAUAAAAUUUUAGGAAUAGUUAAUAAUAAUUUAUAAUUUUGAUUAGAUU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9HNY_CA.MSA.fasta, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\nout: (620, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 58 sec\ngpu_mem_used: 14 GB\n4 80 8VK7_B 118 GUUACGGCGGUCCAUAGCGGCAGGGAAACGCCCGGUCCCAUCCCGAACCCGGAAGCUAAGCCUGCCAGCGCCGAU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8VK7_B.MSA.fasta\n   Target sequence length: 118\n   Row 0: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 1: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 2: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 3: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   Row 4: seq length 118, parse_seq_onehot shape (118, 6), expected L=118\n   MSA loaded: 100 sequences, 118 positions\n   MSA tensor shape: torch.Size([100, 118, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([118, 6])\nout: (118, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n4 81 8WF8_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF8_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 82 8QHU_S4 76 GCGCGGAUAGCUCAGUCGGUAGAGCAGGGGAUUGAAAAUCCCCGUGUCCUUGGUUCGAUUCCGAGUCCGCGCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_S4.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 83 8ZAU_A 69 GCGGGCUGACCGACCCCCCGAGUUCGCUUGGGGACAACUAGACAUACAGUAUGAAAAUGCUGAGCCCGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8ZAU_A.MSA.fasta, using single sequence\n   aa_type shape: (69, 6)\n   MSA tensor shape: torch.Size([2, 69, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([69, 6])\nout: (69, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 84 9AR7_B 120 GUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAGGC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR7_B.MSA.fasta, using single sequence\n   aa_type shape: (120, 6)\n   MSA tensor shape: torch.Size([2, 120, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([120, 6])\nout: (120, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 85 9AR4_B 149 GGUAGGAUGGCAAGAUCCUGGUAGUCAUAGUUCCCCUGGAAACAGGGUUACUAUGAUAAGGGCUUUCUGCCUAUA...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9AR4_B.MSA.fasta, using single sequence\n   aa_type shape: (149, 6)\n   MSA tensor shape: torch.Size([2, 149, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([149, 6])\nout: (149, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 03 sec\ngpu_mem_used: 3 GB\n4 86 8Y9N_B 62 GUGCUGGCCGCUCUCGCUAGAGGGAGGUCAGAGCACAUAAUAUCAAUGGAAUAUAGCAAGCU...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8Y9N_B.MSA.fasta\n   Target sequence length: 62\n   Row 0: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   Row 1: seq length 62, parse_seq_onehot shape (62, 6), expected L=62\n   MSA loaded: 2 sequences, 62 positions\n   MSA tensor shape: torch.Size([2, 62, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([62, 6])\nout: (62, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 87 8QHU_4 184 GUGAGAUUGUGAAGGGAUCUCGCAGGUAUCGUGAGGGAAGUAUGGGGUAGUACGAGAGGAACUCCCAUGCCGUGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8QHU_4.MSA.fasta\n   Target sequence length: 184\n   Row 0: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 1: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 2: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 3: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   Row 4: seq length 184, parse_seq_onehot shape (184, 6), expected L=184\n   MSA loaded: 100 sequences, 184 positions\n   MSA tensor shape: torch.Size([100, 184, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([184, 6])\nout: (184, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 10 sec\ngpu_mem_used: 4 GB\n4 88 8RWG_C 121 UGCUUGACGAUCAUAGAGCGUUGGAACCACCUGAUCCCUUCCCGAACUCAGAAGUGAAACGACGCAUCGCCGAUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/8RWG_C.MSA.fasta\n   Target sequence length: 121\n   Row 0: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 1: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 2: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 3: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   Row 4: seq length 121, parse_seq_onehot shape (121, 6), expected L=121\n   MSA loaded: 100 sequences, 121 positions\n   MSA tensor shape: torch.Size([100, 121, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([121, 6])\nout: (121, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 04 sec\ngpu_mem_used: 3 GB\n4 89 8YII_C 104 GGUUCGCUCCCGGCGCUUCACAGGCGCUGGAAAAUCUUAACCGCCGGAAGUCACUUCCGCUGGCUUUGAUUUUCC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8YII_C.MSA.fasta, using single sequence\n   aa_type shape: (104, 6)\n   MSA tensor shape: torch.Size([2, 104, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([104, 6])\nout: (104, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 90 9DCF_C 90 GGUAAAACAGCCUGUGGGUUGAUCCCACCCACAGGGCCCAUUGGGCGCUAGCACUCUGGUAUCACGGUACCUUUG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9DCF_C.MSA.fasta\n   Target sequence length: 90\n   Row 0: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 1: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 2: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 3: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   Row 4: seq length 90, parse_seq_onehot shape (90, 6), expected L=90\n   MSA loaded: 100 sequences, 90 positions\n   MSA tensor shape: torch.Size([100, 90, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([90, 6])\nout: (90, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 91 9L5T_6 101 GCCCUUCGGGGCAUUUGGUCAAUUUGAAACGAUACAGAGAAGAUUAGCAUGGCCCCUGCACUAAGGAUGACACGC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5T_6.MSA.fasta\n   Target sequence length: 101\n   Row 0: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 1: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 2: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 3: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   Row 4: seq length 101, parse_seq_onehot shape (101, 6), expected L=101\n   MSA loaded: 100 sequences, 101 positions\n   MSA tensor shape: torch.Size([100, 101, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([101, 6])\nout: (101, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n4 92 8WF9_B 66 UCUAAACCAUCCUGCGGCCUCUACUCUGCAGUUGUGGAAGGUCCAGUUUUGAGGGGCUAUUACAAC...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/8WF9_B.MSA.fasta, using single sequence\n   aa_type shape: (66, 6)\n   MSA tensor shape: torch.Size([2, 66, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([66, 6])\nout: (66, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 01 sec\ngpu_mem_used: 2 GB\n4 93 9GBZ_R 135 AAAAAGGGCUUCUGUCGUGAGUGGCACACGUAGGGCAACUCGAUUGCUCUGCGUGCGGAAUCGACAUCAAGAGAU...\n⚠️ MSA file not found: /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GBZ_R.MSA.fasta, using single sequence\n   aa_type shape: (135, 6)\n   MSA tensor shape: torch.Size([2, 135, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([135, 6])\nout: (135, 3, 3)\n   📊 Processed with single sequence (no MSA)\ntime_taken:  0 min 02 sec\ngpu_mem_used: 2 GB\n\nMAX_LENGTH 480\n### total_time_taken:  1 hr 08 min\n### max_gpu_mem_used: 14 GB\n### MSA usage: 60/94 targets had MSA files\n### MSA coverage: 63.8%\n\n             ID resname  resid        x_1        y_1        z_1        x_2        y_2        z_2        x_3        y_3        z_3        x_4        y_4        z_4        x_5         y_5        z_5\n0      9L5R_2_1       A      1 -69.214279 -23.408482  31.439661 -13.780300  -7.097895  -9.979194 -30.261040   2.294019 -40.388870  79.142715 -87.077515 -44.171051  77.809448    3.145823  34.940613\n1      9L5R_2_2       G      2 -65.585381 -25.671036  35.633755 -13.545410  -4.784512  -5.981824 -33.850349   6.436975 -42.450409  75.716629 -87.523689 -49.060989  75.739563    7.939509  31.964142\n2      9L5R_2_3       C      3 -61.313110 -29.077950  37.234219 -11.774554  -3.442421  -1.477194 -36.386971  11.372717 -41.859169  70.188538 -88.164291 -50.321690  75.970680   11.619222  27.445574\n3      9L5R_2_4       U      4 -57.189007 -32.372082  36.091148  -7.296203  -3.508822   1.792214 -37.077675  16.385622 -39.328022  64.415413 -88.527573 -48.774822  78.323128   13.316706  22.476101\n4      9L5R_2_5       C      5 -52.757298 -33.901562  32.628284  -2.283791  -3.139763   3.124781 -35.030422  21.375221 -36.610363  59.920078 -87.058334 -45.480045  80.913078   12.366913  17.383877\n..          ...     ...    ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...         ...        ...\n130  9GBZ_R_131       C    131 -18.522394  25.867212  -6.842880  42.600761 -19.077759  15.154884   2.419822 -30.289505 -13.893044 -10.751239  37.355293 -16.380182 -29.875610   86.917503   2.248111\n131  9GBZ_R_132       C    132 -20.611263  23.413794  -1.742350  46.263031 -17.171043  11.249230   0.053686 -33.696754 -18.352144 -11.379569  31.801193 -17.117085 -27.352821   91.578461   3.362567\n132  9GBZ_R_133       U    133 -24.089878  23.753033   2.568858  48.287197 -12.959512   8.232779  -4.023068 -37.580467 -19.437082 -11.193551  27.400553 -20.701859 -25.070272   95.929733   2.104393\n133  9GBZ_R_134       U    134 -27.389006  25.128757   5.099888  48.890209  -7.730174   6.991531  -8.455057 -40.096401 -18.690519  -9.948381  25.204584 -24.447260 -23.272701   98.832146  -0.875431\n134  9GBZ_R_135       U    135 -31.069698  27.254736   5.486392  48.300087  -2.200924   8.824411 -12.980618 -40.293476 -16.565689  -7.435666  23.540432 -28.057138 -22.969107  101.185226  -6.082818\n\n[13782 rows x 18 columns]\nSUBMIT OK!!!!!!\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import shutil\nimport os\n\n# Copy USalign to working directory and make it executable\nshutil.copy2(\"/kaggle/input/usalign/USalign\", \"/kaggle/working/USalign\")\nos.chmod(\"/kaggle/working/USalign\", 0o755)\n\nprint(\"USalign copied to /kaggle/working/ and made executable\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T22:50:50.239898Z","iopub.execute_input":"2025-06-22T22:50:50.240167Z","iopub.status.idle":"2025-06-22T22:50:50.248287Z","shell.execute_reply.started":"2025-06-22T22:50:50.240145Z","shell.execute_reply":"2025-06-22T22:50:50.247536Z"}},"outputs":[{"name":"stdout","text":"USalign copied to /kaggle/working/ and made executable\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# score ribonanza msa\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\n\n\ndef parse_tmscore_output(output):\n    tm_score_match = re.findall(r'TM-score=\\s+([\\d.]+)', output)[1]\n    return float(tm_score_match)\n\n\ndef write_target_line(\n        atom_name, atom_serial, residue_name, chain_id, residue_num,\n        x_coord, y_coord, z_coord, occupancy=1.0, b_factor=0.0, atom_type='P'\n) -> str:\n    return (\n        f'ATOM  {atom_serial:>5d}  {atom_name:<5s} {residue_name:<3s} '\n        f'{residue_num:>3d}    {x_coord:>8.3f}{y_coord:>8.3f}'\n        f'{z_coord:>8.3f}{occupancy:>6.2f}{b_factor:>6.2f}           {atom_type}\\n'\n    )\n\n\ndef write2pdb(df: pd.DataFrame, xyz_id: int, target_path: str) -> int:\n    resolved_cnt = 0\n    with open(target_path, 'w') as f:\n        for _, row in df.iterrows():\n            x = row[f'x_{xyz_id}'];\n            y = row[f'y_{xyz_id}'];\n            z = row[f'z_{xyz_id}']\n            if x > -1e17 and y > -1e17 and z > -1e17:\n                resolved_cnt += 1\n                f.write(write_target_line(\n                    atom_name=\"C1'\", atom_serial=int(row['resid']),\n                    residue_name=row['resname'], chain_id='0',\n                    residue_num=int(row['resid']),\n                    x_coord=x, y_coord=y, z_coord=z, atom_type='C'\n                ))\n    return resolved_cnt\n\n\ndef get_base_target_id(long_id):\n    return \"_\".join(str(long_id).split(\"_\")[:-1])\n\n\ndef score_and_report(solution: pd.DataFrame, submission: pd.DataFrame):\n    solution['target_id'] = solution['ID'].apply(get_base_target_id)\n    submission['target_id'] = submission['ID'].apply(get_base_target_id)\n\n    native_idxs = sorted(int(c.split('_')[1])\n                         for c in solution.columns if c.startswith('x_'))\n\n    usalign = \"/kaggle/working/USalign\"\n    per_target = {}\n\n    # Find common targets to iterate over\n    common_targets = sorted(list(set(solution['target_id'].unique()) & set(submission['target_id'].unique())))\n\n    print(f\"Scoring {len(common_targets)} common targets...\")\n\n    for tid in common_targets:\n        grp_nat = solution[solution['target_id'] == tid]\n        grp_pred = submission[submission['target_id'] == tid]\n        best_of_five = []\n\n        for pred_cnt in range(1, 6):\n            best_for_this_pred = 0.0\n            for nat_cnt in native_idxs:\n                n_nat = write2pdb(grp_nat, nat_cnt, 'native.pdb')\n                n_pred = write2pdb(grp_pred, pred_cnt, 'predicted.pdb')\n                if n_nat > 0 and n_pred > 0:\n                    out = os.popen(\n                        f'{usalign} predicted.pdb native.pdb -atom \" C1\\'\"'\n                    ).read()\n                    best_for_this_pred = max(best_for_this_pred, parse_tmscore_output(out))\n            best_of_five.append(best_for_this_pred)\n\n        per_target[tid] = best_of_five\n        print(f\"{tid}: TM-scores per model = {best_of_five}, \"\n              f\"best = {max(best_of_five):.4f}\")\n\n    # Calculate mean TM score\n    all_best_scores = [max(scores) for scores in per_target.values()]\n    mean_tm = np.mean(all_best_scores) if all_best_scores else 0.0\n\n    return per_target, mean_tm\n\nsolution = pd.read_csv(\"/kaggle/input/validation-labels-clean-csv/validation_labels_clean.csv\")\n\nper_target_scores, mean_tm = score_and_report(solution, submission)\nprint(f\"\\nMean TM-score: {mean_tm:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:18:05.664143Z","iopub.execute_input":"2025-06-22T23:18:05.664532Z","iopub.status.idle":"2025-06-22T23:18:57.181927Z","shell.execute_reply.started":"2025-06-22T23:18:05.664505Z","shell.execute_reply":"2025-06-22T23:18:57.181152Z"}},"outputs":[{"name":"stdout","text":"Scoring 94 common targets...\n8K85_A: TM-scores per model = [0.19024, 0.15696, 0.22208, 0.19176, 0.18334], best = 0.2221\n8KEB_A: TM-scores per model = [0.49828, 0.46364, 0.49765, 0.5103, 0.45835], best = 0.5103\n8KHH_A: TM-scores per model = [0.50146, 0.46329, 0.48286, 0.51902, 0.47486], best = 0.5190\n8QHU_3: TM-scores per model = [0.45829, 0.23139, 0.33023, 0.40393, 0.38157], best = 0.4583\n8QHU_4: TM-scores per model = [0.66726, 0.78863, 0.78007, 0.78966, 0.75343], best = 0.7897\n8QHU_5: TM-scores per model = [0.40837, 0.30469, 0.28117, 0.28498, 0.295], best = 0.4084\n8QHU_7: TM-scores per model = [0.73092, 0.75568, 0.74335, 0.75858, 0.76601], best = 0.7660\n8QHU_S4: TM-scores per model = [0.4687, 0.46158, 0.47405, 0.47401, 0.4653], best = 0.4741\n8R7N_A: TM-scores per model = [0.22897, 0.17878, 0.26483, 0.27512, 0.22709], best = 0.2751\n8RRI_Ax: TM-scores per model = [0.47363, 0.46722, 0.49106, 0.50096, 0.50634], best = 0.5063\n8RWG_C: TM-scores per model = [0.71766, 0.71795, 0.71001, 0.7231, 0.71825], best = 0.7231\n8SYK_B: TM-scores per model = [0.22888, 0.21121, 0.23884, 0.22108, 0.30121], best = 0.3012\n8SYK_C: TM-scores per model = [0.22761, 0.20519, 0.22988, 0.23222, 0.31036], best = 0.3104\n8T5O_A: TM-scores per model = [0.27042, 0.29186, 0.30144, 0.28842, 0.27337], best = 0.3014\n8VK7_B: TM-scores per model = [0.68582, 0.67931, 0.67173, 0.69742, 0.68591], best = 0.6974\n8VXZ_C: TM-scores per model = [0.16559, 0.16195, 0.17461, 0.18914, 0.17672], best = 0.1891\n8VZ6_S: TM-scores per model = [0.21875, 0.17941, 0.174, 0.16541, 0.19667], best = 0.2188\n8WF8_B: TM-scores per model = [0.18389, 0.139, 0.17456, 0.18745, 0.11757], best = 0.1875\n8WF9_B: TM-scores per model = [0.20166, 0.25552, 0.23397, 0.25925, 0.25133], best = 0.2592\n8WFA_B: TM-scores per model = [0.20381, 0.21717, 0.22836, 0.24485, 0.25744], best = 0.2574\n8WFB_B: TM-scores per model = [0.20727, 0.25203, 0.21714, 0.2673, 0.22276], best = 0.2673\n8XTP_A: TM-scores per model = [0.22266, 0.20053, 0.21106, 0.24776, 0.20939], best = 0.2478\n8XTP_B: TM-scores per model = [0.16436, 0.13248, 0.165, 0.17715, 0.16195], best = 0.1772\n8XTR_A: TM-scores per model = [0.21502, 0.18416, 0.19274, 0.24005, 0.24168], best = 0.2417\n8Y9L_B: TM-scores per model = [0.25145, 0.23879, 0.25274, 0.25706, 0.3111], best = 0.3111\n8Y9M_B: TM-scores per model = [0.2503, 0.24309, 0.2375, 0.25703, 0.28444], best = 0.2844\n8Y9N_B: TM-scores per model = [0.24319, 0.24302, 0.23559, 0.24629, 0.19382], best = 0.2463\n8YIG_C: TM-scores per model = [0.47288, 0.4277, 0.43329, 0.49031, 0.45369], best = 0.4903\n8YIH_C: TM-scores per model = [0.45794, 0.42494, 0.3996, 0.4519, 0.43806], best = 0.4579\n8YII_C: TM-scores per model = [0.50906, 0.45443, 0.46092, 0.48165, 0.50707], best = 0.5091\n8Z8Q_B: TM-scores per model = [0.45986, 0.41695, 0.48678, 0.48646, 0.46122], best = 0.4868\n8Z8U_B: TM-scores per model = [0.1652, 0.15776, 0.18384, 0.17032, 0.17942], best = 0.1838\n8Z9K_B: TM-scores per model = [0.17242, 0.17202, 0.17744, 0.17256, 0.18064], best = 0.1806\n8ZAU_A: TM-scores per model = [0.25065, 0.23319, 0.27233, 0.28485, 0.27841], best = 0.2848\n8ZDR_A: TM-scores per model = [0.2904, 0.31758, 0.3142, 0.31634, 0.32553], best = 0.3255\n8ZMH_A: TM-scores per model = [0.50882, 0.50981, 0.64256, 0.53538, 0.45179], best = 0.6426\n8ZQ9_A: TM-scores per model = [0.23634, 0.24036, 0.19781, 0.25091, 0.25183], best = 0.2518\n8ZTU_Y: TM-scores per model = [0.52264, 0.47699, 0.53363, 0.53432, 0.51775], best = 0.5343\n8ZTV_Y: TM-scores per model = [0.51626, 0.47167, 0.52831, 0.53013, 0.51127], best = 0.5301\n9AR4_B: TM-scores per model = [0.42306, 0.38933, 0.39923, 0.37828, 0.3913], best = 0.4231\n9AR6_B: TM-scores per model = [0.43509, 0.43129, 0.40246, 0.45741, 0.36755], best = 0.4574\n9AR7_B: TM-scores per model = [0.38896, 0.34307, 0.39395, 0.38504, 0.42906], best = 0.4291\n9B0Q_AP: TM-scores per model = [0.49548, 0.46795, 0.48828, 0.50659, 0.46992], best = 0.5066\n9B0S_Et: TM-scores per model = [0.4829, 0.46596, 0.48732, 0.48984, 0.49115], best = 0.4911\n9B1Y_4: TM-scores per model = [0.23583, 0.18534, 0.15587, 0.19593, 0.21864], best = 0.2358\n9B2K_B: TM-scores per model = [0.21751, 0.23325, 0.23657, 0.22819, 0.24006], best = 0.2401\n9B83_C: TM-scores per model = [0.17624, 0.18181, 0.16719, 0.1508, 0.17013], best = 0.1818\n9B84_F: TM-scores per model = [0.45061, 0.39655, 0.38718, 0.38719, 0.40685], best = 0.4506\n9B89_C: TM-scores per model = [0.23192, 0.24464, 0.23874, 0.23402, 0.23531], best = 0.2446\n9C8K_2: TM-scores per model = [0.19823, 0.17109, 0.15412, 0.18057, 0.14802], best = 0.1982\n9DCF_C: TM-scores per model = [0.33655, 0.34408, 0.35086, 0.34597, 0.26805], best = 0.3509\n9DE5_C: TM-scores per model = [0.32524, 0.33566, 0.39297, 0.34616, 0.2843], best = 0.3930\n9DE5_D: TM-scores per model = [0.30448, 0.325, 0.36058, 0.32887, 0.28131], best = 0.3606\n9DE6_A: TM-scores per model = [0.30874, 0.32568, 0.34541, 0.32486, 0.27449], best = 0.3454\n9DE6_B: TM-scores per model = [0.3246, 0.35635, 0.3871, 0.34918, 0.31502], best = 0.3871\n9DE7_A: TM-scores per model = [0.41034, 0.35725, 0.37766, 0.38088, 0.33728], best = 0.4103\n9DE8_A: TM-scores per model = [0.38021, 0.33407, 0.40041, 0.36884, 0.28152], best = 0.4004\n9DPA_C: TM-scores per model = [0.46634, 0.43941, 0.48494, 0.4826, 0.46549], best = 0.4849\n9DPB_C: TM-scores per model = [0.51469, 0.49617, 0.49698, 0.51987, 0.50916], best = 0.5199\n9DPL_C: TM-scores per model = [0.37984, 0.35222, 0.39103, 0.3935, 0.37777], best = 0.3935\n9DRS_C: TM-scores per model = [0.49853, 0.45103, 0.46817, 0.50787, 0.46266], best = 0.5079\n9E2W_F: TM-scores per model = [0.14606, 0.14736, 0.21248, 0.19897, 0.15428], best = 0.2125\n9E2Y_F: TM-scores per model = [0.1959, 0.19318, 0.13943, 0.13125, 0.19007], best = 0.1959\n9E2Z_F: TM-scores per model = [0.16237, 0.13415, 0.12997, 0.13599, 0.15548], best = 0.1624\n9FCV_B: TM-scores per model = [0.19645, 0.18441, 0.26129, 0.19906, 0.25439], best = 0.2613\n9FIB_Y: TM-scores per model = [0.07355, 0.09546, 0.1302, 0.08184, 0.08928], best = 0.1302\n9FN3_B: TM-scores per model = [0.19909, 0.23637, 0.22807, 0.21047, 0.21596], best = 0.2364\n9G06_a: TM-scores per model = [0.41252, 0.42865, 0.43212, 0.44192, 0.42066], best = 0.4419\n9GBW_R: TM-scores per model = [0.19382, 0.20561, 0.26083, 0.25461, 0.24301], best = 0.2608\n9GBZ_R: TM-scores per model = [0.22223, 0.17286, 0.28916, 0.19648, 0.16117], best = 0.2892\n9GC0_Q: TM-scores per model = [0.18197, 0.15778, 0.16439, 0.19204, 0.17944], best = 0.1920\n9GCL_A: TM-scores per model = [0.15463, 0.15383, 0.16921, 0.1997, 0.17389], best = 0.1997\n9GCM_A: TM-scores per model = [0.23025, 0.202, 0.28035, 0.22803, 0.1859], best = 0.2803\n9GFT_A3: TM-scores per model = [0.47713, 0.46663, 0.50499, 0.47317, 0.47493], best = 0.5050\n9GFT_AU: TM-scores per model = [0.4533, 0.42803, 0.45877, 0.48006, 0.46363], best = 0.4801\n9GHF_Z: TM-scores per model = [0.54375, 0.52137, 0.54963, 0.56646, 0.56209], best = 0.5665\n9HNY_CA: TM-scores per model = [0.13756, 0.08942, 0.12106, 0.09435, 0.11188], best = 0.1376\n9IS7_B: TM-scores per model = [0.17676, 0.14973, 0.1695, 0.21451, 0.20315], best = 0.2145\n9ISV_A: TM-scores per model = [0.1864, 0.15506, 0.17057, 0.1874, 0.18889], best = 0.1889\n9J3R_B: TM-scores per model = [0.16024, 0.12292, 0.14517, 0.15086, 0.1713], best = 0.1713\n9J3T_B: TM-scores per model = [0.14326, 0.14443, 0.13726, 0.13506, 0.12882], best = 0.1444\n9J6Y_E: TM-scores per model = [0.17006, 0.1225, 0.13849, 0.1641, 0.14909], best = 0.1701\n9KPO_B: TM-scores per model = [0.22584, 0.16289, 0.18222, 0.18277, 0.23273], best = 0.2327\n9L0R_K: TM-scores per model = [0.12737, 0.11874, 0.12565, 0.12641, 0.14264], best = 0.1426\n9L5R_2: TM-scores per model = [0.216, 0.14966, 0.12505, 0.17272, 0.19906], best = 0.2160\n9L5R_6: TM-scores per model = [0.17186, 0.17852, 0.14566, 0.15684, 0.18876], best = 0.1888\n9L5S_5: TM-scores per model = [0.29774, 0.32502, 0.32168, 0.30783, 0.30133], best = 0.3250\n9L5S_6: TM-scores per model = [0.1741, 0.17246, 0.16838, 0.16871, 0.18298], best = 0.1830\n9L5T_6: TM-scores per model = [0.18275, 0.18269, 0.17218, 0.16071, 0.14837], best = 0.1827\n9LCR_B: TM-scores per model = [0.26178, 0.2212, 0.24737, 0.22402, 0.2506], best = 0.2618\n9LMF_F: TM-scores per model = [0.19638, 0.15905, 0.20672, 0.18884, 0.2062], best = 0.2067\n9MTY_C: TM-scores per model = [0.109, 0.13215, 0.12368, 0.13516, 0.11942], best = 0.1352\n9N2B_5: TM-scores per model = [0.7191, 0.72112, 0.71161, 0.72369, 0.72914], best = 0.7291\n9N2C_Pt: TM-scores per model = [0.54398, 0.51455, 0.55635, 0.56971, 0.55485], best = 0.5697\n\nMean TM-score: 0.3431\n","output_type":"stream"}],"execution_count":18}]}