{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87793,"databundleVersionId":12276181,"sourceType":"competition"},{"sourceId":10855324,"sourceType":"datasetVersion","datasetId":6742586},{"sourceId":10880419,"sourceType":"datasetVersion","datasetId":6760509},{"sourceId":11065669,"sourceType":"datasetVersion","datasetId":6889817},{"sourceId":11752460,"sourceType":"datasetVersion","datasetId":7356947},{"sourceId":11913080,"sourceType":"datasetVersion","datasetId":7489544},{"sourceId":12199811,"sourceType":"datasetVersion","datasetId":7684811},{"sourceId":12199817,"sourceType":"datasetVersion","datasetId":7684816}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os,sys\n\nimport pandas as pd\npd.set_option('display.max_columns', 20)\npd.set_option('display.expand_frame_repr', False)\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom timeit import default_timer as timer\nimport re\nimport optuna\nimport matplotlib \nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nimport os, sys, shutil\n\n# 1) make sure you have a writable copy\nshutil.copytree(\n  '/kaggle/input/drfold2/DRfold2',\n  '/kaggle/working/drfold',\n  dirs_exist_ok=True\n)\n\n# 2) point Python at the cfg_97 folder under that copy\nBASE = '/kaggle/working/drfold'\nsys.path.insert(0, os.path.join(BASE, 'cfg_97'))\nfrom EvoMSA2XYZ import MSA2XYZ\nfrom RNALM2.Model import RNA2nd\nfrom data import parse_seq, Get_base\nfrom util import outpdb\n# parse_pdb_to_xyz write_frame_coor_to_pdb\n\nimport numpy as np\n\n\n# helper--\nclass dotdict(dict):\n\t__setattr__ = dict.__setitem__\n\t__delattr__ = dict.__delitem__\n\n\tdef __getattr__(self, name):\n\t\ttry:\n\t\t\treturn self[name]\n\t\texcept KeyError:\n\t\t\traise AttributeError(name)\n\ndef time_to_str(t, mode='min'):\n\tif mode=='min':\n\t\tt  = int(t)/60\n\t\thr = t//60\n\t\tmin = t%60\n\t\treturn '%2d hr %02d min'%(hr,min) \n\telif mode=='sec':\n\t\tt   = int(t)\n\t\tmin = t//60\n\t\tsec = t%60\n\t\treturn '%2d min %02d sec'%(min,sec)\n\n\telse:\n\t\traise NotImplementedError\n\ndef gpu_memory_use():\n    if torch.cuda.is_available():\n        device = torch.device(0)\n        free, total = torch.cuda.mem_get_info(device)\n        used= (total - free) / 1024 ** 3\n        return int(round(used))\n    else:\n        return 0\n\ndef set_aspect_equal(ax):\n\tx_limits = ax.get_xlim()\n\ty_limits = ax.get_ylim()\n\tz_limits = ax.get_zlim()\n\n\t# Compute the mean of each axis\n\tx_middle = np.mean(x_limits)\n\ty_middle = np.mean(y_limits)\n\tz_middle = np.mean(z_limits)\n\n\t# Compute the max range across all axes\n\tmax_range = max(x_limits[1] - x_limits[0],\n\t\t\t\t\ty_limits[1] - y_limits[0],\n\t\t\t\t\tz_limits[1] - z_limits[0]) / 2.0\n\n\t# Set the new limits to ensure equal scaling\n\tax.set_xlim(x_middle - max_range, x_middle + max_range)\n\tax.set_ylim(y_middle - max_range, y_middle + max_range)\n\tax.set_zlim(z_middle - max_range, z_middle + max_range)\n\n\nprint('torch',torch.__version__)\nprint('torch.cuda',torch.version.cuda)\n\nprint('IMPORT OK!!!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:40:33.713422Z","iopub.execute_input":"2025-06-22T15:40:33.713666Z","iopub.status.idle":"2025-06-22T15:40:56.477980Z","shell.execute_reply.started":"2025-06-22T15:40:33.713643Z","shell.execute_reply":"2025-06-22T15:40:56.477057Z"}},"outputs":[{"name":"stdout","text":"will do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/drfold/cfg_97/EvoMSA2XYZ.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  RNAlm.load_state_dict(torch.load(saved_model,map_location=torch.device('cpu')),strict=False)\n","output_type":"stream"},{"name":"stdout","text":"torch 2.5.1+cu121\ntorch.cuda 12.1\nIMPORT OK!!!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ─── TOP-OF-NOTEBOOK INTEGRATION CELL ───\nCFG_FOLDER  = \"/kaggle/input/drfold2/DRfold2/model_hub/cfg_97\"\nROOT_FOLDER = \"/kaggle/input/drfold2/DRfold2\"\ndef write_frame_coor_to_pdb(coord, seq, savefile):\n    \"\"\"\n    coord: np.ndarray of shape (L,3,3) from your model (P, sugar, N)\n    seq:   string of length L\n    savefile: path to write PDB with only C1' atom per residue\n    \"\"\"\n    L = coord.shape[0]\n    with open(savefile, 'w') as f:\n        count = 1\n        for i, res in enumerate(seq):\n            x, y, z = coord[i, 1]   # channel=1 → sugar atom\n            # PDB ATOM line building:\n            # atom serial, atom name, residue name, chain A, residue seq, x,y,z, occup,temp, element\n            f.write(\n                f\"ATOM  {count:5d}  C1' {res:>3s} A{ i+1:4d}\"\n                f\"{x:8.3f}{y:8.3f}{z:8.3f}  1.00  0.00           C\\n\"\n            )\n            count += 1\n        f.write(\"TER\\n\")\n\n\n# ── pure-Python FASTA I/O ──\ndef parse_fasta(path):\n    recs, h, seqs = [], None, []\n    with open(path) as f:\n        for line in f:\n            line = line.rstrip(\"\\n\")\n            if line.startswith(\">\"):\n                if h is not None:\n                    recs.append((h, \"\".join(seqs)))\n                h = line[1:].split()[0]\n                seqs = []\n            else:\n                seqs.append(line)\n        if h is not None:\n            recs.append((h, \"\".join(seqs)))\n    return recs\n\n# ── load model & weights ──\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nconfig = dict(seq_dim=6, msa_dim=7, N_ensemble=1, N_cycle=8, m_dim=64, s_dim=64, z_dim=64)\nmodel = MSA2XYZ(**config).to(device)\n\nweights_path = os.path.join(CFG_FOLDER, \"model_0\")\nmodel.load_state_dict(torch.load(weights_path, map_location=device), strict=False)\nmodel.eval()\n\nprint(\"✅ DRfold2 + MSA integration ready. Device:\", device)\nprint(\"   parse_seq → (L,6); parse_msa → (N,L,7); model loaded.\")\n# ─── end of integration cell ───\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:41:08.235966Z","iopub.execute_input":"2025-06-22T15:41:08.236257Z","iopub.status.idle":"2025-06-22T15:41:10.370221Z","shell.execute_reply.started":"2025-06-22T15:41:08.236235Z","shell.execute_reply":"2025-06-22T15:41:10.369299Z"}},"outputs":[{"name":"stdout","text":"will do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-02d836b58166>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(weights_path, map_location=device), strict=False)\n","output_type":"stream"},{"name":"stdout","text":"✅ DRfold2 + MSA integration ready. Device: cuda\n   parse_seq → (L,6); parse_msa → (N,L,7); model loaded.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install /kaggle/input/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:41:12.596657Z","iopub.execute_input":"2025-06-22T15:41:12.596993Z","iopub.status.idle":"2025-06-22T15:41:18.071261Z","shell.execute_reply.started":"2025-06-22T15:41:12.596967Z","shell.execute_reply":"2025-06-22T15:41:18.070382Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython==1.85) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->biopython==1.85) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython==1.85) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->biopython==1.85) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->biopython==1.85) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->biopython==1.85) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->biopython==1.85) (2024.2.0)\nInstalling collected packages: biopython\nSuccessfully installed biopython-1.85\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#tuning\nclass SequenceStructureDataset(Dataset):\n    def __init__(self, csv_file, data_dir, transform=None):\n        self.df = pd.read_csv(csv_file)\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        features = np.load(self.data_dir / f\"{row['ID']}_features.npy\")\n        coords   = np.load(self.data_dir / f\"{row['ID']}_coords.npy\")\n        sample = {'features': torch.from_numpy(features).float(),\n                  'coords':   torch.from_numpy(coords).float()}\n        return self.transform(sample) if self.transform else sample\n\n\nDATA_KAGGLE_DIR = '/kaggle/input/stanford-rna-3d-folding'\n\ntrain_dataset = SequenceStructureDataset(\n    csv_file=f\"{DATA_KAGGLE_DIR}/train_labels.csv\",\n    data_dir=f\"{DATA_KAGGLE_DIR}/train_data\"\n)\nval_dataset = SequenceStructureDataset(\n    csv_file=f\"{DATA_KAGGLE_DIR}/validation_labels.csv\",\n    data_dir=f\"{DATA_KAGGLE_DIR}/validation_data\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:41:20.219816Z","iopub.execute_input":"2025-06-22T15:41:20.220133Z","iopub.status.idle":"2025-06-22T15:41:20.616518Z","shell.execute_reply.started":"2025-06-22T15:41:20.220109Z","shell.execute_reply":"2025-06-22T15:41:20.615824Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# from datetime import datetime\n# import pytz\n# print('LOGGING TIME OF START:',  datetime.strftime(datetime.now(pytz.timezone('Asia/Singapore')), \"%Y-%m-%d %H:%M:%S\"))\n\n\n# try:\n#     import Bio\n# except:\n#     #for drfold2 --------\n#     #!pip install biopython\n#     !pip install /kaggle/input/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# !pip install /kaggle/input/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# !pip install biopython\n# print('PIP INSTALL OK !!!!')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:57:48.199554Z","iopub.execute_input":"2025-05-15T12:57:48.199912Z","iopub.status.idle":"2025-05-15T12:57:51.056598Z","shell.execute_reply.started":"2025-05-15T12:57:48.199871Z","shell.execute_reply":"2025-05-15T12:57:51.055868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MODE = 'local' #'local' # submit\nMODE = 'local'\nMSA_DIR = '/kaggle/input/stanford-rna-3d-folding/MSA_v2'\n\nDATA_KAGGLE_DIR = '/kaggle/input/stanford-rna-3d-folding'\nif MODE == 'local':\n    valid_df = pd.read_csv(\"/kaggle/input/validation-sequences-clean-csv/validation_sequences_clean.csv\")\n    label_df = pd.read_csv(\"/kaggle/input/validation-labels-clean-csv/validation_labels_clean.csv\")\n    label_df['target_id'] = label_df['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n\nif MODE == 'submit':\n\tvalid_df = pd.read_csv(f'{DATA_KAGGLE_DIR}/test_sequences.csv')\n\nprint('len(valid_df)',len(valid_df))\nprint(valid_df.iloc[0])\nprint('')\n\n\n# cfg = dotdict(\n#     num_conf = 5,\n#     max_length=480,\n# )\nNUM_CONF=5\nMAX_LENGTH=480\nDEVICE='cuda' #'cpu'\n\nprint('MODE:', MODE)\nprint('SETTING OK!!!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:41:24.824961Z","iopub.execute_input":"2025-06-22T15:41:24.825251Z","iopub.status.idle":"2025-06-22T15:41:24.909755Z","shell.execute_reply.started":"2025-06-22T15:41:24.825229Z","shell.execute_reply":"2025-06-22T15:41:24.908844Z"}},"outputs":[{"name":"stdout","text":"len(valid_df) 94\ntarget_id                                                     9L5R_2\nsequence           AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUU...\ntemporal_cutoff                                           2025-03-12\ndescription        Cryo-EM structure of the thermophile spliceoso...\nall_sequences      >9L5R_1|Chain A[auth 2]|U2 snRNA|Chaetomium th...\nName: 0, dtype: object\n\nMODE: local\nSETTING OK!!!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nos.makedirs('/kaggle/working/drfold', exist_ok=True)\n!cp -r /kaggle/input/drfold2/DRfold2/* /kaggle/working/drfold/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:41:27.898193Z","iopub.execute_input":"2025-06-22T15:41:27.898509Z","iopub.status.idle":"2025-06-22T15:41:30.229816Z","shell.execute_reply.started":"2025-06-22T15:41:27.898482Z","shell.execute_reply":"2025-06-22T15:41:30.228777Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import sys, os\n# CFG97 = os.path.join('/kaggle/input/drfold/DRfold2/DRfold2', 'cfg_97')\n# assert os.path.isdir(CFG97), f\"{CFG97} not found!\"\n# sys.path.insert(0, CFG97)\n\n# # now this should succeed:\n# from EvoMSA2XYZ import MSA2XYZ\n# from RNALM2.Model import RNA2nd\n# from data         import parse_seq, Get_base, BASE_COOR\n# from data         import write_frame_coor_to_pdb, parse_pdb_to_xyz\n\n# print(\"imported!\", MSA2XYZ, RNA2nd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:57:53.193764Z","iopub.execute_input":"2025-05-15T12:57:53.194043Z","iopub.status.idle":"2025-05-15T12:57:53.197779Z","shell.execute_reply.started":"2025-05-15T12:57:53.19402Z","shell.execute_reply":"2025-05-15T12:57:53.196866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\n\n# 1) Point at the cfg_97 folder (adjust this to your actual path)\nCFG97 = '/kaggle/input/drfold2/DRfold2/cfg_97'\n\n# 2) Load the base.npy file once into a Python variable\nBASE_COOR = np.load(os.path.join(CFG97, 'base.npy'))\n\n# 3) Now import your parsing and model code\nimport sys\nsys.path.insert(0, CFG97)\nfrom data import parse_seq, Get_base\n# (no BASE_COOR to import from data.py)\n\n# 4) When you need the 3×3 base coordinates for a sequence, call:\nsequence = \"ACGUACGUA\"\nbase_coords = Get_base(sequence, BASE_COOR)\n# base_coords.shape == (len(sequence), 3, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:41:32.784515Z","iopub.execute_input":"2025-06-22T15:41:32.784938Z","iopub.status.idle":"2025-06-22T15:41:32.791498Z","shell.execute_reply.started":"2025-06-22T15:41:32.784904Z","shell.execute_reply":"2025-06-22T15:41:32.790544Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def parse_pdb_to_xyz(pdb_file, atom_name=\" P  \"):\n    coords, resid, resname = [], [], []\n    with open(pdb_file) as f:\n        for line in f:\n            if line.startswith(\"ATOM\") and line[12:16].strip() == atom_name.strip():\n                x = float(line[30:38]); y = float(line[38:46]); z = float(line[46:54])\n                coords.append((x,y,z))\n                resid.append(int(line[22:26]))\n                resname.append(line[17:20].strip())\n    return np.array(coords, dtype=np.float32), resname, resid\n\n\ndef parse_msa(path: str, max_seqs: int = 100) -> np.ndarray:\n    \"\"\"\n    Reads a FASTA/A3M MSA, caps at max_seqs rows, one-hots each → (L,6),\n    and stacks into (N,L,6) - no target marker channel for now.\n    \"\"\"\n    recs = parse_fasta(path)[:max_seqs]\n    if not recs:\n        raise ValueError(f\"no sequences in MSA at {path}\")\n    N = len(recs)\n    L = len(recs[0][1])\n    msa = np.zeros((N, L, 6), dtype=np.float32)  # Changed from 7 to 6\n    \n    for i, (_, seq) in enumerate(recs):\n        # Always use our one-hot function for MSA\n        sq = parse_seq_onehot(seq)\n        if i < 5:  # Only print first 5 rows to reduce spam\n            print(f\"   Row {i}: seq length {len(seq)}, parse_seq_onehot shape {sq.shape}, expected L={L}\")\n        \n        if sq.shape[0] != L:\n            raise ValueError(f\"row {i} length {sq.shape[0]} != expected {L}\")\n        if sq.shape[1] != 6:\n            raise ValueError(f\"row {i} features {sq.shape[1]} != expected 6\")\n        \n        msa[i,:,:6] = sq\n        # Remove the target marker for now\n    return msa\n\n# ── one-hot encoder for a single RNA sequence ──\ndef parse_seq_onehot(seq: str) -> np.ndarray:\n    \"\"\"\n    Returns (L,6) float32 array:\n      ch0–3 = one-hot A/G/C/U,\n      ch4   = everything else (gaps/unknown),\n      ch5   = mask = 1.0 everywhere.\n    \"\"\"\n    L = len(seq)\n    feat = np.zeros((L,6), dtype=np.float32)\n    cmap = {'A':0, 'G':1, 'C':2, 'U':3, 'T':3}\n    for i, r in enumerate(seq):\n        feat[i, cmap.get(r,4)] = 1.0\n    feat[:,5] = 1.0\n    return feat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:44:30.838226Z","iopub.execute_input":"2025-06-22T15:44:30.838598Z","iopub.status.idle":"2025-06-22T15:44:30.847332Z","shell.execute_reply.started":"2025-06-22T15:44:30.838568Z","shell.execute_reply":"2025-06-22T15:44:30.846461Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n###########################################################3\nKAGGLE_TRUTH_PDB_DIR ='/kaggle/working/drfold/kaggle-casp15-truth'\nUSALIGN = '/kaggle/working/USalign' \nos.system('cp /kaggle/input/usalign/USalign /kaggle/working/')\nos.system('sudo chmod u+x /kaggle/working/USalign')\n\n# evaluate helper\ndef get_truth_df(target_id, label_df):\n    truth_df = label_df[label_df['target_id'] == target_id]\n    truth_df = truth_df.reset_index(drop=True)\n    return truth_df\n\ndef parse_usalign_for_tm_score(output):\n    # Extract TM-score based on length of reference structure (second)\n    tm_score_match = re.findall(r'TM-score=\\s+([\\d.]+)', output)[1]\n    if not tm_score_match:\n        raise ValueError('No TM score found')\n    return float(tm_score_match)\n\ndef parse_usalign_for_transform(output):\n    # Locate the rotation matrix section\n    matrix_lines = []\n    found_matrix = False\n\n    for line in output.splitlines():\n        if \"The rotation matrix to rotate Structure_1 to Structure_2\" in line:\n            found_matrix = True\n        elif found_matrix and re.match(r'^\\d+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+\\s+[-\\d.]+$', line):\n            matrix_lines.append(line)\n        elif found_matrix and not line.strip():\n            break  # Stop parsing if an empty line is encountered after the matrix\n\n    # Parse the rotation matrix values\n    rotation_matrix = []\n    for line in matrix_lines:\n        parts = line.split()\n        row_values = list(map(float, parts[1:]))  # Skip the first column (index)\n        rotation_matrix.append(row_values)\n    return np.array(rotation_matrix)\n\n\n\n# data helper\ndef make_data(seq, target_id, msa_dir):\n    base = Get_base(seq, BASE_COOR)\n    seq_idx = np.arange(len(seq)) + 1\n    \n    # Try to load MSA, fallback to single sequence if not found\n    msa_file = os.path.join(msa_dir, f\"{target_id}.MSA.fasta\")\n    \n    if os.path.exists(msa_file):\n        print(f\"✅ Loading MSA from {msa_file}\")\n        print(f\"   Target sequence length: {len(seq)}\")\n        try:\n            msa_data = parse_msa(msa_file, max_seqs=100)  # Shape: (N,L,7)\n            print(f\"   MSA loaded: {msa_data.shape[0]} sequences, {msa_data.shape[1]} positions\")\n            \n            # Verify MSA matches sequence length\n            if msa_data.shape[1] != len(seq):\n                print(f\"⚠️ MSA length {msa_data.shape[1]} != sequence length {len(seq)}, using single sequence\")\n                msa_data = create_single_sequence_msa(seq)\n            else:\n                # Verify target sequence matches first row of MSA\n                target_from_msa = msa_to_sequence(msa_data[0, :, :6])\n                if not sequences_match(target_from_msa, seq):\n                    print(f\"⚠️ Target sequence mismatch with MSA, using single sequence\")\n                    msa_data = create_single_sequence_msa(seq)\n        except Exception as e:\n            print(f\"⚠️ Error loading MSA: {e}, using single sequence\")\n            msa_data = create_single_sequence_msa(seq)\n    else:\n        print(f\"⚠️ MSA file not found: {msa_file}, using single sequence\")\n        msa_data = create_single_sequence_msa(seq)\n    \n    msa = torch.from_numpy(msa_data).float()  # Shape: (N,L,7)\n    base_x = torch.from_numpy(base).float()\n    seq_idx = torch.from_numpy(seq_idx).long()\n    \n    return msa, base_x, seq_idx\n\ndef create_single_sequence_msa(seq):\n    \"\"\"Create a fake MSA with just the target sequence duplicated\"\"\"\n    # Always use our one-hot function\n    aa_type = parse_seq_onehot(seq)  \n    print(f\"   aa_type shape: {aa_type.shape}\")\n    \n    # Create MSA with 2 identical sequences, 6 channels only\n    msa_data = np.zeros((2, len(seq), 6), dtype=np.float32)  # Changed from 7 to 6\n    msa_data[0, :, :6] = aa_type  # First sequence\n    msa_data[1, :, :6] = aa_type  # Duplicate\n    # Remove target marker for now\n    return msa_data\n\ndef msa_to_sequence(msa_row):\n    \"\"\"Convert one-hot MSA row back to sequence string\"\"\"\n    seq = \"\"\n    for i in range(msa_row.shape[0]):\n        if msa_row[i, 0] == 1.0: seq += \"A\"\n        elif msa_row[i, 1] == 1.0: seq += \"G\"\n        elif msa_row[i, 2] == 1.0: seq += \"C\"\n        elif msa_row[i, 3] == 1.0: seq += \"U\"\n        else: seq += \"N\"  # Unknown/gap\n    return seq\n\ndef sequences_match(seq1, seq2, gap_chars=\"-N\"):\n    \"\"\"Check if sequences match, ignoring gaps\"\"\"\n    s1 = ''.join(c for c in seq1.upper() if c not in gap_chars)\n    s2 = ''.join(c for c in seq2.upper() if c not in gap_chars)\n    return s1 == s2\n    \ndef make_dummy_solution():\n    solution=dotdict()\n    for i, row in valid_df.iterrows():\n        target_id = row.target_id\n        sequence = row.sequence\n        solution[target_id]=dotdict(\n            target_id=target_id,\n            sequence=sequence,\n            coord=[],\n        )\n    return solution\n\ndef solution_to_submit_df(solution):\n    submit_df = []\n    for k,s in solution.items():\n        df = coord_to_df(s.sequence, s.coord, s.target_id)\n        submit_df.append(df)\n    \n    submit_df = pd.concat(submit_df)\n    return submit_df\n \n\ndef coord_to_df(sequence, coord, target_id):\n    L = len(sequence)\n    df = pd.DataFrame()\n    df['ID'] = [f'{target_id}_{i + 1}' for i in range(L)]\n    df['resname'] = [s for s in sequence]\n    df['resid'] = [i + 1 for i in range(L)]\n\n    num_coord = len(coord)\n    for j in range(num_coord):\n        df[f'x_{j+1}'] = coord[j][:, 0]\n        df[f'y_{j+1}'] = coord[j][:, 1]\n        df[f'z_{j+1}'] = coord[j][:, 2]\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:44:33.687365Z","iopub.execute_input":"2025-06-22T15:44:33.687665Z","iopub.status.idle":"2025-06-22T15:44:33.721959Z","shell.execute_reply.started":"2025-06-22T15:44:33.687642Z","shell.execute_reply":"2025-06-22T15:44:33.721268Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Add this diagnostic code to check MSA availability\nimport os\n\nmsa_directory = \"/kaggle/input/stanford-rna-3d-folding/MSA_v2\"\ntest_data = pd.read_csv(\"/kaggle/input/validation-sequences-clean-csv/validation_sequences_clean.csv\")\n\nprint(f\"MSA directory exists: {os.path.exists(msa_directory)}\")\n\nif os.path.exists(msa_directory):\n    msa_files = os.listdir(msa_directory)\n    print(f\"Number of MSA files found: {len(msa_files)}\")\n    print(f\"First 10 MSA files: {msa_files[:10]}\")\n    \n    # Check what target IDs we're looking for\n    target_ids = test_data['target_id'].tolist()\n    print(f\"First 10 target IDs: {target_ids}\")\n    \n    # Check if any MSA files match our target IDs\n    matches = []\n    for target_id in target_ids:\n        expected_file = f\"{target_id}.MSA.fasta\"\n        if expected_file in msa_files:\n            matches.append(expected_file)\n    \n    print(f\"Matching MSA files found: {len(matches)}\")\n\nelse:\n    print(\"MSA directory does not exist!\")\n    print(\"Available directories in /kaggle/input/stanford-rna-3d-folding/:\")\n    if os.path.exists(\"/kaggle/input/stanford-rna-3d-folding/\"):\n        print(os.listdir(\"/kaggle/input/stanford-rna-3d-folding/\"))\n    else:\n        print(\"Stanford RNA 3D folding dataset not available!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:41:42.884409Z","iopub.execute_input":"2025-06-22T15:41:42.884889Z","iopub.status.idle":"2025-06-22T15:41:42.949522Z","shell.execute_reply.started":"2025-06-22T15:41:42.884843Z","shell.execute_reply":"2025-06-22T15:41:42.948776Z"}},"outputs":[{"name":"stdout","text":"MSA directory exists: True\nNumber of MSA files found: 2534\nFirst 10 MSA files: ['3JCS_6.MSA.fasta', '7MSF_R.MSA.fasta', '2OOM_B.MSA.fasta', '1ZDI_S.MSA.fasta', '5FJ1_H.MSA.fasta', '6UF1_C.MSA.fasta', '4V8A_AB.MSA.fasta', '6JDG_G.MSA.fasta', '5NCO_1.MSA.fasta', '5DI4_A.MSA.fasta']\nFirst 10 target IDs: ['9L5R_2', '9GFT_AU', '9L0R_K', '9GFT_A3', '9B2K_B', '9B0S_Et', '9J3T_B', '9LCR_B', '8KEB_A', '9L5S_5', '8VXZ_C', '9J6Y_E', '8QHU_5', '9GHF_Z', '9KPO_B', '9N2B_5', '9N2C_Pt', '9B1Y_4', '9G06_a', '9DE8_A', '9B83_C', '8ZMH_A', '9E2Y_F', '9DE7_A', '8Y9L_B', '9FIB_Y', '9J3R_B', '9DPB_C', '8XTP_A', '8ZTV_Y', '8Y9M_B', '8ZQ9_A', '8XTP_B', '9B89_C', '8SYK_C', '9FN3_B', '8QHU_3', '9DRS_C', '8XTR_A', '9LMF_F', '9DE6_B', '8SYK_B', '9DE6_A', '8R7N_A', '8K85_A', '9FCV_B', '9DPA_C', '9DE5_C', '8VZ6_S', '8YIG_C', '9B84_F', '9C8K_2', '9B0Q_AP', '9E2Z_F', '8Z8Q_B', '9E2W_F', '8KHH_A', '8Z8U_B', '8ZTU_Y', '9GCL_A', '8RRI_Ax', '9L5S_6', '9GCM_A', '8Z9K_B', '9MTY_C', '8QHU_7', '9GBW_R', '8T5O_A', '9DPL_C', '8WFA_B', '9ISV_A', '9AR6_B', '9DE5_D', '8ZDR_A', '8WFB_B', '9L5R_6', '9IS7_B', '9GC0_Q', '8YIH_C', '9HNY_CA', '8VK7_B', '8WF8_B', '8QHU_S4', '8ZAU_A', '9AR7_B', '9AR4_B', '8Y9N_B', '8QHU_4', '8RWG_C', '8YII_C', '9DCF_C', '9L5T_6', '8WF9_B', '9GBZ_R']\nMatching MSA files found: 60\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"################### start here !!! #######################################################3\nout_dir = '/kaggle/working/model-output'\nos.makedirs(out_dir, exist_ok=True)\nsolution = make_dummy_solution()\n\n\n#load model (these are moified versions, not the same from their github repo)\nrnalm = RNA2nd(dict(\n    s_in_dim=5,\n    z_in_dim=2,\n    s_dim= 512,\n    z_dim= 128,\n    N_elayers=18,\n))\nrnalm_file = '/kaggle/working/drfold/model_hub/RCLM/epoch_67000'\nprint(rnalm_file)\nprint(\n    rnalm.load_state_dict(torch.load(rnalm_file, map_location='cpu', weights_only=True), strict=False)\n    #Unexpected key(s) in state_dict: \"ss_head.linear.weight\", \"ss_head.linear.bias\".\n)\nrnalm = rnalm.to(DEVICE)\nrnalm = rnalm.eval()\ntotal_time_taken = 0\nmax_gpu_mem_used = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:44:41.201481Z","iopub.execute_input":"2025-06-22T15:44:41.201870Z","iopub.status.idle":"2025-06-22T15:44:42.179114Z","shell.execute_reply.started":"2025-06-22T15:44:41.201832Z","shell.execute_reply":"2025-06-22T15:44:42.178158Z"}},"outputs":[{"name":"stdout","text":"will do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n/kaggle/working/drfold/model_hub/RCLM/epoch_67000\n_IncompatibleKeys(missing_keys=[], unexpected_keys=['ss_head.linear.weight', 'ss_head.linear.bias'])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"cfg = dict(\n    seq_dim=6,\n    msa_dim=7,\n    N_ensemble=1,   # how many ensemble members\n    N_cycle=8,      # how many recycling cycles\n    m_dim=64,\n    s_dim=64,\n    z_dim=64,\n)\nfor c in range(NUM_CONF): \n    msa2xyz = MSA2XYZ(**cfg)\n    msa2xyz_file = [\n        f'/kaggle/working/drfold/model_hub/cfg_97/model_{k}' for k in [0,1,2,8,9]\n    ][c]\n    print(msa2xyz_file)\n    print(\n        msa2xyz.load_state_dict(torch.load(msa2xyz_file, map_location='cpu', weights_only=True), strict=True)\n    )\n    msa2xyz.msaxyzone.premsa.rnalm = rnalm\n    msa2xyz = msa2xyz.to(DEVICE)\n    msa2xyz = msa2xyz.eval()\n \n    for i,row in valid_df.iterrows():\n        start_timer = timer()\n        \n        target_id = row.target_id\n        sequence = row.sequence\n        seq = row.sequence    \n        \n        L = len(sequence)\n        if L>MAX_LENGTH:\n            i0 = np.random.choice(L-MAX_LENGTH+1)\n            i1 = i0 + MAX_LENGTH\n        else:\n            i0 = 0\n            i1 = L\n        \n        seq = sequence[i0:i1]\n        print(c,i,target_id, L, seq[:75]+'...')\n        \n        msa, base_x, seq_idx = make_data(seq, target_id, MSA_DIR)\n        msa, base_x, seq_idx = msa.to(DEVICE), base_x.to(DEVICE), seq_idx.to(DEVICE)\n        \n        # MSA usage verification\n        print(f\"   MSA tensor shape: {msa.shape}\")\n        if msa.shape[0] > 2:\n            print(f\"   ✅ Using real MSA with {msa.shape[0]} sequences\")\n        else:\n            print(f\"   ⚠️ Using single sequence (duplicated)\")\n            \n        # Extract sequence features (first 6 channels) for the model\n        # The model expects seq to be (L, 6), not (L, 7)\n        seq_features = msa[0, :, :6]  # Take first row, first 6 channels\n        print(f\"   Seq features shape: {seq_features.shape}\")\n            \n        secondary = None #secondary structure\n    \n        with torch.no_grad(): \n            out = msa2xyz.pred(msa, seq_idx, secondary, base_x, np.array(list(seq)))\n\n        # key = list(out.keys()) # plddt(L,L), coor(L,3,3), dist_p(L,L,38), dist_c, dist_n,\n        # for k in key:\n        #     print(k, type(out[k]), out[k].shape)\n \n        \n        if L!=len(seq):\n             out['coor'] = np.pad(out['coor'] ,((i0, L - i1), (0, 0), (0, 0)), 'constant', constant_values=0)\n\n\n        print('out:',  out['coor'].shape)\n        \n        # Log MSA usage statistics\n        msa_depth = msa.shape[0]\n        if msa_depth > 2:\n            print(f\"   📊 Processed with MSA depth: {msa_depth}\")\n        else:\n            print(f\"   📊 Processed with single sequence (no MSA)\")\n            \n        time_taken = timer()-start_timer\n        time_taken = timer()-start_timer\n        total_time_taken += time_taken\n        print('time_taken:', time_to_str(time_taken, mode='sec')) \n        \n        gpu_mem_used = gpu_memory_use()\n        max_gpu_mem_used = max(max_gpu_mem_used,gpu_mem_used)\n        print('gpu_mem_used:', gpu_mem_used, 'GB')\n\n        torch.cuda.empty_cache() \n        sugar_xyz = out['coor'][:, 1, :]   # shape (L,3)\n        solution[target_id].coord.append(sugar_xyz)\n    print('')\n    \n#-----end of conformation generation ----\nprint('MAX_LENGTH', MAX_LENGTH)\nprint('### total_time_taken:', time_to_str(total_time_taken, mode='min'))\nprint('### max_gpu_mem_used:', max_gpu_mem_used, 'GB')\n\n# MSA usage summary\nmsa_used_count = 0\ntotal_targets = len(valid_df)\nfor i, row in valid_df.iterrows():\n    target_id = row.target_id\n    msa_file = os.path.join(MSA_DIR, f\"{target_id}.MSA.fasta\")\n    if os.path.exists(msa_file):\n        msa_used_count += 1\n\nprint(f'### MSA usage: {msa_used_count}/{total_targets} targets had MSA files')\nprint(f'### MSA coverage: {msa_used_count/total_targets*100:.1f}%')\nprint('')\n\nsubmit_df = solution_to_submit_df(solution)\nsubmit_df.to_csv(f'submission.csv', index=False)\nprint(submit_df)\nprint('SUBMIT OK!!!!!!')\nprint('')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:44:45.351618Z","iopub.execute_input":"2025-06-22T15:44:45.351949Z","execution_failed":"2025-06-22T15:45:53.760Z"}},"outputs":[{"name":"stdout","text":"will do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\nwill do checkpoint\n/kaggle/working/drfold/model_hub/cfg_97/model_0\n<All keys matched successfully>\n0 0 9L5R_2 193 AGCUCUCUUUGCCUUUUGGCUUAGAUCAAGUGUAGUAUCUGUUCUUUUCAGUUUAAUCUCUGAAACUGCUCUACG...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L5R_2.MSA.fasta\n   Target sequence length: 193\n   Row 0: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 1: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 2: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 3: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   Row 4: seq length 193, parse_seq_onehot shape (193, 6), expected L=193\n   MSA loaded: 100 sequences, 193 positions\n   MSA tensor shape: torch.Size([100, 193, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([193, 6])\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"out: (193, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 12 sec\ngpu_mem_used: 6 GB\n0 1 9GFT_AU 76 GGGGCUAUAGCUCAGCUGGGAGAGCGCUUGCAUGGCAUGCAAGAGGUCAGCGGUUCGAUCCCGCUUAGCUCCACC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9GFT_AU.MSA.fasta\n   Target sequence length: 76\n   Row 0: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 1: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 2: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 3: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   Row 4: seq length 76, parse_seq_onehot shape (76, 6), expected L=76\n   MSA loaded: 100 sequences, 76 positions\n   MSA tensor shape: torch.Size([100, 76, 6])\n   ✅ Using real MSA with 100 sequences\n   Seq features shape: torch.Size([76, 6])\nout: (76, 3, 3)\n   📊 Processed with MSA depth: 100\ntime_taken:  0 min 02 sec\ngpu_mem_used: 3 GB\n0 2 9L0R_K 700 CUGACGAAGUUCAAGGGUGGUACCAAGAGCGGUCAGCAUGUUGUGCAACGCUGGGAGGAUAUCCCAGUCAAACAC...\n✅ Loading MSA from /kaggle/input/stanford-rna-3d-folding/MSA_v2/9L0R_K.MSA.fasta\n   Target sequence length: 480\n   Row 0: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 1: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 2: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 3: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   Row 4: seq length 700, parse_seq_onehot shape (700, 6), expected L=700\n   MSA loaded: 100 sequences, 700 positions\n⚠️ MSA length 700 != sequence length 480, using single sequence\n   aa_type shape: (480, 6)\n   MSA tensor shape: torch.Size([2, 480, 6])\n   ⚠️ Using single sequence (duplicated)\n   Seq features shape: torch.Size([480, 6])\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"submit_df = solution_to_submit_df(solution)\nsubmit_df.to_csv(f'submission.csv', index=False)\nprint(submit_df)\nprint('SUBMIT OK!!!!!!')\nprint('')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:58:35.881243Z","iopub.status.idle":"2025-05-15T12:58:35.881528Z","shell.execute_reply":"2025-05-15T12:58:35.881415Z"}},"outputs":[],"execution_count":null}]}